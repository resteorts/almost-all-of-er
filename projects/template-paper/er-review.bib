@incollection{ball2000salvadoran,
    author = {Ball, Patrick},
    booktitle = {MAKING THE CASE: Investigating Large Scale Human Rights Violations Using Information Systems and Data Analysis},
    editor = {Ball, Patrick and Spirer, Herbert F. and Spirer, Louise},
    pages = {15----24},
    publisher = {American Association for the Advancement of Science},
    title = {{The Salvadoran Human Rights Commission: Data Processing, Data Representation, and Generating Analytical Reports}},
    year = {2000}
}


@article{howland2008rescate,
  title={How El Rescate, a small nongovernmental organization, contributed to the transformation of the human rights situation in El Salvador},
  author={Howland, Todd},
  journal={Human Rights Quarterly},
  pages={703--757},
  year={2008},
  publisher={JSTOR}
}


@article{wortman2018simultaneous,
	Author = {Wortman, Joan Heck and Reiter, Jerome P},
	Journal = {Statistics in Medicine},
	Publisher = {Wiley Online Library},
	Title = {Simultaneous record linkage and causal inference with propensity score subclassification},
	Year = {2018}}
	
@techreport{betancur1993madness,
    author = {Betancur, Belisario and Planchart, Reinaldo Figueredo and Buergenthal, Thomas},
    booktitle = {Report on The Commission on the Truth for El Salvador},
    institution = {UN Security Council},
    number = {S/25500, Annex},
    title = {{From Madness to Hope: {T}he 12-year war in {E}l {S}alvador}},
    year = {1993}
}
	
@phdthesis{wortman2019record,
	Author = {Wortman, Joan Pearson Heck},
	School = {Duke University},
	Title = {Record Linkage Methods with Applications to Causal Inference and Election Voting Data},
	Url = {https://hdl.handle.net/10161/18657},
	Year = 2019,
	Bdsk-Url-1 = {https://hdl.handle.net/10161/18657}}	


@article{tancredi2018unified,
  title={A Unified Framework for De-Duplication and Population Size Estimation (with Discussion)},
  author={Tancredi, Andrea and Steorts, Rebecca and Liseo, Brunero},
  journal={Bayesian Analysis},
  year={2020},
  volume = {15}, 
  number={2},
  pages={33–682},
  publisher={International Society for Bayesian Analysis}
}

@article{sadinle2020discussion,
  title={A Unified Framework for De-Duplication and Population Size Estimation (Invited Discussion)},
  author={Sadinle, Maurico},
  journal={Bayesian Analysis},
  year={2020},
  volume = {15}, 
  number={2},
  pages={659–663},
  publisher={International Society for Bayesian Analysis}
}

@article{murray2020discussion,
  title={A Unified Framework for De-Duplication and Population Size Estimation (Invited Discussion)},
  author={Murray, Jared},
  journal={Bayesian Analysis},
  year={2020},
  volume = {15}, 
  number={2},
  pages={664–669},
  publisher={International Society for Bayesian Analysis}
}

@article{ju2020discussion,
  title={A Unified Framework for De-Duplication and Population Size Estimation (Contributed Discussion)},
  author={Nianqiao, Ju and Biswas, Niloy and Jacob, Pierre and Mena, Gonzalo and O'Leary, John and Pompe, Emilia},
  journal={Bayesian Analysis},
  year={2020},
  volume = {15}, 
  number={2},
  pages={670–672},
  publisher={International Society for Bayesian Analysis}
}

@article{tancredi2020rejoinder,
  title={A Unified Framework for De-Duplication and Population Size Estimation (Rejoiner)},
  author={Tancredi, Andrea and Steorts, Rebecca and Liseo, Brunero},
  journal={Bayesian Analysis},
  year={2020},
  volume = {15}, 
  number={2},
  pages={675–682},
  publisher={International Society for Bayesian Analysis}
}


@inproceedings{bohannon2005cost,
  title={A Cost-Based Model and Effective Heuristic for Repairing Constraints by Value Modification},
  author={Bohannon, Philip and Fan, Wenfei and Flaster, Michael and Rastogi, Rajeev},
  booktitle={Proceedings of the 2005 ACM SIGMOD international conference on Management of data},
  pages={143--154},
  year={2005}
}

@article{korn200721,
  title={IL-21 initiates an alternative pathway to induce proinflammatory TH 17 cells},
  author={Korn, Thomas and Bettelli, Estelle and Gao, Wenda and Awasthi, Amit and J{\"a}ger, Anneli and Strom, Terry B and Oukka, Mohamed and Kuchroo, Vijay K},
  journal={Nature},
  volume={448},
  number={7152},
  pages={484--487},
  year={2007},
  publisher={Nature Publishing Group}
}

@inproceedings{yan1999conflict,
  title={Conflict tolerant queries in AURORA},
  author={Yan, Ling Ling and Ozsu, M Tamer},
  booktitle={Proceedings Fourth IFCIS International Conference on Cooperative Information Systems. CoopIS 99 (Cat. No. PR00384)},
  pages={279--290},
  year={1999},
  organization={IEEE}
}

@inproceedings{cohen2005incremental,
    author = {Cohen, Sara and Sagiv, Yehoshua},
    title = {An Incremental Algorithm for Computing Ranked Full Disjunctions},
    year = {2005},
    isbn = {1595930620},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    booktitle = {Proceedings of the Twenty-Fourth ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems},
    pages = {98–107},
    numpages = {10},
    location = {Baltimore, Maryland},
}
  

@inproceedings{culotta2007canonicalization,
    author = {Culotta, Aron and Wick, Michael and Hall, Robert and Marzilli, Matthew and McCallum, Andrew},
    title = {Canonicalization of Database Records Using Adaptive Similarity Measures},
    year = {2007},
    isbn = {9781595936097},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
    pages = {201–209},
    numpages = {9},
    keywords = {data mining, information extraction, data cleaning},
    location = {San Jose, California, USA},
}
  

	
@article{bleiholder2009data,
  title={Data Fusion},
  author={Bleiholder, Jens and Naumann, Felix},
  journal={ACM computing surveys (CSUR)},
  volume={41},
  number={1},
  pages={1--41},
  year={2009},
  publisher={ACM New York, NY, USA}
}	
	
@article{kaplan,
  title={Entity Resolution and the Downstream Task: A Case Study of North Carolina Voter Registration Records},
  author={Andee Kaplan and Brenda Betancourt and Rebecca C. Steorts},
  journal={Submitted},
  year={2020}
}	
	
@misc{ncsbe,
	Author = {North Carolina State Board of Elections},
	Note = {Accessed: 2019-12-24},
	Title = {North Carolina State Board of Elections},
	Year = {2019}
}	

@inproceedings{cohen2003comparison,
    author = {Cohen, William W. and Ravikumar, Pradeep and Fienberg, Stephen E.},
    title = {A Comparison of String Distance Metrics for Name-Matching Tasks},
    year = {2003},
    publisher = {AAAI Press},
    booktitle = {Proceedings of the 2003 International Conference on Information Integration on the Web},
    pages = {73–78},
    numpages = {6},
    location = {Acapulco, Mexico},
}



@inproceedings{frisoli2019novel,
  title={A Novel Record Linkage Interface That Incorporates Group Structure to Rapidly Collect Richer Labels},
  author={Frisoli, Kayla and LeRoy, Benjamin and Nugent, Rebecca},
  booktitle={2019 IEEE International Conference on Data Science and Advanced Analytics (DSAA)},
  pages={580--589},
  year={2019}
}

@article{Frisoli2018ExploringTE,
  title={Exploring the Effect of Household Structure in Historical Record Linkage of Early 1900s Ireland Census Records},
  author={Kayla Frisoli and Rebecca Nugent},
  journal={2018 IEEE International Conference on Data Mining Workshops (ICDMW)},
  year={2018},
  pages={502-509}
}

@article{hogan2013quality,
  title={Quality and the 2010 Census},
  author={Hogan, Howard and Cantwell, Patrick J and Devine, Jason and Mule, Vincent T and Velkoff, Victoria},
  journal={Population Research and Policy Review},
  volume={32},
  number={5},
  pages={637--662},
  year={2013},
  publisher={Springer}
}

	

@article{marchant2019d,
  title={d-blink: Distributed End-to-End Bayesian Entity Resolution},
  author={Marchant, Neil G and Steorts, Rebecca C and Kaplan, Andee and Rubinstein, Benjamin IP and Elazar, Daniel N},
  journal={arXiv preprint arXiv:1909.06039},
  year={2019}
}

@article{enamorado_using_2019, 
  title={Using a {Probabilistic} {Model} to {Assist} {Merging} of {Large-Scale} {Administrative} {Records}}, 
  volume={113}, 
  DOI={10.1017/S0003055418000783}, 
  number={2}, 
  journal={American Political Science Review}, 
  publisher={Cambridge University Press}, 
  author={Enamorado, Ted and Fifield, Benjamin and Imai, Kosuke}, 
  year={2019}, 
  pages={353--371}
}

@article{johndrow2017theoretical,
  title={Theoretical limits of record linkage and microclustering},
  author={Johndrow, James E and Lum, Kristian and Dunson, David B},
  journal={arXiv preprint arXiv:1703.04955},
  year={2017}
}

@misc{Sariyar2016,
	Author = {Sariyar, Murat and Andreas Borg},
	Date-Modified = {2017-06-18 19:55:51 +0000},
	Howpublished = {\url{http://cran.r-project.org/package=RecordLinkage}},
	Title = {Record Linkage in R. R package. Version 0.4-10},
	Year = {2016}}


@ARTICLE{gutman_2013,
  AUTHOR =       {Roee Gutman and Christopher C. Afendulis and Alan M. Zaslavsky},
  TITLE =        {{A Bayesian Procedure for File Linking to Analyze End-of-Life Medical Costs}},
  JOURNAL =      {Journal of the American Statistical Association },
  YEAR =         {2013},
  NUMBER = 	 {501},
  VOLUME =	 {108},
  PAGES =	 {34--47},
}

@article{lahiri_2005,
	Author = {Lahiri, P. and Larsen, M.},
	Journal = {Journal of the American Statistical Association},
	Number = {469},
	Pages = {222-230},
	Title = {Regression Analysis With Linked Data},
	Volume = {100},
	Year = {2005}
}

@ARTICLE{liseo_2011,
  AUTHOR =       {Andrea Tancredi and Brunero Liseo},
  TITLE =        {{A Hierarchical Bayesian Approach to Record Linkage and Size Population Problems}},
  JOURNAL =      {Annals of Applied Statistics},
  YEAR =         {2011},
  NUMBER = 	 {2B},
  VOLUME =	 {5},
  PAGES =	 {1553--1585},
}


@article{broderick_variational_2014,
	title = {Variational {Bayes} for {Merging} {Noisy} {Databases}},
	abstract = {Bayesian entity resolution merges together multiple, noisy databases and returns the minimal collection of unique individuals represented, together with their true, latent record values. Bayesian methods allow flexible generative models that share power across databases as well as principled quantification of uncertainty for queries of the final, resolved database. However, existing Bayesian methods for entity resolution use Markov monte Carlo method (MCMC) approximations and are too slow to run on modern databases containing millions or billions of records. Instead, we propose applying variational approximations to allow scalable Bayesian inference in these models. We derive a coordinate-ascent approximation for mean-field variational Bayes, qualitatively compare our algorithm to existing methods, note unique challenges for inference that arise from the expected distribution of cluster sizes in entity resolution, and discuss directions for future work in this domain.},
	urldate = {2016-03-30},
	journal = {arXiv:1410.4792 [stat]},
	author = {Broderick, Tamara and Steorts, Rebecca C.},
	year = {2014},
	note = {arXiv: 1410.4792},
	keywords = {Statistics - Machine Learning, Statistics - Methodology},
	annote = {Comment: 12 pages}
}

@article{christen2019data,
  title={Data linkage: The big picture},
  author={Christen, Peter},
  journal={Harvard Data Science Review},
  year={2019}
}

@article{SIPP,
	Author = {{{U. S. Census Bureau}}},
	Date-Added = {2020-02-02 16:35:20 -0500},
	Date-Modified = {2020-02-02 16:45:32 -0500},
	Doi = {10.3886/ICPSR04517.v1},
	Publisher = {Inter-university Consortium for Political and Social Research {$[$}distributor{$]$}},
	Title = {Survey of Income and Program Participation (SIPP) 2004 Panel},
	Ty = {DATA},
	Year = {2009}
	}

@article{spahn2017before,
  title={Before The American Voter},
  author={Spahn, Bradley},
  journal={Available at SSRN 3478473},
  year={2017}
}

@article{mcveigh2017practical,
  title={Practical bayesian inference for record linkage},
  author={McVeigh, Brendan S and Murray, Jared S},
  journal={arXiv preprint arXiv:1710.10558},
  year={2017}
}

@article{mcveigh2019scaling,
  title={Scaling Bayesian Probabilistic Record Linkage with Post-Hoc Blocking: An Application to the California Great Registers},
  author={McVeigh, Brendan S and Spahn, Bradley T and Murray, Jared S},
  journal={arXiv preprint arXiv:1905.05337},
  year={2019}
}


@article{tancredi2011hierarchical,
  title={A hierarchical Bayesian approach to record linkage and population size problems},
  author={Tancredi, Andrea and Liseo, Brunero and others},
  journal={The Annals of Applied Statistics},
  volume={5},
  number={2B},
  pages={1553--1585},
  year={2011},
  publisher={Institute of Mathematical Statistics}
}

@article{kim12,
	Acmid = {2206477},
	Address = {Amsterdam, The Netherlands, The Netherlands},
	Author = {Kim, Gunky and Chambers, Raymond},
	Issn = {0167-9473},
	Issue_Date = {September, 2012},
	Journal = {Comput. Stat. Data Anal.},
	Number = {9},
	Numpages = {15},
	Pages = {2756--2770},
	Publisher = {Elsevier Science Publishers B. V.},
	Title = {Regression Analysis Under Incomplete Linkage},
	Volume = {56},
	Year = {2012}}
	
@article{goldstein12,
	Author = {Goldstein, Harvey and Harron, Katie and Wade, Angie},
	Journal = {Statistics in Medicine},
	Number = {28},
	Pages = {3481--3493},
	Publisher = {Wiley Online Library},
	Title = {The Analysis of Record-Linked Data Using Multiple Imputation With Data Value Priors},
	Volume = {31},
	Year = {2012}}	

@article{larsen:lahiri,
	Author = {P Lahiri and Michael D Larsen},
	Journal = {Journal of the American Statistical Association},
	Number = {469},
	Pages = {222-230},
	Publisher = {Taylor & Francis},
	Title = {Regression Analysis With Linked Data},
	Volume = {100},
	Year = {2005}}

@article{kaplan2018posterior,
  title={Posterior Prototyping: Bridging the Gap between Bayesian Record Linkage and Regression},
  author={Kaplan, Andee and Betancourt, Brenda and Steorts, Rebecca C},
  journal={arXiv preprint arXiv:1810.01538},
  year={2018}
}


@article{marchant_distributed_2019,
	title = {d-blink: {D}istributed End-to-End {B}ayesian Entity Resolution},
	abstract = {Entity resolution (ER) (record linkage or de-duplication) is the process of merging together noisy databases, often in the absence of a unique identifier. A major advancement in ER methodology has been the application of Bayesian generative models. Such models provide a natural framework for clustering records to unobserved
(latent) entities, while providing exact uncertainty quantification and tight performance bounds. Despite these advancements, existing models do not scale to realistically- sized databases (larger than 1000 records) and they do not incorporate probabilistic blocking. In this paper, we propose “distributed blink” or d-blink—the first scalable and distributed end-to-end Bayesian model for ER, which propagates uncertainty in blocking, matching and merging. We make several novel contributions, including:
(i) incorporating probabilistic blocking directly into the model through auxiliary partitions; (ii) support for missing values; (iii) a partially-collapsed Gibbs sampler; and (iv) a novel perturbation sampling algorithm (leveraging the Vose-Alias method) that enables fast updates of the entity attributes. Finally, we conduct experiments on real and synthetic data which show that d-blink can achieve significant efficiency gains—in excess of 200×—when compared to existing methodology.},
	urldate = {2019-04-02},
	journal = {Submitted},
	author={Marchant, Neil and Steorts, Rebecca and Kaplan, Andee and Rubinstein, Benjamin and Elazar, Daniel},
	year = {2019},
	keywords = {Statistics - Machine Learning, Statistics - Methodology}
}


@article{steorts_bayesian_2016,
	title = {A {Bayesian} {Approach} to {Graphical} {Record} {Linkage} and {Deduplication}},
	volume = {111},
	issn = {0162-1459},
	doi = {10.1080/01621459.2015.1105807},
	abstract = {We propose an unsupervised approach for linking records across arbitrarily many files, while simultaneously detecting duplicate records within files. Our key innovation involves the representation of the pattern of links between records as a bipartite graph, in which records are directly linked to latent true individuals, and only indirectly linked to other records. This flexible representation of the linkage structure naturally allows us to estimate the attributes of the unique observable people in the population, calculate transitive linkage probabilities across records (and represent this visually), and propagate the uncertainty of record linkage into later analyses. Our method makes it particularly easy to integrate record linkage with post-processing procedures such as logistic regression, capture–recapture, etc. Our linkage structure lends itself to an efficient, linear-time, hybrid Markov chain Monte Carlo algorithm, which overcomes many obstacles encountered by previously record linkage approaches, despite the high-dimensional parameter space. We illustrate our method using longitudinal data from the National Long Term Care Survey and with data from the Italian Survey on Household and Wealth, where we assess the accuracy of our method and show it to be better in terms of error rates and empirical scalability than other approaches in the literature. Supplementary materials for this article are available online.},
	language = {en},
	number = {516},
	urldate = {2017-06-08},
	journal = {Journal of the American Statistical Association},
	author = {Steorts, Rebecca C. and Hall, Rob and Fienberg, Stephen E.},
	year = {2016},
	pages = {1660--1672}
}

@article{enamorado2019using,
  title={Using a Probabilistic Model to Assist Merging of Large-Scale Administrative Records},
  author={Ted Enamorado and Benjamin Fifield and Kosuke Imai},
  journal={American Political Science Review},
  year={2019},
  volume={113},
  pages={353-371}
}

@book{winkler1991application,
  title={An application of the Fellegi-Sunter model of record linkage to the 1990 US decennial census},
  author={Winkler, William E and Thibaudeau, Yves},
  year={1991},
  publisher={Citeseer}
}

@article{hoover2011repertoires,
  title={Repertoires of violence against noncombatants: The role of armed group institutions and ideology [PhD Thesis]},
  author={Hoover Green, Amelia},
  journal={New Haven: Yale University, Department of Political Science},
  year={2011}
}

@article{green2019civilian,
  title={Civilian Killings and Disappearances During Civil War in El Salvador (1980--1992)},
  author={Green, Amelia Hoover and Ball, Patrick},
  journal={Demographic Research},
  volume={41},
  pages={781--814},
  year={2019},
  publisher={JSTOR}
}

@article{sadinle2018bayesian,
  title={Bayesian propagation of record linkage uncertainty into population size estimation of human rights violations},
  author={Sadinle, Mauricio and others},
  journal={The Annals of Applied Statistics},
  volume={12},
  number={2},
  pages={1013--1038},
  year={2018},
  publisher={Institute of Mathematical Statistics}
}

@article{sadinle_generalized_2013,
	title = {A {Generalized} {Fellegi}-{Sunter} {Framework} for {Multiple} {Record} {Linkage} {With} {Application} to {Homicide} {Record} {Systems}},
	volume = {108},
	issn = {0162-1459},
	doi = {10.1080/01621459.2012.757231},
	abstract = {We present a probabilistic method for linking multiple datafiles. This task is not trivial in the absence of unique identifiers for the individuals recorded. This is a common scenario when linking census data to coverage measurement surveys for census coverage evaluation, and in general when multiple record systems need to be integrated for posterior analysis. Our method generalizes the Fellegi–Sunter theory for linking records from two datafiles and its modern implementations. The goal of multiple record linkage is to classify the record K-tuples coming from K datafiles according to the different matching patterns. Our method incorporates the transitivity of agreement in the computation of the data used to model matching probabilities. We use a mixture model to fit matching probabilities via maximum likelihood using the Expectation–Maximization algorithm. We present a method to decide the record K-tuples membership to the subsets of matching patterns and we prove its optimality. We apply our method to the integration of the three Colombian homicide record systems and perform a simulation study to explore the performance of the method under measurement error and different scenarios. The proposed method works well and opens new directions for future research.},
	language = {en},
	number = {502},
	urldate = {2017-07-06},
	journal = {Journal of the American Statistical Association},
	author = {Sadinle, Mauricio and Fienberg, Stephen E.},
	year = {2013},
	keywords = {EM algorithm, data matching, mixture model, Bell number, Census undercount, Data linkage, Multiple systems estimation, Partially ordered set},
	pages = {385--397}
}

@article{jain_split-merge_2004,
	title = {A {Split}-{Merge} {Markov} chain {Monte} {Carlo} {Procedure} for the {Dirichlet} {Process} {Mixture} {Model}},
	volume = {13},
	issn = {1061-8600, 1537-2715},
	doi = {10.1198/1061860043001},
	language = {en},
	number = {1},
	urldate = {2017-08-14},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Jain, Sonia and Neal, Radford M.},
	year = {2004},
	pages = {158--182}
}

@article{fritsch_improved_2009,
	title = {Improved criteria for clustering based on the posterior similarity matrix},
	volume = {4},
	issn = {1936-0975, 1931-6690},
	doi = {10.1214/09-BA414},
	abstract = {In this paper we address the problem of obtaining a single clustering estimate c{\textasciicircum}c{\textasciicircum}{\textbackslash}hat\{c\} based on an MCMC sample of clusterings c(1),c(2)…,c(M)c(1),c(2)…,c(M)c{\textasciicircum}\{(1)\},c{\textasciicircum}\{(2)\}{\textbackslash}ldots,c{\textasciicircum}\{(M)\} from the posterior distribution of a Bayesian cluster model. Methods to derive c{\textasciicircum}c{\textasciicircum}{\textbackslash}hat\{c\} when the number of groups KKK varies between the clusterings are reviewed and discussed. These include the maximum a posteriori (MAP) estimate and methods based on the posterior similarity matrix, a matrix containing the posterior probabilities that the observations iii and jjj are in the same cluster. The posterior similarity matrix is related to a commonly used loss function by Binder (1978). Minimization of the loss is shown to be equivalent to maximizing the Rand index between estimated and true clustering. We propose new criteria for estimating a clustering, which are based on the posterior expected adjusted Rand index. The criteria are shown to possess a shrinkage property and outperform Binder's loss in a simulation study and in an application to gene expression data. They also perform favorably compared to other clustering procedures.},
	language = {EN},
	number = {2},
	urldate = {2017-09-27},
	journal = {Bayesian Analysis},
	author = {Fritsch, Arno and Ickstadt, Katja},
	year = {2009},
	mrnumber = {MR2507368},
	zmnumber = {1330.62249},
	keywords = {Markov chain Monte Carlo, Dirichlet process mixture model, adjusted Rand index, cluster analysis},
	pages = {367--391}
}

@article{newman_distributed_2009,
	title = {Distributed algorithms for topic models},
	volume = {10},
	abstract = {We describe distributed algorithms for two widely-used topic models, namely the Latent Dirichlet Allocation (LDA) model, and the Hierarchical Dirichet Process (HDP) model. In our distributed algorithms the data is partitioned across separate processors and inference is done in a parallel, distributed fashion. We propose two distributed algorithms for LDA. The first algorithm is a straightforward mapping of LDA to a distributed processor setting. In this algorithm processors concurrently perform Gibbs sampling over local data followed by a global update of topic counts. The algorithm is simple to implement and can be viewed as an approximation to Gibbs-sampled LDA. The second version is a model that uses a hierarchical Bayesian extension of LDA to directly account for distributed data. This model has a theoretical guarantee of convergence but is more complex to implement than the first algorithm. Our distributed algorithm for HDP takes the straightforward mapping approach, and merges newly-created topics either by matching or by topic-id. Using five real-world text corpora we show that distributed learning works well in practice. For both LDA and HDP, we show that the converged test-data log probability for distributed learning is indistinguishable from that obtained with single-processor learning. Our extensive experimental results include learning topic models for two multi-million document collections using a 1024-processor parallel computer.},
	number = {Aug},
	journal = {Journal of Machine Learning Research},
	author = {Newman, David and Asuncion, Arthur and Smyth, Padhraic and Welling, Max},
	editor = {McCallum, Andrew},
	year = {2009},
	pages = {1801--1828}
}

@article{lovell_clustercluster:_2013,
	title = {{ClusterCluster}: {Parallel} {Markov} {Chain} {Monte} {Carlo} for {Dirichlet} {Process} {Mixtures}},
	shorttitle = {{ClusterCluster}},
	abstract = {The Dirichlet process (DP) is a fundamental mathematical tool for Bayesian nonparametric modeling, and is widely used in tasks such as density estimation, natural language processing, and time series modeling. Although MCMC inference methods for the DP often provide a gold standard in terms asymptotic accuracy, they can be computationally expensive and are not obviously parallelizable. We propose a reparameterization of the Dirichlet process that induces conditional independencies between the atoms that form the random measure. This conditional independence enables many of the Markov chain transition operators for DP inference to be simulated in parallel across multiple cores. Applied to mixture modeling, our approach enables the Dirichlet process to simultaneously learn clusters that describe the data and superclusters that define the granularity of parallelization. Unlike previous approaches, our technique does not require alteration of the model and leaves the true posterior distribution invariant. It also naturally lends itself to a distributed software implementation in terms of Map-Reduce, which we test in cluster configurations of over 50 machines and 100 cores. We present experiments exploring the parallel efficiency and convergence properties of our approach on both synthetic and real-world data, including runs on 1MM data vectors in 256 dimensions.},
	urldate = {2018-01-11},
	journal = {arXiv:1304.2302 [cs, stat]},
	author = {Lovell, Dan and Malmaud, Jonathan and Adams, Ryan P. and Mansinghka, Vikash K.},
	year = {2013},
	note = {arXiv: 1304.2302},
	keywords = {Computer Science - Learning, Statistics - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing},
	annote = {Comment: 12 pages, 10 figures. Submitted to ICML 2013 during third submission cycle}
}

@inproceedings{liang_permutation-augmented_2007,
	address = {New York, NY, USA},
	series = {{ICML} '07},
	title = {A {Permutation}-augmented {Sampler} for {DP} {Mixture} {Models}},
	isbn = {978-1-59593-793-3},
	doi = {10.1145/1273496.1273565},
	abstract = {We introduce a new inference algorithm for Dirichlet process mixture models. While Gibbs sampling and variational methods focus on local moves, the new algorithm makes more global moves. This is done by introducing a permutation of the data points as an auxiliary variable. The algorithm is a blocked sampler which alternates between sampling the clustering and sampling the permutation. The key to the efficiency of this approach is that it is possible to use dynamic programming to consider all exponentially many clusterings consistent with a given permutation. We also show that random projections can be used to effectively sample the permutation. The result is a stochastic hill-climbing algorithm that yields burn-in times significantly smaller than those of collapsed Gibbs sampling.},
	urldate = {2018-03-23},
	booktitle = {Proceedings of the 24th {International} {Conference} on {Machine} {Learning}},
	publisher = {ACM},
	author = {Liang, Percy and Jordan, Michael I. and Taskar, Ben},
	year = {2007},
	pages = {545--552}
}

@techreport{haramoto_efficient_2006,
	title = {Efficient jump ahead for {F}2-linear random number generators},
	abstract = {The fastest long-period random number generators currently available are based on linear re-currences modulo 2. So far, software that provides multiple disjoint streams and substreams has not been available for these generators because of the lack of efficient jump-ahead fa-cilities. In principle, it suffices to multiply the state (a k-bit vector) by an appropriate k × k binary matrix to find the new state far ahead in the sequence. However, when k is large (e.g., for a generator such as the popular Mersenne twister, for which k = 19937), this matrix-vector multiplication is slow and a large amount of memory is required to store the k × k matrix. In this paper, we provide a faster algorithm to jump ahead by a large number of steps in a linear recurrence modulo 2. The method uses much less than the k 2 bits of memory required by the matrix method. It is based on polynomial calculus modulo the characteristic polynomial of the recurrence and uses a sliding window algorithm for the multiplication. Key words: simulation; random number generation; jumping ahead; multiple streams 1.},
	author = {Haramoto, Hiroshi and Matsumoto, Makoto and Nishimura, Takuji and Panneton, François},
	year = {2006}
}

@article{yujian_normalized_2007,
	title = {A {Normalized} {Levenshtein} {Distance} {Metric}},
	volume = {29},
	issn = {0162-8828},
	doi = {10.1109/TPAMI.2007.1078},
	abstract = {Although a number of normalized edit distances presented so far may offer good performance in some applications, none of them can be regarded as a genuine metric between strings because they do not satisfy the triangle inequality. Given two strings X and Y over a finite alphabet, this paper defines a new normalized edit distance between X and Y as a simple function of their lengths ({\textbar}X{\textbar} and {\textbar}Y{\textbar}) and the Generalized Levenshtein Distance (GLD) between them. The new distance can be easily computed through GLD with a complexity of O({\textbar}X{\textbar} cdot {\textbar}Y{\textbar}) and it is a metric valued in [0, 1] under the condition that the weight function is a metric over the set of elementary edit operations with all costs of insertions/deletions having the same weight. Experiments using the AESA algorithm in handwritten digit recognition show that the new distance can generally provide similar results to some other normalized edit distances and may perform slightly better if the triangle inequality is violated in a particular data set.},
	number = {6},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Yujian, Li and Bo, Liu},
	year = {2007},
	keywords = {Algorithms, Artificial Intelligence, Information Storage and Retrieval, Models, Statistical, Pattern Recognition, Automated, Signal processing algorithms, AESA., Biomedical signal processing, Computational biology, Cost function, Error correction, Handwriting recognition, Image Enhancement, Image Interpretation, Computer-Assisted, Image recognition, Imaging, Three-Dimensional, Information retrieval, Levenshtein distance, metric, normalized edit distance, Pattern recognition, Sequence comparison, Sequences},
	pages = {1091--1095}
}

@inproceedings{ge_distributed_2015,
	address = {Lille, France},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Distributed {Inference} for {Dirichlet} {Process} {Mixture} {Models}},
	volume = {37},
	abstract = {Bayesian nonparametric mixture models based on the Dirichlet process (DP) have been widely used for solving problems like clustering, density estimation and topic modelling. These models make weak assumptions about the underlying process that generated the observed data. Thus, when more data are collected, the complexity of these models can change accordingly. These theoretical properties often lead to superior predictive performance when compared to traditional finite mixture models. However, despite the increasing amount of data available, the application of Bayesian nonparametric mixture models is so far limited to relatively small data sets. In this paper, we propose an efficient distributed inference algorithm for the DP and the HDP mixture model. The proposed method is based on a variant of the slice sampler for DPs. Since this sampler does not involve a pre-determined truncation, the stationary distribution of the sampling algorithm is unbiased. We provide both local thread-level and distributed machine-level parallel implementations and study the performance of this sampler through an extensive set of experiments on image and text data. When compared to existing inference algorithms, the proposed method exhibits state-of-the-art accuracy and strong scalability with up to 512 cores.},
	booktitle = {Proceedings of the 32nd {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Ge, Hong and Chen, Yutian and Wan, Moquan and Ghahramani, Zoubin},
	editor = {Bach, Francis and Blei, David},
	year = {2015},
	pages = {2276--2284}
}

@inproceedings{chang_parallel_2013,
	address = {NY, USA},
	series = {{NIPS}'13},
	title = {Parallel {Sampling} of {DP} {Mixture} {Models} {Using} {Sub}-clusters {Splits}},
	volume = {1},
	booktitle = {Proceedings of the 26th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Chang, Jason and Fisher, III, John W.},
	year = {2013},
	pages = {620--628}
}

@inproceedings{williamson_parallel_2013,
	address = {Atlanta, Georgia, USA},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Parallel {Markov} {Chain} {Monte} {Carlo} for {Nonparametric} {Mixture} {Models}},
	volume = {28},
	abstract = {Nonparametric mixture models based on the Dirichlet process are an elegant alternative to finite models when the number of underlying components is unknown, but inference in such models can be slow. Existing attempts to parallelize inference in such models have relied on introducing approximations, which can lead to inaccuracies in the posterior estimate. In this paper, we describe auxiliary variable representations for the Dirichlet process and the hierarchical Dirichlet process that allow us to perform MCMC using the correct equilibrium distribution, in a distributed manner. We show that our approach allows scalable inference without the deterioration in estimate quality that accompanies existing methods.},
	booktitle = {Proceedings of the 30th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Williamson, Sinead and Dubey, Avinava and Xing, Eric},
	editor = {Dasgupta, Sanjoy and McAllester, David},
	year = {2013},
	pages = {98--106}
}

@inproceedings{zanella_flexible_2016,
	address = {NY, USA},
	series = {{NIPS}'16},
	title = {Flexible {Models} for {Microclustering} with {Application} to {Entity} {Resolution}},
	isbn = {978-1-5108-3881-9},
	booktitle = {Proceedings of the 30th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Zanella, Giacomo and Betancourt, Brenda and Wallach, Hanna and Miller, Jeffrey and Zaidi, Abbas and Steorts, Rebecca C.},
	year = {2016},
	pages = {1425--1433}
}

@inproceedings{steorts_performance_2017,
	address = {Fort Lauderdale, FL, USA},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Performance {Bounds} for {Graphical} {Record} {Linkage}},
	volume = {54},
	abstract = {Record linkage involves merging records in large, noisy databases to remove duplicate entities. It has become an important area because of its widespread occurrence in bibliometrics, public health, official statistics production, political science, and beyond. Traditional linkage methods directly linking records to one another are computationally infeasible as the number of records grows. As a result, it is increasingly common for researchers to treat linkage as a clustering task, in which each latent entity is associated with one or more noisy database records. We critically assess performance bounds using the Kullback-Leibler (KL) divergence under a Bayesian record linkage framework, making connections to Kolchin partition models. We provide an upper bound using the KL divergence and a lower bound on the minimum probability of misclassifying a latent entity. We give insights for when our bounds hold using simulated data and provide practical user guidance.},
	booktitle = {Proceedings of the 20th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Steorts, Rebecca C. and Barnes, Mattew and Neiswanger, Willie},
	editor = {Singh, Aarti and Zhu, Jerry},
	year = {2017},
	pages = {298--306}
}

@article{vinh_information_2010,
	title = {Information theoretic measures for clusterings comparison: {Variants}, properties, normalization and correction for chance},
	volume = {11},
	shorttitle = {Information theoretic measures for clusterings comparison},
	number = {Oct},
	journal = {Journal of Machine Learning Research},
	author = {Vinh, Nguyen Xuan and Epps, Julien and Bailey, James},
	year = {2010},
	pages = {2837--2854}
}

@article{tierney_markov_1994,
	title = {Markov {Chains} for {Exploring} {Posterior} {Distributions}},
	volume = {22},
	issn = {0090-5364, 2168-8966},
	doi = {10.1214/aos/1176325750},
	abstract = {Several Markov chain methods are available for sampling from a posterior distribution. Two important examples are the Gibbs sampler and the Metropolis algorithm. In addition, several strategies are available for constructing hybrid algorithms. This paper outlines some of the basic methods and strategies and discusses some related theoretical and practical issues. On the theoretical side, results from the theory of general state space Markov chains can be used to obtain convergence rates, laws of large numbers and central limit theorems for estimates obtained from Markov chain methods. These theoretical results can be used to guide the construction of more efficient algorithms. For the practical use of Markov chain methods, standard simulation methodology provides several variance reduction techniques and also give guidance on the choice of sample size and allocation.},
	language = {EN},
	number = {4},
	journal = {The Annals of Statistics},
	author = {Tierney, Luke},
	year = {1994},
	mrnumber = {MR1329166},
	zmnumber = {0829.62080},
	keywords = {62-04, Gibbs sampler, Metropolis-Hastings algorithm, Monte Carlo, variance reduction},
	pages = {1701--1728}
}

@inproceedings{wallach_alternative_2010,
	address = {Chia Laguna Resort, Sardinia, Italy},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {An {Alternative} {Prior} {Process} for {Nonparametric} {Bayesian} {Clustering}},
	volume = {9},
	abstract = {Prior distributions play a crucial role in Bayesian approaches to clustering. Two commonly-used prior distributions are the Dirichlet and Pitman-Yor processes. In this paper, we investigate the predictive probabilities that underlie these processes, and the implicit “rich-get-richer” characteristic of the resulting partitions. We explore an alternative prior for nonparametric Bayesian clustering, the uniform process, for applications where the “rich-get-richer” property is undesirable. We also explore the cost of this new process: partitions are no longer exchangeable with respect to the ordering of variables. We present new asymptotic and simulation-based results for the clustering characteristics of the uniform process and compare these with known results for the Dirichlet and Pitman-Yor processes. Finally, we compare performance on a real document clustering task, demonstrating the practical advantage of the uniform process despite its lack of exchangeability over orderings.},
	booktitle = {Proceedings of the {Thirteenth} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Wallach, Hanna and Jensen, Shane and Dicker, Lee and Heller, Katherine},
	editor = {Teh, Yee Whye and Titterington, Mike},
	year = {2010},
	pages = {892--899}
}

@article{gnedin_exchangeable_2006,
	title = {Exchangeable {Gibbs} partitions and {Stirling} triangles},
	volume = {138},
	issn = {1072-3374, 1573-8795},
	doi = {10.1007/s10958-006-0335-z},
	abstract = {For two collections of nonnegative and suitably normalized weights W = (Wj) and V = (Vn,k), a probability distribution on the set of partitions of the set \{1, …, n\} is defined by assigning to a generic partition \{Aj, j ≤ k\} the probability Vn,k Vn,kW{\textbar}A1{\textbar}⋯W{\textbar}Ak{\textbar}Vn,kW{\textbar}A1{\textbar}⋯W{\textbar}Ak{\textbar}V\_\{n,k\} W\_\{{\textbackslash}left{\textbar} \{A\_1 \} {\textbackslash}right{\textbar}\} {\textbackslash}cdots W\_\{{\textbackslash}left{\textbar} \{A\_k \} {\textbackslash}right{\textbar}\} , where {\textbar}Aj{\textbar} is the number of elements of Aj. We impose constraints on the weights by assuming that the resulting random partitions Π n of [n] are consistent as n varies, meaning that they define an exchangeable partition of the set of all natural numbers. This implies that the weights W must be of a very special form depending on a single parameter α ∈ [− ∞, 1]. The case α = 1 is trivial, and for each value of α ≠ = 1 the set of possible V-weights is an infinite-dimensional simplex. We identify the extreme points of the simplex by solving the boundary problem for a generalized Stirling triangle. In particular, we show that the boundary is discrete for − ∞ ≤ α {\textless} 0 and continuous for 0 ≤ α {\textless} 1. For α ≤ 0 the extremes correspond to the members of the Ewens-Pitman family of random partitions indexed by (α,θ), while for 0 {\textless} α {\textless} 1 the extremes are obtained by conditioning an (α,θ)-partition on the asymptotics of the number of blocks of Πn as n tends to infinity. Bibliography: 29 titles.},
	language = {en},
	number = {3},
	journal = {Journal of Mathematical Sciences},
	author = {Gnedin, Alexander and Pitman, Jim},
	year = {2006},
	pages = {5674--5685}
}

@article{buntine_bayesian_2010,
	title = {A {Bayesian} {View} of the {Poisson}-{Dirichlet} {Process}},
	abstract = {The two parameter Poisson-Dirichlet Process (PDP), a generalisation of the Dirichlet Process, is increasingly being used for probabilistic modelling in discrete areas such as language technology, bioinformatics, and image analysis. There is a rich literature about the PDP and its derivative distributions such as the Chinese Restaurant Process (CRP). This article reviews some of the basic theory and then the major results needed for Bayesian modelling of discrete problems including details of priors, posteriors and computation. The PDP allows one to build distributions over countable partitions. The PDP has two other remarkable properties: first it is partially conjugate to itself, which allows one to build hierarchies of PDPs, and second using a marginalised relative the CRP, one gets fragmentation and clustering properties that lets one layer partitions to build trees. This article presents the basic theory for understanding the notion of partitions and distributions over them, the PDP and the CRP, and the important properties of conjugacy, fragmentation and clustering, as well as some key related properties such as consistency and convergence. This article also presents a Bayesian interpretation of the Poisson-Dirichlet process based on an improper and infinite dimensional Dirichlet distribution. This means we can understand the process as just another Dirichlet and thus all its sampling properties emerge naturally. The theory of PDPs is usually presented for continuous distributions (more generally referred to as non-atomic distributions), however, when applied to discrete distributions its remarkable conjugacy property emerges. This context and basic results are also presented, as well as techniques for computing the second order Stirling numbers that occur in the posteriors for discrete distributions.},
	urldate = {2018-06-07},
	journal = {arXiv:1007.0296 [cs, math, stat]},
	author = {Buntine, Wray and Hutter, Marcus},
	year = {2010},
	note = {arXiv: 1007.0296},
	keywords = {Computer Science - Learning, Mathematics - Statistics Theory, Mathematics - Probability},
	annote = {Comment: 50 LaTeX pages, 10 figures, 3 tables, 1 algorithm}
}

@article{marzal_computation_1993,
	title = {Computation of normalized edit distance and applications},
	volume = {15},
	issn = {0162-8828},
	doi = {10.1109/34.232078},
	abstract = {Given two strings X and Y over a finite alphabet, the normalized edit distance between X and Y, d(X,Y) is defined as the minimum of W(P)/L(P), where P is an editing path between X and Y, W(P) is the sum of the weights of the elementary edit operations of P, and L(P) is the number of these operations (length of P). It is shown that in general, d(X ,Y) cannot be computed by first obtaining the conventional (unnormalized) edit distance between X and Y and then normalizing this value by the length of the corresponding editing path. In order to compute normalized edit distances, an algorithm that can be implemented to work in O(m×n2) time and O( n2) memory space is proposed, where m and n are the lengths of the strings under consideration, and m ⩾n. Experiments in hand-written digit recognition are presented, revealing that the normalized edit distance consistently provides better results than both unnormalized or post-normalized classical edit distances},
	number = {9},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Marzal, Andrés and Vidal, Enrique},
	year = {1993},
	keywords = {Error correction, normalized edit distance, Pattern recognition, Character recognition, character strings, computational complexity, finite alphabet, hand-written digit recognition, Optical character recognition software, pattern recognition, Speech recognition, words},
	pages = {926--932}
}

@article{zanella_informed_2017,
	title = {Informed proposals for local {MCMC} in discrete spaces},
	abstract = {There is a lack of methodological results to design efficient Markov chain Monte Carlo (MCMC) algorithms for statistical models with discrete-valued high-dimensional parameters. Motivated by this consideration, we propose a simple framework for the design of informed MCMC proposals (i.e. Metropolis-Hastings proposal distributions that appropriately incorporate local information about the target) which is naturally applicable to both discrete and continuous spaces. We explicitly characterize the class of optimal proposal distributions under this framework, which we refer to as locally-balanced proposals, and prove their Peskun-optimality in high-dimensional regimes. The resulting algorithms are straightforward to implement in discrete spaces and provide orders of magnitude improvements in efficiency compared to alternative MCMC schemes, including discrete versions of Hamiltonian Monte Carlo. Simulations are performed with both simulated and real datasets, including a detailed application to Bayesian record linkage. A direct connection with gradient-based MCMC suggests that locally-balanced proposals may be seen as a natural way to extend the latter to discrete spaces.},
	urldate = {2018-06-07},
	journal = {arXiv:1711.07424 [math, stat]},
	author = {Zanella, Giacomo},
	year = {2017},
	note = {arXiv: 1711.07424},
	keywords = {Statistics - Computation, Mathematics - Probability},
	annote = {Comment: 20 pages + 14 pages of supplementary, 10 figures}
}

@article{dyk_partially_2008,
	title = {Partially {Collapsed} {Gibbs} {Samplers}},
	volume = {103},
	issn = {0162-1459},
	doi = {10.1198/016214508000000409},
	abstract = {Ever-increasing computational power, along with ever–more sophisticated statistical computing techniques, is making it possible to fit ever–more complex statistical models. Among the more computationally intensive methods, the Gibbs sampler is popular because of its simplicity and power to effectively generate samples from a high-dimensional probability distribution. Despite its simple implementation and description, however, the Gibbs sampler is criticized for its sometimes slow convergence, especially when it is used to fit highly structured complex models. Here we present partially collapsed Gibbs sampling strategies that improve the convergence by capitalizing on a set of functionally incompatible conditional distributions. Such incompatibility generally is avoided in the construction of a Gibbs sampler, because the resulting convergence properties are not well understood. We introduce three basic tools (marginalization, permutation, and trimming) that allow us to transform a Gibbs sampler into a partially collapsed Gibbs sampler with known stationary distribution and faster convergence.},
	number = {482},
	urldate = {2018-06-07},
	journal = {Journal of the American Statistical Association},
	author = {Dyk, David A. van and Park, Taeyoung},
	year = {2008},
	keywords = {Gibbs sampler, Blocking, Incompatible Gibbs sampler, Marginal data augmentation, Rate of convergence},
	pages = {790--796}
}

@article{vose_linear_1991,
	title = {A linear algorithm for generating random numbers with a given distribution},
	volume = {17},
	issn = {0098-5589},
	doi = {10.1109/32.92917},
	abstract = {Let ξ be a random variable over a finite set with an arbitrary probability distribution. Improvements to a fast method of generating sample values for ξ in constant time are suggested. The proposed modification reduces the time required for initialization to O( n). For a simple genetic algorithm, this improvement changes an O(g n 1n n) algorithm into an O(g n) algorithm (where g is the number of generations, and n is the population size)},
	number = {9},
	journal = {IEEE Transactions on Software Engineering},
	author = {Vose, Michael D.},
	year = {1991},
	keywords = {Computational modeling, Computer science, Genetic algorithms, Probability distribution, probability, arbitrary probability distribution, finite set, genetic algorithms, linear algorithm, random number generation, Random number generation, random numbers, random variable, Random variables, Roundoff errors, simple genetic algorithm},
	pages = {972--975}
}

@article{lecuyer_good_1999,
	title = {Good {Parameters} and {Implementations} for {Combined} {Multiple} {Recursive} {Random} {Number} {Generators}},
	volume = {47},
	issn = {0030-364X},
	doi = {10.1287/opre.47.1.159},
	abstract = {Combining parallel multiple recursive sequences provides an efficient way of implementing random number generators with long periods and good structural properties. Such generators are statistically more robust than simple linear congruential generators that fit into a computer word. We made extensive computer searches for good parameter sets, with respect to the spectral test, for combined multiple recursive generators of different sizes. We also compare different implementations and give a specific code in C that is faster than previous implementations of similar generators.},
	number = {1},
	urldate = {2018-06-07},
	journal = {Operations Research},
	author = {L'Ecuyer, Pierre},
	year = {1999},
	pages = {159--164}
}

@inproceedings{lomeli_hybrid_2015,
	address = {Cambridge, MA, USA},
	series = {{NIPS}'15},
	title = {A {Hybrid} {Sampler} for {Poisson}-{Kingman} {Mixture} {Models}},
	volume = {2},
	abstract = {This paper concerns the introduction of a new Markov Chain Monte Carlo scheme for posterior sampling in Bayesian nonparametric mixture models with priors that belong to the general Poisson-Kingman class. We present a novel compact way of representing the infinite dimensional component of the model such that while explicitly representing this infinite component it has less memory and storage requirements than previous MCMC schemes. We describe comparative simulation results demonstrating the efficacy of the proposed MCMC algorithm against existing marginal and conditional MCMC samplers.},
	urldate = {2018-06-07},
	booktitle = {Proceedings of the 28th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Lomelí, María and Favaro, Stefano and Teh, Yee Whye},
	year = {2015},
	pages = {2161--2169}
}

@article{steorts_entity_2015,
	title = {Entity {Resolution} with {Empirically} {Motivated} {Priors}},
	volume = {10},
	issn = {1936-0975, 1931-6690},
	doi = {10.1214/15-BA965SI},
	abstract = {Databases often contain corrupted, degraded, and noisy data with duplicate entries across and within each database. Such problems arise in citations, medical databases, genetics, human rights databases, and a variety of other applied settings. The target of statistical inference can be viewed as an unsupervised problem of determining the edges of a bipartite graph that links the observed records to unobserved latent entities. Bayesian approaches provide attractive benefits, naturally providing uncertainty quantification via posterior probabilities. We propose a novel record linkage approach based on empirical Bayesian principles. Specifically, the empirical Bayesian-type step consists of taking the empirical distribution function of the data as the prior for the latent entities. This approach improves on the earlier HB approach not only by avoiding the prior specification problem but also by allowing both categorical and string-valued variables. Our extension to string-valued variables also involves the proposal of a new probabilistic mechanism by which observed record values for string fields can deviate from the values of their associated latent entities. Categorical fields that deviate from their corresponding true value are simply drawn from the empirical distribution function. We apply our proposed methodology to a simulated data set of German names and an Italian household survey on income and wealth, showing our method performs favorably compared to several standard methods in the literature. We also consider the robustness of our methods to changes in the hyper-parameters.},
	language = {EN},
	number = {4},
	journal = {Bayesian Analysis},
	author = {Steorts, Rebecca C.},
	year = {2015},
	mrnumber = {MR3432242},
	zmnumber = {1335.62023},
	pages = {849--875}
}

@incollection{bhattacharya_latent_2006,
	series = {Proceedings},
	title = {A {Latent} {Dirichlet} {Model} for {Unsupervised} {Entity} {Resolution}},
	isbn = {978-0-89871-611-5},
	abstract = {Entity resolution has received considerable attention in recent years. Given many references to underlying entities, the goal is to predict which references correspond to the same entity. We show how to extend the Latent Dirichlet Allocation model for this task and propose a probabilistic model for collective entity resolution for relational domains where references are connected to each other. Our approach differs from other recently proposed entity resolution approaches in that it is a) generative, b) does not make pair-wise decisions and c) captures relations between entities through a hidden group variable. We propose a novel sampling algorithm for collective entity resolution which is unsupervised and also takes entity relations into account. Additionally, we do not assume the domain of entities to be known and show how to infer the number of entities from the data. We demonstrate the utility and practicality of our relational entity resolution approach for author resolution in two real-world bibliographic datasets. In addition, we present preliminary results on characterizing conditions under which relational information is useful.},
	urldate = {2018-06-13},
	booktitle = {Proceedings of the 2006 {SIAM} {International} {Conference} on {Data} {Mining}},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Bhattacharya, Indrajit and Getoor, Lise},
	year = {2006},
	doi = {10.1137/1.9781611972764.5},
	pages = {47--58}
}



@techreport{CCES:17ajps,
	Author = {Ansolabehere, Stephen and Schaffner, Brian and Luks, Sam},
	Date-Modified = {2018-07-15 20:43:52 +0000},
	Institution = {Harvard University},
	Note = {Data Release No. 2},
	Title = {Guide to the 2016 Cooperative Congressional Election Survey},
	Url = {\url{https://dataverse.harvard.edu/file.xhtml?persistentId=doi:10.7910/DVN/GDF6Z0/RK0ONG&version=4.0}},
	Year = {2017},
	Bdsk-Url-1 = {https://dataverse.harvard.edu/file.xhtml?persistentId=doi:10.7910/DVN/GDF6Z0/RK0ONG&version=4.0}}
	
@techreport{anes16:meth,
	Address = {Ann Arbor, MI and Palo Alto, CA},
	Author = {Matthew DeBell and Michelle Amsbary and Vanessa Meldener and Shelly Brock and Natalya Maisel},
	Date-Added = {2018-03-02 06:18:07 +0000},
	Date-Modified = {2018-07-15 20:48:13 +0000},
	Institution = {Stanford University and the University of Michigan.},
	Title = {Methodology Report for the ANES 2016 Time Series Study.},
	Year = {2016}}
	

@misc{bilenko2006riddle,
  title={Riddle: Repository of Information on Duplicate Detection, Record Linkage, and Identity Uncertainty},
  author={Bilenko, Misha and Mooney, R},
  year={2006}
}

@article{sadinle_bayesian_2017,
	title = {Bayesian {Estimation} of {Bipartite} {Matchings} for {Record} {Linkage}},
	volume = {112},
	issn = {0162-1459},
	doi = {10.1080/01621459.2016.1148612},
	abstract = {The bipartite record linkage task consists of merging two disparate datafiles containing information on two overlapping sets of entities. This is nontrivial in the absence of unique identifiers and it is important for a wide variety of applications given that it needs to be solved whenever we have to combine information from different sources. Most statistical techniques currently used for record linkage are derived from a seminal article by Fellegi and Sunter in 1969. These techniques usually assume independence in the matching statuses of record pairs to derive estimation procedures and optimal point estimators. We argue that this independence assumption is unreasonable and instead target a bipartite matching between the two datafiles as our parameter of interest. Bayesian implementations allow us to quantify uncertainty on the matching decisions and derive a variety of point estimators using different loss functions. We propose partial Bayes estimates that allow uncertain parts of the bipartite matching to be left unresolved. We evaluate our approach to record linkage using a variety of challenging scenarios and show that it outperforms the traditional methodology. We illustrate the advantages of our methods merging two datafiles on casualties from the civil war of El Salvador. Supplementary materials for this article are available online.},
	number = {518},
	urldate = {2018-06-13},
	journal = {Journal of the American Statistical Association},
	author = {Sadinle, Mauricio},
	year = {2017},
	keywords = {Assignment problem, Bayes estimate, Data matching, Fellegi–Sunter decision rule, Mixture model, Reject option},
	pages = {600--612}
}

@article{tancredi_hierarchical_2011,
	title = {A hierarchical {Bayesian} approach to record linkage and population size problems},
	volume = {5},
	issn = {1932-6157, 1941-7330},
	doi = {10.1214/10-AOAS447},
	abstract = {We propose and illustrate a hierarchical Bayesian approach for matching statistical records observed on different occasions. We show how this model can be profitably adopted both in record linkage problems and in capture–recapture setups, where the size of a finite population is the real object of interest. There are at least two important differences between the proposed model-based approach and the current practice in record linkage. First, the statistical model is built up on the actually observed categorical variables and no reduction (to 0–1 comparisons) of the available information takes place. Second, the hierarchical structure of the model allows a two-way propagation of the uncertainty between the parameter estimation step and the matching procedure so that no plug-in estimates are used and the correct uncertainty is accounted for both in estimating the population size and in performing the record linkage. We illustrate and motivate our proposal through a real data example and simulations.},
	language = {EN},
	number = {2B},
	urldate = {2018-06-13},
	journal = {The Annals of Applied Statistics},
	author = {Tancredi, Andrea and Liseo, Brunero},
	year = {2011},
	mrnumber = {MR2849786},
	zmnumber = {1223.62015},
	keywords = {Gibbs sampling, record linkage, Capture–recapture methods, conditional independence, Metropolis–Hastings},
	pages = {1553--1585}
}

@article{gutman_bayesian_2013,
	title = {A {Bayesian} {Procedure} for {File} {Linking} to {Analyze} {End}-of-{Life} {Medical} {Costs}},
	volume = {108},
	issn = {0162-1459},
	doi = {10.1080/01621459.2012.726889},
	abstract = {End-of-life medical expenses are a significant proportion of all health care expenditures. These costs were studied using costs of services from Medicare claims and cause of death (CoD) from death certificates. In the absence of a unique identifier linking the two datasets, common variables identified unique matches for only 33\% of deaths. The remaining cases formed cells with multiple cases (32\% in cells with an equal number of cases from each file and 35\% in cells with an unequal number). We sampled from the joint posterior distribution of model parameters and the permutations that link cases from the two files within each cell. The linking models included the regression of location of death on CoD and other parameters, and the regression of cost measures with a monotone missing data pattern on CoD and other demographic characteristics. Permutations were sampled by enumerating the exact distribution for small cells and by the Metropolis algorithm for large cells. Sparse matrix data structures enabled efficient calculations despite the large dataset (≈1.7 million cases). The procedure generates m datasets in which the matches between the two files are imputed. The m datasets can be analyzed independently and results can be combined using Rubin’s multiple imputation rules. Our approach can be applied in other file-linking applications. Supplementary materials for this article are available online.},
	number = {501},
	urldate = {2018-06-13},
	journal = {Journal of the American Statistical Association},
	author = {Gutman, Roee and Afendulis, Christopher C. and Zaslavsky, Alan M.},
	year = {2013},
	pmid = {23645944},
	keywords = {Administrative data, Bayesian analysis, Missing data, Record linkage, Statistical matching},
	pages = {34--47}
}

@article{bentley_multidimensional_1975,
	title = {Multidimensional {Binary} {Search} {Trees} {Used} for {Associative} {Searching}},
	volume = {18},
	issn = {0001-0782},
	doi = {10.1145/361002.361007},
	abstract = {This paper develops the multidimensional binary search tree (or k-d tree, where k is the dimensionality of the search space) as a data structure for storage of information to be retrieved by associative searches. The k-d tree is defined and examples are given. It is shown to be quite efficient in its storage requirements. A significant advantage of this structure is that a single data structure can handle many types of queries very efficiently. Various utility algorithms are developed; their proven average running times in an n record file are: insertion, O(log n); deletion of the root, O(n(k-1)/k); deletion of a random node, O(log n); and optimization (guarantees logarithmic performance of searches), O(n log n). Search algorithms are given for partial match queries with t keys specified [proven maximum running time of O(n(k-t)/k)] and for nearest neighbor queries [empirically observed average running time of O(log n).] These performances far surpass the best currently known algorithms for these tasks. An algorithm is presented to handle any general intersection query. The main focus of this paper is theoretical. It is felt, however, that k-d trees could be quite useful in many applications, and examples of potential uses are given.},
	number = {9},
	urldate = {2018-06-21},
	journal = {Commun. ACM},
	author = {Bentley, Jon Louis},
	year = {1975},
	keywords = {associative retrieval, attribute, binary search trees, binary tree insertion, information retrieval system, intersection queries, key, nearest neighbor queries, partial match queries},
	pages = {509--517}
}

@article{friedman_algorithm_1977,
	title = {An {Algorithm} for {Finding} {Best} {Matches} in {Logarithmic} {Expected} {Time}},
	volume = {3},
	issn = {0098-3500},
	doi = {10.1145/355744.355745},
	number = {3},
	urldate = {2018-06-21},
	journal = {ACM Trans. Math. Softw.},
	author = {Friedman, Jerome H. and Bentley, Jon Louis and Finkel, Raphael Ari},
	year = {1977},
	pages = {209--226}
}

@misc{liseo_2013,
	Author = {Liseo, Brunero and Tancredi, Andrea},
	Date-Modified = {2013-05-30 04:51:43 +0000},
	Title = {Some advances on {B}ayesian record linkage and inference for linked data},
	Url = {http://www.ine.es/e/essnetdi_ws2011/ppts/Liseo_Tancredi.pdf},
	Year = 2013,
	Bdsk-Url-1 = {http://www.ine.es/e/essnetdi_ws2011/ppts/Liseo_Tancredi.pdf}}
	

@ARTICLE{Fortinietal01,
  AUTHOR =       {Fortini, M. and Liseo, B. and Nuccitelli, A. and Scanu, M.},
  TITLE =        {{On Bayesian Record Linkage}},
  JOURNAL =      {Research in Official Statistics},
  YEAR =         {2001},
  NUMBER = 	 {1},
  VOLUME =	 {4},
  PAGES =	 {185--198},
}

@MISC{Larsen12,
  AUTHOR =       {Michael D. Larsen},
  TITLE =        {{An Experiment with Hierarchical Bayesian Record Linkage}},
  HOWPUBLISHED =      {Preprint in arXiv: http://arxiv.org/abs/1212.5203},
  YEAR =         {2012},
}

@PHDTHESIS{Matsakis10,
  AUTHOR =       {Nicholas Elias Matsakis},
  TITLE =        {{Active Duplicate Detection with Bayesian Nonparametric Models}},
  SCHOOL =      {Massachusetts Institute of Technology},
  YEAR =         {2010},
}

	
@article{christen_2012,
  title={A survey of indexing techniques for scalable record linkage and deduplication},
  author={Christen, Peter},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={24},
  number={9},
  pages={1537--1555},
  year={2012},
  publisher={IEEE}
}
	

@book{Herzog_2007,
	Address = {New York},
	Author = {Herzog, T.N. and Scheuren, F.J. and Winkler, W.E.},
	Date-Added = {2013-05-30 03:49:33 +0000},
	Date-Modified = {2013-05-30 03:51:42 +0000},
	Publisher = {Springer},
	Title = {Data Quality and Record Linkage Techniques},
	Year = {2007}}

@article{belin_1995,
	title = {A {Method} for {Calibrating} {False}-{Match} {Rates} in {Record} {Linkage}},
	volume = {90},
	issn = {0162-1459},
	doi = {10.2307/2291082},
	number = {430},
	journal = {Journal of the American Statistical Association},
	author = {Belin, Thomas R. and Rubin, Donald B.},
	year = {1995},
	pages = {694--707}
}

@article{larsen_2001,
	title = {Iterative {Automated} {Record} {Linkage} {Using} {Mixture} {Models}},
	volume = {96},
	issn = {0162-1459},
	doi = {10.1198/016214501750332956},
	abstract = {The goal of record linkage is to link quickly and accurately records that correspond to the same person or entity. Whereas certain patterns of agreements and disagreements on variables are more likely among records pertaining to a single person than among records for different people, the observed patterns for pairs of records can be viewed as arising from a mixture of matches and nonmatches. Mixture model estimates can be used to partition record pairs into two or more groups that can be labeled as probable matches (links) and probable nonmatches (nonlinks). A method is proposed and illustrated that uses marginal information in the database to select mixture models, identifies sets of records for clerks to review based on the models and marginal information, incorporates clerically reviewed data, as they become available, into estimates of model parameters, and classifies pairs as links, nonlinks, or in need of further clerical review. The procedure is illustrated with five datasets from the U.S. Bureau of the Census. It appears to be robust to variations in record-linkage sites. The clerical review corrects classifications of some pairs directly and leads to changes in classification of others through reestimation of mixture models.},
	number = {453},
	urldate = {2018-07-19},
	journal = {Journal of the American Statistical Association},
	author = {Larsen, Michael D. and Rubin, Donald B.},
	year = {2001},
	pages = {32--41}
}

@inproceedings{domingos_2004,
  title={Multi-relational record linkage},
  author={Domingos, Parag and Domingos, Pedro},
  booktitle={Proceedings of the KDD-2004 Workshop on Multi-Relational Data Mining},
  year={2004},
  organization={ACM}
}

	
@INPROCEEDINGS{Larsen02,
  AUTHOR =       {Larsen, Michael D.},
  TITLE =        {{Comments on Hierarchical Bayesian Record Linkage}},
  BOOKTITLE =    {Proceedings of the Joint Statistical Meetings, Section on Survey Research Methods},
  YEAR =         {2002},
  ORGANIZATION = {The American Statistical Association},
  PAGES =	 {1995--2000},
} 

@article{winkler1995matching,
  title={Matching and record linkage},
  author={Winkler, William E},
  journal={Business survey methods},
  volume={1},
  pages={355--384},
  year={1995},
  publisher={New York}
}


@INPROCEEDINGS{Larsen05,
  AUTHOR =       {Larsen, Michael D.},
  TITLE =        {{Advances in Record Linkage Theory: Hierarchical Bayesian Record Linkage Theory}},
  BOOKTITLE =    {Proceedings of the Joint Statistical Meetings, Section on Survey Research Methods},
  YEAR =         {2005},
  ORGANIZATION = {The American Statistical Association},
  PAGES =	 {3277--3284}
} 

@article{murray2016probabilistic,
  title={Probabilistic Record Linkage and Deduplication after Indexing, Blocking, and Filtering},
  author={Murray, Jared S.},
  journal={Journal of Privacy and Confidentiality},
  volume={7},
  number={1},
  pages={3--24},
  year={2016}
}	

@Article{steorts14smered,
  author =       {Steorts, Rebecca C. and Hall, Rob and Fienberg, Stephen E.},
  title =        {{SMERED}: A {B}ayesian Approach to Graphical Record Linkage and De-duplication},
  journal =      {Journal of Machine Learning Research},
  year =         {2014},
  OPTkey =       {},
  volume =       {33},
  OPTnumber =    {},
  pages =        {922--930}
}


@techreport{winkler_overview_2006,
	title = {Overview of record linkage and current research directions},
	abstract = {This paper provides background on record linkage methods that can be used in combining data from a variety of sources such as person lists business lists. It also gives some areas of current research.},
	number = {Statistics \#2006-2},
	institution = {U.S. Bureau of the Census},
	author = {Winkler, William E.},
	year = {2006}
}

@techreport{winkler_state_1999,
	title = {The {State} of {Record} {Linkage} and {Current} {Research} {Problems}},
	abstract = {This paper provides an overview of methods and systems developed for record linkage. Modern record linkage begins with the pioneering work of Newcombe and is especially based on the formal mathematical model of Fellegi and Sunter. In their seminal work, Fellegi and Sunter introduced many powerful ideas for estimating record linkage parameters and other ideas that still influence record linkage today. Record linkage research is characterized by its synergism of statistics, computer science, and operations research. Many difficult algorithms have been developed and put in software systems. Record linkage practice is still very limited. Some limits are due to existing software. Other limits are due to the difficulty in automatically estimating matching parameters and error rates, with current research highlighted by the work of Larsen and Rubin.},
	institution = {Statistical Research Division, U.S. Bureau of the Census},
	author = {Winkler, William E.},
	year = {1999}
}

@article{sadinle_detecting_2014,
	title = {Detecting duplicates in a homicide registry using a {Bayesian} partitioning approach},
	volume = {8},
	issn = {1932-6157, 1941-7330},
	doi = {10.1214/14-AOAS779},
	language = {EN},
	number = {4},
	urldate = {2018-06-26},
	journal = {The Annals of Applied Statistics},
	author = {Sadinle, Mauricio},
	year = {2014},
	mrnumber = {MR3292503},
	zmnumber = {06408784},
	keywords = {Deduplication, distribution on partitions, duplicate detection, entity resolution, Hispanic names, homicide records, human rights, record linkage, string similarity, United Nations Truth Commission for El Salvador},
	pages = {2404--2434}
}

@article{fellegi_theory_1969,
	title = {A {Theory} for {Record} {Linkage}},
	volume = {64},
	issn = {0162-1459},
	doi = {10.1080/01621459.1969.10501049},
	abstract = {A mathematical model is developed to provide a theoretical framework for a computer-oriented solution to the problem of recognizing those records in two files which represent identical persons, objects or events (said to be matched). A comparison is to be made between the recorded characteristics and values in two records (one from each file) and a decision made as to whether or not the members of the comparison-pair represent the same person or event, or whether there is insufficient evidence to justify either of these decisions at stipulated levels of error. These three decisions are referred to as link (A 1), a non-link (A 3), and a possible link (A 2). The first two decisions are called positive dispositions. The two types of error are defined as the error of the decision A 1 when the members of the comparison pair are in fact unmatched, and the error of the decision A 3 when the members of the comparison pair are, in fact matched. The probabilities of these errors are defined as and respectively where u(γ), m(γ) are the probabilities of realizing γ (a comparison vector whose components are the coded agreements and disagreements on each characteristic) for unmatched and matched record pairs respectively. The summation is over the whole comparison space r of possible realizations. A linkage rule assigns probabilities P(A 1{\textbar}γ), and P(A 2{\textbar}γ), and P(A 3{\textbar}γ) to each possible realization of γ ε Γ. An optimal linkage rule L (μ, λ, Γ) is defined for each value of (μ, λ) as the rule that minimizes P(A 2) at those error levels. In other words, for fixed levels of error, the rule minimizes the probability of failing to make positive dispositions. A theorem describing the construction and properties of the optimal linkage rule and two corollaries to the theorem which make it a practical working tool are given.},
	number = {328},
	urldate = {2018-06-26},
	journal = {Journal of the American Statistical Association},
	author = {Fellegi, Ivan P. and Sunter, Alan B.},
	year = {1969},
	pages = {1183--1210}
}

@article{newcombe_automatic_1959,
	title = {Automatic {Linkage} of {Vital} {Records}: {Computers} can be used to extract "follow-up" statistics of families from files of routine records},
	volume = {130},
	issn = {0036-8075, 1095-9203},
	shorttitle = {Automatic {Linkage} of {Vital} {Records}},
	doi = {10.1126/science.130.3381.954},
	language = {en},
	number = {3381},
	urldate = {2018-06-26},
	journal = {Science},
	author = {Newcombe, H. B. and Kennedy, J. M. and Axford, S. J. and James, A. P.},
	year = {1959},
	pmid = {14426783},
	pages = {954--959}
}

@inproceedings{tang2020,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Bayesian Modeling for Simultaneous Regression and Record Linkage},
	urldate = {2018-06-26},
	booktitle = {Privacy in {Statistical} {Databases}},
	publisher = {Springer, Cham},
	author = {Tang, Jiurui  and Reiter, Jerome and Steorts, Rebecca C.},
	year = {2020}
}

@inproceedings{steorts_comparison_2014,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Comparison} of {Blocking} {Methods} for {Record} {Linkage}},
	isbn = {978-3-319-11256-5 978-3-319-11257-2},
	doi = {10.1007/978-3-319-11257-2_20},
	abstract = {Record linkage seeks to merge databases and to remove duplicates when unique identifiers are not available. Most approaches use blocking techniques to reduce the computational complexity associated with record linkage. We review traditional blocking techniques, which typically partition the records according to a set of field attributes, and consider two variants of a method known as locality sensitive hashing, sometimes referred to as “private blocking.” We compare these approaches in terms of their recall, reduction ratio, and computational complexity. We evaluate these methods using different synthetic datafiles and conclude with a discussion of privacy-related issues.},
	language = {en},
	urldate = {2018-06-26},
	booktitle = {Privacy in {Statistical} {Databases}},
	publisher = {Springer, Cham},
	author = {Steorts, Rebecca C. and Ventura, Samuel L. and Sadinle, Mauricio and Fienberg, Stephen E.},
	year = {2014},
	pages = {253--268}
}


@article{geman_stochastic_1984,
	title = {Stochastic {Relaxation}, {Gibbs} {Distributions}, and the {Bayesian} {Restoration} of {Images}},
	volume = {PAMI-6},
	issn = {0162-8828},
	doi = {10.1109/TPAMI.1984.4767596},
	abstract = {We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of edges are viewed as states of atoms or molecules in a lattice-like physical system. The assignment of an energy function in the physical system determines its Gibbs distribution. Because of the Gibbs distribution, Markov random field (MRF) equivalence, this assignment also determines an MRF image model. The energy function is a more convenient and natural mechanism for embodying picture attributes than are the local characteristics of the MRF. For a range of degradation mechanisms, including blurring, nonlinear deformations, and multiplicative or additive noise, the posterior distribution is an MRF with a structure akin to the image model. By the analogy, the posterior distribution defines another (imaginary) physical system. Gradual temperature reduction in the physical system isolates low energy states (“annealing”), or what is the same thing, the most probable states under the Gibbs distribution. The analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations. The result is a highly parallel “relaxation” algorithm for MAP estimation. We establish convergence properties of the algorithm and we experiment with some simple pictures, for which good restorations are obtained at low signal-to-noise ratios.},
	number = {6},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Geman, Stuart and Geman, Donald},
	year = {1984},
	keywords = {Bayesian methods, Markov random fields, Stochastic processes, Additive noise, Annealing, Deformable models, Degradation, Energy states, Gibbs distribution, image restoration, Image restoration, line process, MAP estimate, Markov random field, relaxation, scene modeling, spatial degradation, Temperature distribution},
	pages = {721--741}
}

@article{sariyar_recordlinkage_2010,
	title = {The {RecordLinkage} {Package}: {Detecting} {Errors} in {Data}},
	volume = {2},
	issn = {2073-4859},
	abstract = {Record linkage deals with detecting homonyms and mainly synonyms in data. The package RecordLinkage provides means to perform and evaluate different record linkage methods. A stochastic framework is implemented which calculates weights through an EM algorithm. The determination of the necessary thresholds in this model can be achieved by tools of extreme value theory. Furthermore, machine learning methods are utilized, including decision trees (rpart), bootstrap aggregating (bagging), ada boost (ada), neural nets (nnet) and support vector machines (svm). The generation of record pairs and comparison patterns from single data items are provided as well. Comparison patterns can be chosen to be binary or based on some string metrics. In order to reduce computation time and memory usage, blocking can be used. Future development will concentrate on additional and reﬁned methods, performance improvements and input/output facilities needed for real-world application.},
	language = {en},
	number = {2},
	journal = {The R Journal},
	author = {Sariyar, Murat and Borg, Andreas},
	year = {2010},
	pages = {61--67}
}

@book{liu_monte_2004,
  address = {New York},
  series = {Springer {Series} in {Statistics}},
  title = {Monte {Carlo} {Strategies} in {Scientific} {Computing}},
  isbn = {978-0-387-76369-9},
  abstract = {This paperback edition is a reprint of the 2001 Springer edition. This book provides a self-contained and up-to-date treatment of the Monte Carlo method and develops a common framework under which various Monte Carlo techniques can be "standardized" and compared. Given the interdisciplinary nature of the topics and a moderate prerequisite for the reader, this book should be of interest to a broad audience of quantitative researchers such as computational biologists, computer scientists, econometricians, engineers, probabilists, and statisticians. It can also be used as the textbook for a graduate-level course on Monte Carlo methods. Many problems discussed in the alter chapters can be potential thesis topics for masters’ or Ph.D. students in statistics or computer science departments. Jun Liu is Professor of Statistics at Harvard University, with a courtesy Professor appointment at Harvard Biostatistics Department. Professor Liu was the recipient of the 2002 COPSS Presidents' Award, the most prestigious one for statisticians and given annually by five leading statistical associations to one individual under age 40. He was selected as a Terman Fellow by Stanford University in 1995, as a Medallion Lecturer by the Institute of Mathematical Statistics (IMS) in 2002, and as a Bernoulli Lecturer by the International Bernoulli Society in 2004. He was elected to the IMS Fellow in 2004 and Fellow of the American Statistical Association in 2005. He and co-workers have published more than 130 research articles and book chapters on Bayesian modeling and computation, bioinformatics, genetics, signal processing, stochastic dynamic systems, Monte Carlo methods, and theoretical statistics. "An excellent survey of current Monte Carlo methods. The applications amply demonstrate the relevance of this approach to modern computing. The book is highly recommended." (Mathematical Reviews) "This book provides comprehensive coverage of Monte Carlo methods, and in the process uncovers and discusses commonalities among seemingly disparate techniques that arose in various areas of application. … The book is well organized; the flow of topics follows a logical development. … The coverage is up-to-date and comprehensive, and so the book is a good resource for people conducting research on Monte Carlo methods. … The book would be an excellent supplementary text for a course in scientific computing … ." (SIAM Review) "The strength of this book is in bringing together advanced Monte Carlo (MC) methods developed in many disciplines. … Throughout the book are examples of techniques invented, or reinvented, in different fields that may be applied elsewhere. … Those interested in using MC to solve difficult problems will find many ideas, collected from a variety of disciplines, and references for further study." (Technometrics)},
  language = {en},
  publisher = {Springer-Verlag},
  author = {Liu, Jun S.},
  year = {2004}
}


@article{vats_multivariate_2015,
	title = {Multivariate {Output} {Analysis} for {Markov} chain {Monte} {Carlo}},
	url = {http://arxiv.org/abs/1512.07713},
	abstract = {Markov chain Monte Carlo (MCMC) produces a correlated sample for estimating expectations with respect to a target distribution. A fundamental question is when should sampling stop so that we have good estimates of the desired quantities? The key to answering this question lies in assessing the Monte Carlo error through a multivariate Markov chain central limit theorem (CLT). The multivariate nature of this Monte Carlo error largely has been ignored in the MCMC literature. We present a multivariate framework for terminating simulation in MCMC. We define a multivariate effective sample size, estimating which requires strongly consistent estimators of the covariance matrix in the Markov chain CLT; a property we show for the multivariate batch means estimator. We then provide a lower bound on the number of minimum effective samples required for a desired level of precision. This lower bound depends on the problem only in the dimension of the expectation being estimated, and not on the underlying stochastic process. This result is obtained by drawing a connection between terminating simulation via effective sample size and terminating simulation using a relative standard deviation fixed-volume sequential stopping rule; which we demonstrate is an asymptotically valid procedure. The finite sample properties of the proposed method are demonstrated in a variety of examples.},
	urldate = {2018-06-29},
	journal = {arXiv:1512.07713 [math, stat]},
	author = {Vats, Dootika and Flegal, James M. and Jones, Galin L.},
	year = {2015},
	note = {arXiv: 1512.07713},
	keywords = {Mathematics - Statistics Theory, Statistics - Computation}
}

@techreport{winkler_methods_2002,
    title = {Methods for {Record Linkage} and {Bayesian Networks}},
    number = {Statistics \#2002-05},
    author = {Winkler, William E.},
    year = {2002},
  	institution = {U.S. Bureau of the Census},
}


@misc{manton_nltcs_2010,
    title = {{National} {Long-Term} {Care} {Survey}: 1982, 1984, 1989, 1994, 1999 and 2004},
    author = {Manton, Kenneth G.},
    year = {2010},
    doi = {10.3886/ICPSR09681.v5},
    publisher = {{Inter-University} {Consortium} for {Political} and {Social} {Research}},
    location = {Ann Arbor, MI}
}

@techreport{christen_preparation_2014,
	title = {Preparation of a Real Temporal Voter Data Set for Record Linkage and Duplicate Detection Research},
	institution = {Australian National University},
	author = {Christen, Peter},
	year = {2014}
}

@misc{bancaitalia_2010,
  title = {Bank of Italy -- Survey on Household Income and Wealth},
  author = {{Banca d'Italia}},
  year = {n.d.},
  howpublished = {\url{http://www.bancaditalia.it/pubblicazioni/indagine-famiglie/index.html}},
  note = {Accessed: 9 March 2018}
}


@article{zaharia_apache_2016,
 author = {Zaharia, Matei and Xin, Reynold S. and Wendell, Patrick and Das, Tathagata and Armbrust, Michael and Dave, Ankur and Meng, Xiangrui and Rosen, Josh and Venkataraman, Shivaram and Franklin, Michael J. and Ghodsi, Ali and Gonzalez, Joseph and Shenker, Scott and Stoica, Ion},
 title = {Apache Spark: A Unified Engine for Big Data Processing},
 journal = {Commun. ACM},
 issue_date = {November 2016},
 volume = {59},
 number = {11},
 year = {2016},
 issn = {0001-0782},
 pages = {56--65},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2934664},
 doi = {10.1145/2934664},
 acmid = {2934664},
 publisher = {ACM},
 address = {New York, NY, USA},
}


@article{jasra_markov_2005,
	title = {Markov {Chain} {Monte} {Carlo} {Methods} and the {Label} {Switching} {Problem} in {Bayesian} {Mixture} {Modeling}},
	volume = {20},
	issn = {0883-4237, 2168-8745},
	doi = {10.1214/088342305000000016},
	abstract = {In the past ten years there has been a dramatic increase of interest in the Bayesian analysis of finite mixture models. This is primarily because of the emergence of Markov chain Monte Carlo (MCMC) methods. While MCMC provides a convenient way to draw inference from complicated statistical models, there are many, perhaps underappreciated, problems associated with the MCMC analysis of mixtures. The problems are mainly caused by the nonidentifiability of the components under symmetric priors, which leads to so-called label switching in the MCMC output. This means that ergodic averages of component specific quantities will be identical and thus useless for inference. We review the solutions to the label switching problem, such as artificial identifiability constraints, relabelling algorithms and label invariant loss functions. We also review various MCMC sampling schemes that have been suggested for mixture models and discuss posterior sensitivity to prior specification.},
	language = {en},
	number = {1},
	urldate = {2018-07-04},
	journal = {Statistical Science},
	author = {Jasra, Ajay and Holmes, Chris C. and Stephens, David A.},
	year = {2005},
	mrnumber = {MR2182987},
	zmnumber = {1100.62032},
	keywords = {Bayesian statistics, identifiability, label switching, MCMC, mixture modeling, sensitivity analysis},
	pages = {50--67}
}


@article{copas_record_1990,
	title = {Record {Linkage}: {Statistical} {Models} for {Matching} {Computer} {Records}},
	volume = {153},
	issn = {0964-1998},
	shorttitle = {Record {Linkage}},
	doi = {10.2307/2982975},
	abstract = {We wish to measure the evidence that a pair of records relates to the same, rather than different, individuals. The paper emphasizes statistical models which can be fitted to a file of record pairs known to be correctly matched, and then used to estimate likelihood ratios. A number of models are developed and applied to UK immigration statistics. The combination of likelihood ratios for possibly correlated record fields is discussed.},
	number = {3},
	urldate = {2018-07-10},
	journal = {Journal of the Royal Statistical Society. Series A (Statistics in Society)},
	author = {Copas, J. B. and Hilton, F. J.},
	year = {1990},
	pages = {287--320}
}

@inproceedings{narayanan2009anonymizing,
	Author = {Narayanan, Arvind and Shmatikov, Vitaly},
	Booktitle = {Security and Privacy, 2009 30th IEEE Symposium on},
	Date-Added = {2015-01-29 08:36:41 +0000},
	Date-Modified = {2015-01-29 08:36:41 +0000},
	Organization = {IEEE},
	Pages = {173--187},
	Title = {De-anonymizing social networks},
	Year = {2009}}
	
@inbook{fienberg10,
	Author = {S.E. Fienberg and A.B. Slavkovi\'c},
	Date-Added = {2015-01-27 02:41:52 +0000},
	Date-Modified = {2015-01-27 02:41:52 +0000},
	Pages = {342-345},
	Publisher = {Springer-Verlag},
	Series = {International Encyclopedia of Statistical Science},
	Title = {Data Privacy and Confidentiality},
	Year = {2010}}	


@techreport{ramanayake10,
	Author = {A. Ramanayake and L. Zayatz},
	Date-Added = {2015-01-29 08:40:57 +0000},
	Date-Modified = {2015-01-29 08:40:57 +0000},
	Institution = {U.S. Census Bureau},
	Number = {2010-04},
	Title = {Balancing Disclosure Risk with Data Quality},
	Type = {Statistical Research Division Research Report Series},
	Year = {2010}}
	
@inproceedings{DMNS06,
	Author = {Cynthia Dwork and Frank McSherry and Kobbi Nissim and Adam Smith},
	Booktitle = {Theory of Cryptography Conference},
	Date-Added = {2011-06-02 09:13:18 -0400},
	Date-Modified = {2011-06-02 09:13:18 -0400},
	Pages = {265--284},
	Printoutnum = {567},
	Publisher = {Springer},
	Title = {Calibrating noise to sensitivity in private data analysis},
	Year = {2006}}	
	
@book{hundepool2012sdc,
	Author = {Hundepool, Anco and Domingo-Ferrer, Josep and Franconi, Luisa and Giessing, Sarah and Nordholt, Eric Schulte and Spicer, Keith and De Wolf, Peter-Paul},
	Date-Added = {2015-01-27 02:43:22 +0000},
	Date-Modified = {2015-01-27 02:43:22 +0000},
	Publisher = {Wiley. com},
	Title = {Statistical disclosure control},
	Year = {2012}}	
	
@article{rubin_1993,
  title={Multiple imputation for statistical disclosure limitation},
  author={Raghunathan, Trivellore E and Reiter, Jerome P and Rubin, Donald B},
  journal={JOURNAL OF OFFICIAL STATISTICS-STOCKHOLM-},
  volume={19},
  number={1},
  pages={1--16},
  year={2003},
  publisher={ALMQVIST \& WIKSELL INTERNATIONAL}
}

@techreport{reiter2010,
  title={Releasing multiply-imputed synthetic data generated in two stages to protect confidentiality},
  author={Reiter, Jerome P and Drechsler, J{\"o}rg},
  year={2010},
  journal={Statistica Sinica},
  volume={20},
  pages={405-421},
}

@article{abowd_2001,
  title={Disclosure limitation in longitudinal linked data},
  author={Abowd, John M and Woodcock, Simon D},
  journal={Confidentiality, Disclosure, and Data Access: Theory and Practical Applications for Statistical Agencies},
  pages={215--277},
  year={2001},
  publisher={Amsterdam: North Holland}
}

@inproceedings{abowd_2008,
  title={How Protective Are Synthetic Data?},
  author={Abowd, John M and Vilhuber, Lars},
  booktitle={Privacy in Statistical Databases},
  pages={239--246},
  year={2008},
  organization={Springer}
}

@article{reiter_2005,
  title={Releasing multiply imputed, synthetic public use microdata: an illustration and empirical study},
  author={Reiter, Jerome P},
  journal={Journal of the Royal Statistical Society: Series A (Statistics in Society)},
  volume={168},
  number={1},
  pages={185--205},
  year={2005},
  publisher={Wiley Online Library}
}

@article{reiter_2003,
  title={Using CART to generate partially synthetic public use microdata},
  author={Reiter, JP},
  journal={Journal of official statistics},
  volume={21},
  number={3},
  pages={441--462},
  year={2005},
  publisher={Stockholm Statistics Sweden}
}

@article{slavkovic2010synthetic,
	Author = {Slavkovi{\'c}, Aleksandra B and Lee, Juyoun},
	Date-Added = {2015-01-27 04:21:21 +0000},
	Date-Modified = {2015-01-27 04:21:21 +0000},
	Journal = {Statistical Methodology},
	Number = {3},
	Pages = {225--239},
	Publisher = {Elsevier},
	Title = {Synthetic two-way contingency tables that preserve conditional frequencies},
	Volume = {7},
	Year = {2010}}
	
@article{reiter_2010,
  title={Sampling with synthesis: A new approach for releasing public use census microdata},
  author={Drechsler, J{\"o}rg and Reiter, Jerome P},
  journal={Journal of the American Statistical Association},
  volume={105},
  number={492},
  pages={1347--1357},
  year={2010},
  publisher={Taylor \& Francis}
}	

@inproceedings{charest_2012,
  title={Empirical evaluation of statistical inference from differentially-private contingency tables},
  author={Charest, Anne-Sophie},
  booktitle={Privacy in Statistical Databases},
  pages={257--272},
  year={2012},
  organization={Springer}
}

@conference{SCDres2013,
	Author = {J. Soria-Cormas and J. Drechsler},
	Date-Added = {2015-01-28 21:41:15 +0000},
	Date-Modified = {2015-01-28 21:41:15 +0000},
	Journal = {In UNECE Conference of European Statisticans},
	Title = {Evaluating the potential of differential privacy mechanisms for census data},
	Year = {2013}}


@techreport{ghosh_2013b,
  title={Perturbed {G}ibbs {S}amplers for Synthetic Data Release},
  author={Yubin Park and Joydeep Ghosh},
  year={2013},
  journal={arXiv},
  url={arXiv:.1312.537},
}

@article{KarwaSla2015AOS,
	Author = {Karwa, Vishesh and Slavkovi{\'c}, Aleksandra},
	Date-Added = {2015-01-27 02:44:08 +0000},
	Date-Modified = {2015-01-27 02:57:19 +0000},
	Journal = {Annals of Statistics},
	Title = {Inference using noisy degrees: Differentially Private $\beta$-model and Synthetic Graphs},
	Volume = {In revision},
	Year = {2015}}

@inproceedings{MKAGV08,
	Author = {Ashwin Machanavajjhala and Daniel Kifer and John M. Abowd and Johannes Gehrke and Lars Vilhuber},
	Bibsource = {DBLP, http://dblp.uni-trier.de},
	Booktitle = {24th International Conference on Data Engineering (ICDE)},
	Pages = {277-286},
	Publisher = {IEEE},
	Title = {Privacy: Theory meets Practice on the Map},
	Year = {2008}}
	
@article{karwa2012differentially,
	Author = {Karwa, V. and Slavkovic, A.},
	Date-Added = {2015-01-27 02:42:33 +0000},
	Date-Modified = {2015-01-27 02:42:33 +0000},
	Journal = {Arxiv preprint arXiv:1205.4697},
	Title = {Differentially Private Synthetic graphs},
	Year = {2012}}
	

@book{willenborg96,
	Address = {New York, NY},
	Author = {L. Willenborg and T. {de Waal}},
	Date-Added = {2015-01-27 02:43:26 +0000},
	Date-Modified = {2015-01-27 02:43:26 +0000},
	Publisher = {Springer},
	Title = {Statistical Disclosure Control in Practice},
	Year = {1996}}


@book{lane2014privacy,
	Author = {Lane, Julia and Stodden, Victoria and Bender, Stefan and Nissenbaum, Helen},
	Date-Added = {2015-01-29 08:34:01 +0000},
	Date-Modified = {2015-01-29 08:34:01 +0000},
	Publisher = {Cambridge University Press},
	Title = {Privacy, Big Data, and the Public Good: Frameworks for Engagement},
	Year = {2014}}


@article{getoor_entity_2012,
    author = {Getoor, Lise and Machanavajjhala, Ashwin},
    title = {Entity Resolution: Theory, Practice \& Open Challenges},
    year = {2012},
    publisher = {VLDB Endowment},
    volume = {5},
    number = {12},
    doi = {10.14778/2367502.2367564},
    journal = {Proceedings of the VLDB Endowment},
    pages = {2018–2019},
    numpages = {2}
}
  


@article{elmagarmid_duplicate_2007,
	title = {Duplicate {Record} {Detection}: {A} {Survey}},
	volume = {19},
	issn = {1041-4347},
	shorttitle = {Duplicate {Record} {Detection}},
	doi = {10.1109/TKDE.2007.250581},
	abstract = {Often, in the real world, entities have two or more representations in databases. Duplicate records do not share a common key and/or they contain errors that make duplicate matching a difficult task. Errors are introduced as the result of transcription errors, incomplete information, lack of standard formats, or any combination of these factors. In this paper, we present a thorough analysis of the literature on duplicate record detection. We cover similarity metrics that are commonly used to detect similar field entries, and we present an extensive set of duplicate detection algorithms that can detect approximately duplicate records in a database. We also cover multiple techniques for improving the efficiency and scalability of approximate duplicate detection algorithms. We conclude with coverage of existing tools and with a brief discussion of the big open problems in the area},
	number = {1},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Elmagarmid, Ahmed K. and Ipeirotis, Panagiotis G. and Verykios, Vassilios S.},
	year = {2007},
	keywords = {Cleaning, Computer errors, Computer Society, Cost function, Couplings, data cleaning, data deduplication, data integration, data integrity, data mining, database hardening, database management system, database management systems, Detection algorithms, Duplicate detection, duplicate detection algorithm, duplicate record detection, entity matching., entity resolution, fuzzy duplicate detection, identity uncertainty, instance identification, Mirrors, name matching, record linkage, Relational databases, Scalability, transcription error, Uncertainty},
	pages = {1--16}
}

@article{wang_crowder:_2012,
	title = {{CrowdER}: {Crowdsourcing} {Entity} {Resolution}},
	volume = {5},
	issn = {2150-8097},
	shorttitle = {{CrowdER}},
	doi = {10.14778/2350229.2350263},
	abstract = {Entity resolution is central to data integration and data cleaning. Algorithmic approaches have been improving in quality, but remain far from perfect. Crowdsourcing platforms offer a more accurate but expensive (and slow) way to bring human insight into the process. Previous work has proposed batching verification tasks for presentation to human workers but even with batching, a human-only approach is infeasible for data sets of even moderate size, due to the large numbers of matches to be tested. Instead, we propose a hybrid human-machine approach in which machines are used to do an initial, coarse pass over all the data, and people are used to verify only the most likely matching pairs. We show that for such a hybrid system, generating the minimum number of verification tasks of a given size is NP-Hard, but we develop a novel two-tiered heuristic approach for creating batched tasks. We describe this method, and present the results of extensive experiments on real data sets using a popular crowdsourcing platform. The experiments show that our hybrid approach achieves both good efficiency and high accuracy compared to machine-only or human-only alternatives.},
	number = {11},
	urldate = {2018-07-10},
	journal = {Proc. VLDB Endow.},
	author = {Wang, Jiannan and Kraska, Tim and Franklin, Michael J. and Feng, Jianhua},
	year = {2012},
	pages = {1483--1494}
}

@inproceedings{gokhale_corleone:_2014,
	address = {New York, NY, USA},
	series = {{SIGMOD} '14},
	title = {Corleone: {Hands}-off {Crowdsourcing} for {Entity} {Matching}},
	isbn = {978-1-4503-2376-5},
	shorttitle = {Corleone},
	doi = {10.1145/2588555.2588576},
	abstract = {Recent approaches to crowdsourcing entity matching (EM) are limited in that they crowdsource only parts of the EM workflow, requiring a developer to execute the remaining parts. Consequently, these approaches do not scale to the growing EM need at enterprises and crowdsourcing startups, and cannot handle scenarios where ordinary users (i.e., the masses) want to leverage crowdsourcing to match entities. In response, we propose the notion of hands-off crowdsourcing (HOC), which crowdsources the entire workflow of a task, thus requiring no developers. We show how HOC can represent a next logical direction for crowdsourcing research, scale up EM at enterprises and crowdsourcing startups, and open up crowdsourcing for the masses. We describe Corleone, a HOC solution for EM, which uses the crowd in all major steps of the EM process. Finally, we discuss the implications of our work to executing crowdsourced RDBMS joins, cleaning learning models, and soliciting complex information types from crowd workers.},
	urldate = {2018-07-10},
	booktitle = {Proceedings of the 2014 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Gokhale, Chaitanya and Das, Sanjib and Doan, AnHai and Naughton, Jeffrey F. and Rampalli, Narasimhan and Shavlik, Jude and Zhu, Xiaojin},
	year = {2014},
	keywords = {active learning, crowdsourcing, entity matching},
	pages = {601--612}
}


@book{christen_data_2012,
	address = {Berlin Heidelberg},
	series = {Data-{Centric} {Systems} and {Applications}},
	title = {Data {Matching}: {Concepts} and {Techniques} for {Record} {Linkage}, {Entity} {Resolution}, and {Duplicate} {Detection}},
	shorttitle = {Data {Matching}},
	language = {en},
	publisher = {Springer-Verlag},
	author = {Christen, Peter},
	year = {2012},
	pages = {270}
}

@article{papadakis_comparative_2016,
	title = {Comparative {Analysis} of {Approximate} {Blocking} {Techniques} for {Entity} {Resolution}},
	volume = {9},
	issn = {2150-8097},
	doi = {10.14778/2947618.2947624},
	abstract = {Entity Resolution is a core task for merging data collections. Due to its quadratic complexity, it typically scales to large volumes of data through blocking: similar entities are clustered into blocks and pair-wise comparisons are executed only between co-occurring entities, at the cost of some missed matches. There are numerous blocking methods, and the aim of this work is to offer a comprehensive empirical survey, extending the dimensions of comparison beyond what is commonly available in the literature. We consider 17 state-of-the-art blocking methods and use 6 popular real datasets to examine the robustness of their internal configurations and their relative balance between effectiveness and time efficiency. We also investigate their scalability over a corpus of 7 established synthetic datasets that range from 10,000 to 2 million entities.},
	number = {9},
	urldate = {2018-07-10},
	journal = {Proc. VLDB Endow.},
	author = {Papadakis, George and Svirsky, Jonathan and Gal, Avigdor and Palpanas, Themis},
	year = {2016},
	pages = {684--695}
}


@inproceedings{mudgal_deep_2018,
	address = {New York, NY, USA},
	series = {{SIGMOD} '18},
	title = {Deep {Learning} for {Entity} {Matching}: {A} {Design} {Space} {Exploration}},
	isbn = {978-1-4503-4703-7},
	shorttitle = {Deep {Learning} for {Entity} {Matching}},
	doi = {10.1145/3183713.3196926},
	abstract = {Entity matching (EM) finds data instances that refer to the same real-world entity. In this paper we examine applying deep learning (DL) to EM, to understand DL's benefits and limitations. We review many DL solutions that have been developed for related matching tasks in text processing (e.g., entity linking, textual entailment, etc.). We categorize these solutions and define a space of DL solutions for EM, as embodied by four solutions with varying representational power: SIF, RNN, Attention, and Hybrid. Next, we investigate the types of EM problems for which DL can be helpful. We consider three such problem types, which match structured data instances, textual instances, and dirty instances, respectively. We empirically compare the above four DL solutions with Magellan, a state-of-the-art learning-based EM solution. The results show that DL does not outperform current solutions on structured EM, but it can significantly outperform them on textual and dirty EM. For practitioners, this suggests that they should seriously consider using DL for textual and dirty EM problems. Finally, we analyze DL's performance and discuss future research directions.},
	urldate = {2018-07-10},
	booktitle = {Proceedings of the 2018 {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Mudgal, Sidharth and Li, Han and Rekatsinas, Theodoros and Doan, AnHai and Park, Youngchoon and Krishnan, Ganesh and Deep, Rohit and Arcaute, Esteban and Raghavendra, Vijay},
	year = {2018},
	keywords = {deep learning, entity matching, entity resolution},
	pages = {19--34}
}

@inproceedings{galhotra_robust_2018,
	address = {New York, NY, USA},
	series = {{SIGMOD} '18},
	title = {Robust {Entity} {Resolution} {Using} {Random} {Graphs}},
	isbn = {978-1-4503-4703-7},
	doi = {10.1145/3183713.3183755},
	abstract = {Entity resolution (ER) seeks to identify which records in a data set refer to the same real-world entity. Given the diversity of ways in which entities can be represented, matched and distinguished, ER is known to be a challenging task for automated strategies, but relatively easier for expert humans. In our work, we abstract the knowledge of experts with the notion of a binary oracle. Our oracle can answer questions of the form "do records u and v refer to the same entity?" under a flexible error model, allowing for some questions to be more difficult to answer correctly than others. Our contribution is a general error correction tool that can be leveraged by a variety of hybrid-human machine ER algorithms, based on a formal way for selecting indirect "control queries''. In our experiments we demonstrate that correction-less ER algorithms equipped with our tool can perform even better than recent ER algorithms specifically designed for correcting errors. Our control queries are selected among those that provide strongest connectivity between records of each cluster, based on the concept ofgraph expanders (which are sparse graphs with formal connectivity properties). We give formal performance guarantees for our toolkit and provide experiments on real and synthetic data.},
	urldate = {2018-07-10},
	booktitle = {Proceedings of the 2018 {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Galhotra, Sainyam and Firmani, Donatella and Saha, Barna and Srivastava, Divesh},
	year = {2018},
	keywords = {crowdsourcing, data cleaning, entity resolution, expander graphs},
	pages = {3--18}
}


@inproceedings{singh_generating_2017,
	address = {New York, NY, USA},
	series = {{SIGMOD} '17},
	title = {Generating {Concise} {Entity} {Matching} {Rules}},
	isbn = {978-1-4503-4197-4},
	doi = {10.1145/3035918.3058739},
	abstract = {Entity matching (EM) is a critical part of data integration and cleaning. In many applications, the users need to understand why two entities are considered a match, which reveals the need for interpretable and concise EM rules. We model EM rules in the form of General Boolean Formulas (GBFs) that allows arbitrary attribute matching combined by conjunctions (∨), disjunctions (∧), and negations. (¬) GBFs can generate more concise rules than traditional EM rules represented in disjunctive normal forms (DNFs). We use program synthesis, a powerful tool to automatically generate rules (or programs) that provably satisfy a high-level specification, to automatically synthesize EM rules in GBF format, given only positive and negative matching examples. In this demo, attendees will experience the following features: (1) Interpretability -- they can see and measure the conciseness of EM rules defined using GBFs; (2) Easy customization -- they can provide custom experiment parameters for various datasets, and, easily modify a rich predefined (default) synthesis grammar, using a Web interface; and (3) High performance -- they will be able to compare the generated concise rules, in terms of accuracy, with probabilistic models (e.g., machine learning methods), and hand-written EM rules provided by experts. Moreover, this system will serve as a general platform for evaluating different methods that discover EM rules, which will be released as an open-source tool on GitHub.},
	urldate = {2018-07-11},
	booktitle = {Proceedings of the 2017 {ACM} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Singh, Rohit and Meduri, Vamsi and Elmagarmid, Ahmed and Madden, Samuel and Papotti, Paolo and Quian{\'e}-Ruiz, Jorge-Arnulfo and Solar-Lezama, Armando and Tang, Nan},
	year = {2017},
	keywords = {disjunctive normal forms, entity matching, general boolean formulas, program synthesis},
	pages = {1635--1638}
}

@article{fan_reasoning_2009,
	title = {Reasoning {About} {Record} {Matching} {Rules}},
	volume = {2},
	issn = {2150-8097},
	doi = {10.14778/1687627.1687674},
	abstract = {To accurately match records it is often necessary to utilize the semantics of the data. Functional dependencies (FDs) have proven useful in identifying tuples in a clean relation, based on the semantics of the data. For all the reasons that FDs and their inference are needed, it is also important to develop dependencies and their reasoning techniques for matching tuples from unreliable data sources. This paper investigates dependencies and their reasoning for record matching. (a) We introduce a class of matching dependencies (MDs) for specifying the semantics of data in unreliable relations, defined in terms of similarity metrics and a dynamic semantics. (b) We identify a special case of MDs, referred to as relative candidate keys (RCKs), to determine what attributes to compare and how to compare them when matching records across possibly different relations. (c) We propose a mechanism for inferring MDs, a departure from traditional implication analysis, such that when we cannot match records by comparing attributes that contain errors, we may still find matches by using other, more reliable attributes. (d) We provide an O(n2) time algorithm for inferring MDs, and an effective algorithm for deducing a set of RCKs from MDs. (e) We experimentally verify that the algorithms help matching tools efficiently identify keys at compile time for matching, blocking or windowing, and that the techniques effectively improve both the quality and efficiency of various record matching methods.},
	number = {1},
	urldate = {2018-07-11},
	journal = {Proc. VLDB Endow.},
	author = {Fan, Wenfei and Jia, Xibei and Li, Jianzhong and Ma, Shuai},
	year = {2009},
	pages = {407--418}
}

@incollection{neal_mcmc_2011,
	address = {New York},
	series = {Handbooks of {Modern} {Statistical} {Methods}},
	title = {{MCMC} {Using} {Hamiltonian} {Dynamics}},
	isbn = {978-1-4200-7942-5},
	abstract = {Markov chain Monte Carlo (MCMC) originated with the classic paper of Metropolis et al. (1953), where it was used to simulate the distribution of states for a system of idealized molecules. Not long after, another approach to molecular simulation was introduced (Alder and Wainwright, 1959), in which the motion of the molecules was deterministic, following Newton’s laws of motion, which have an elegant formalization as Hamiltonian dynamics. For finding the properties of bulk materials, these approaches are asymptotically equivalent, since even in a deterministic simulation, each local region of the material experiences effectively random influences from distant regions. Despite the large overlap in their application areas, the MCMC and molecular dynamics approaches have continued to coexist in the following decades (see Frenkel and Smit, 1996).},
	language = {en},
	booktitle = {Handbook of {Markov} {Chain} {Monte} {Carlo}},
	publisher = {Chapman and Hall/CRC},
	author = {Neal, Radford M.},
	editor = {Brooks, Steve and Gelman, Andrew and Jones, Galin L. and Meng, Xiao-Li},
	year = {2011},
	note = {Google-Books-ID: qfRsAIKZ4rIC},
	keywords = {Mathematics / Probability \& Statistics / General, Science / Life Sciences / Biology},
	pages = {50}
}


@article{blei_variational_2017,
	title = {Variational {Inference}: {A} {Review} for {Statisticians}},
	volume = {112},
	issn = {0162-1459},
	shorttitle = {Variational {Inference}},
	doi = {10.1080/01621459.2017.1285773},
	abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this article, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find a member of that family which is close to the target density. Closeness is measured by Kullback–Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this article is to catalyze statistical research on this class of algorithms. Supplementary materials for this article are available online.},
	number = {518},
	urldate = {2018-07-11},
	journal = {Journal of the American Statistical Association},
	author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
	year = {2017},
	keywords = {Algorithms, Computationally intensive methods, Statistical computing}
}


@inproceedings{gal_pitfalls_2014,
	address = {Beijing, China},
	series = {{ICML}'14},
	title = {Pitfalls in the {Use} of {Parallel} {Inference} for the {Dirichlet} {Process}},
	abstract = {Recent work done by Lovell, Adams, and Mansingka (2012) and Williamson, Dubey, and Xing (2013) has suggested an alternative parametrisation for the Dirichlet process in order to derive non-approximate parallel MCMC inference for it - work which has been picked-up and implemented in several different fields. In this paper we show that the approach suggested is impractical due to an extremely unbalanced distribution of the data. We characterise the requirements of efficient parallel inference for the Dirichlet process and show that the proposed inference fails most of these requirements (while approximate approaches often satisfy most of them). We present both theoretical and experimental evidence, analysing the load balance for the inference and showing that it is independent of the size of the dataset and the number of nodes available in the parallel implementation. We end with suggestions of alternative paths of research for efficient non-approximate parallel inference for the Dirichlet process.},
	urldate = {2018-07-11},
	booktitle = {Proceedings of the 31st {International} {Conference} on {International} {Conference} on {Machine} {Learning} - {Volume} 32},
	publisher = {JMLR.org},
	author = {Gal, Yarin and Ghahramani, Zoubin},
	year = {2014},
	pages = {II--208--II--216}
}


@article{smola_architecture_2010,
	title = {An {Architecture} for {Parallel} {Topic} {Models}},
	volume = {3},
	issn = {2150-8097},
	doi = {10.14778/1920841.1920931},
	abstract = {This paper describes a high performance sampling architecture for inference of latent topic models on a cluster of workstations. Our system is faster than previous work by over an order of magnitude and it is capable of dealing with hundreds of millions of documents and thousands of topics. The algorithm relies on a novel communication structure, namely the use of a distributed (key, value) storage for synchronizing the sampler state between computers. Our architecture entirely obviates the need for separate computation and synchronization phases. Instead, disk, CPU, and network are used simultaneously to achieve high performance. We show that this architecture is entirely general and that it can be extended easily to more sophisticated latent variable models such as n-grams and hierarchies.},
	number = {1-2},
	urldate = {2018-07-11},
	journal = {Proc. VLDB Endow.},
	author = {Smola, Alexander and Narayanamurthy, Shravan},
	year = {2010},
	pages = {703--710}
}


@inproceedings{ahn_distributed_2014,
	address = {Beijing, China},
	series = {{ICML}'14},
	title = {Distributed {Stochastic} {Gradient} {MCMC}},
	abstract = {Probabilistic inference on a big data scale is becoming increasingly relevant to both the machine learning and statistics communities. Here we introduce the first fully distributed MCMC algorithm based on stochastic gradients. We argue that stochastic gradient MCMC algorithms are particularly suited for distributed inference because individual chains can draw mini-batches from their local pool of data for a flexible amount of time before jumping to or syncing with other chains. This greatly reduces communication overhead and allows adaptive load balancing. Our experiments for LDA on Wikipedia and Pubmed show that relative to the state of the art in distributed MCMC we reduce compute time from 27 hours to half an hour in order to reach the same perplexity level.},
	urldate = {2018-07-12},
	booktitle = {Proceedings of the 31st {International} {Conference} on {International} {Conference} on {Machine} {Learning} - {Volume} 32},
	publisher = {JMLR.org},
	author = {Ahn, Sungjin and Shahbaba, Babak and Welling, Max},
	year = {2014},
	pages = {II--1044--II--1052}
}


@book{naumann_introduction_2010,
	title = {An Introduction to Duplicate Detection},
	publisher = {Morgan and Claypool Publishers},
	author = {Naumann, Felix and Herschel, Melanie},
	year = {2010}
}

@book{gelman_bayesian_2013,
	address = {New York},
	edition = {3rd edition},
	series = {Texts in {Statistical} {Science}},
	title = {Bayesian {Data} {Analysis}},
	isbn = {978-1-4398-9820-8},
	language = {en},
	urldate = {2018-07-13},
	publisher = {Chapman and Hall/CRC},
	author = {Gelman, Andrew and Carlin, John B. and Stern, Hal S. and Dunson, David B. and Vehtari, Aki and Rubin, Donald B.},
	year = {2013},
	doi = {10.1201/b16018}
}

@article{winkler1990string,
  title={String Comparator Metrics and Enhanced Decision Rules in the Fellegi-Sunter Model of Record Linkage.},
  author={Winkler, William E},
  year={1990},
  publisher={ERIC}
}

@article{winkler2014matching,
  title = {Matching and record linkage},
  author = {Winkler, William E.},
  journal = {Wiley Interdisciplinary Reviews: Computational Statistics},
  volume = {6},
  number = {5},
  pages = {313--325},
  year = {2014},
  publisher = {Wiley Online Library},
  copyright = {Published 2014. This article is a U.S. Government work and is in the public domain in the USA.},
  issn = {1939-0068},
  doi = {10.1002/wics.1317},
  abstract = {This overview gives background on a number of statistical methods that have been proven effective for record linkage. To prepare data for the main computational algorithms, we need parsing/standardization that allows us to structure the free-form names, addresses, and other fields into corresponding components. The main parameter-estimation methods are unsupervised methods that yield ‘optimal’ record linkage parameters. Extended methods provide estimates of false match rates in both unsupervised and, with greater accuracy, in semi-supervised situations. Finally, the paper describes ongoing research for adjusting standard statistical analyses for linkage error.},
  language = {en}
}


@book{dong_big_2015,
	title = {Big {Data} {Integration}},
	number = {1},
	journal = {Synthesis Lectures on Data Management},
	author = {Dong, Xin Luna and Srivastava, Divesh},
	year = {2015},
	pages = {1--178},
	publisher={Morgan and Claypool Publishers}
}


@article{bleiholder_data_2009,
	title = {Data {Fusion}},
	volume = {41},
	number = {1},
	journal = {ACM computing surveys (CSUR)},
	author = {Bleiholder, Jens and Naumann, Felix},
	year = {2009},
	pages = {1--41}
}


@article{zhao_bayesian_2012,
	title = {A {Bayesian} {Approach} to {Discovering} {Truth} from {Conflicting} {Sources} for {Data} {Integration}},
	volume = {5},
	issn = {2150-8097},
	doi = {10.14778/2168651.2168656},
	abstract = {In practical data integration systems, it is common for the data sources being integrated to provide conflicting information about the same entity. Consequently, a major challenge for data integration is to derive the most complete and accurate integrated records from diverse and sometimes conflicting sources. We term this challenge the truth finding problem. We observe that some sources are generally more reliable than others, and therefore a good model of source quality is the key to solving the truth finding problem. In this work, we propose a probabilistic graphical model that can automatically infer true records and source quality without any supervision. In contrast to previous methods, our principled approach leverages a generative process of two types of errors (false positive and false negative) by modeling two different aspects of source quality. In so doing, ours is also the first approach designed to merge multi-valued attribute types. Our method is scalable, due to an efficient sampling-based inference algorithm that needs very few iterations in practice and enjoys linear time complexity, with an even faster incremental variant. Experiments on two real world datasets show that our new method outperforms existing state-of-the-art approaches to the truth finding problem.},
	number = {6},
	journal = {Proc. VLDB Endow.},
	author = {Zhao, Bo and Rubinstein, Benjamin I. P. and Gemmell, Jim and Han, Jiawei},
	year = {2012},
	pages = {550--561}
}

@article{li_survey_2016,
	title = {A {Survey} on {Truth} {Discovery}},
	volume = {17},
	issn = {1931-0145},
	doi = {10.1145/2897350.2897352},
	abstract = {Thanks to information explosion, data for the objects of interest can be collected from increasingly more sources. However, for the same object, there usually exist conflicts among the collected multi-source information. To tackle this challenge, truth discovery, which integrates multi-source noisy information by estimating the reliability of each source, has emerged as a hot topic. Several truth discovery methods have been proposed for various scenarios, and they have been successfully applied in diverse application domains. In this survey, we focus on providing a comprehensive overview of truth discovery methods, and summarizing them from different aspects. We also discuss some future directions of truth discovery research. We hope that this survey will promote a better understanding of the current progress on truth discovery, and offer some guidelines on how to apply these approaches in application domains.},
	number = {2},
	journal = {SIGKDD Explor. Newsl.},
	author = {Li, Yaliang and Gao, Jing and Meng, Chuishi and Li, Qi and Su, Lu and Zhao, Bo and Fan, Wei and Han, Jiawei},
	year = {2016},
	pages = {1--16}
}

@inproceedings{li_resolving_2014,
	address = {New York, NY, USA},
	series = {{SIGMOD} '14},
	title = {Resolving {Conflicts} in {Heterogeneous} {Data} by {Truth} {Discovery} and {Source} {Reliability} {Estimation}},
	isbn = {978-1-4503-2376-5},
	doi = {10.1145/2588555.2610509},
	abstract = {In many applications, one can obtain descriptions about the same objects or events from a variety of sources. As a result, this will inevitably lead to data or information conflicts. One important problem is to identify the true information (i.e., the truths) among conflicting sources of data. It is intuitive to trust reliable sources more when deriving the truths, but it is usually unknown which one is more reliable a priori. Moreover, each source possesses a variety of properties with different data types. An accurate estimation of source reliability has to be made by modeling multiple properties in a unified model. Existing conflict resolution work either does not conduct source reliability estimation, or models multiple properties separately. In this paper, we propose to resolve conflicts among multiple sources of heterogeneous data types. We model the problem using an optimization framework where truths and source reliability are defined as two sets of unknown variables. The objective is to minimize the overall weighted deviation between the truths and the multi-source observations where each source is weighted by its reliability. Different loss functions can be incorporated into this framework to recognize the characteristics of various data types, and efficient computation approaches are developed. Experiments on real-world weather, stock and flight data as well as simulated multi-source data demonstrate the necessity of jointly modeling different data types in the proposed framework.},
	booktitle = {Proceedings of the 2014 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Li, Qi and Li, Yaliang and Gao, Jing and Zhao, Bo and Fan, Wei and Han, Jiawei},
	year = {2014},
	keywords = {data fusion, heterogeneous data, truth discovery},
	pages = {1187--1198}
}

@article{zheng_truth_2017,
	title = {Truth {Inference} in {Crowdsourcing}: {Is} the {Problem} {Solved}?},
	volume = {10},
	issn = {2150-8097},
	shorttitle = {Truth {Inference} in {Crowdsourcing}},
	doi = {10.14778/3055540.3055547},
	abstract = {Crowdsourcing has emerged as a novel problem-solving paradigm, which facilitates addressing problems that are hard for computers, e.g., entity resolution and sentiment analysis. However, due to the openness of crowdsourcing, workers may yield low-quality answers, and a redundancy-based method is widely employed, which first assigns each task to multiple workers and then infers the correct answer (called truth) for the task based on the answers of the assigned workers. A fundamental problem in this method is Truth Inference, which decides how to effectively infer the truth. Recently, the database community and data mining community independently study this problem and propose various algorithms. However, these algorithms are not compared extensively under the same framework and it is hard for practitioners to select appropriate algorithms. To alleviate this problem, we provide a detailed survey on 17 existing algorithms and perform a comprehensive evaluation using 5 real datasets. We make all codes and datasets public for future research. Through experiments we find that existing algorithms are not stable across different datasets and there is no algorithm that outperforms others consistently. We believe that the truth inference problem is not fully solved, and identify the limitations of existing algorithms and point out promising research directions.},
	number = {5},
	journal = {Proc. VLDB Endow.},
	author = {Zheng, Yudian and Li, Guoliang and Li, Yuanbing and Shan, Caihua and Cheng, Reynold},
	year = {2017},
	pages = {541--552}
}


@inproceedings{pasternack_latent_2013,
	address = {New York, NY, USA},
	series = {{WWW} '13},
	title = {Latent {Credibility} {Analysis}},
	isbn = {978-1-4503-2035-1},
	doi = {10.1145/2488388.2488476},
	abstract = {A frequent problem when dealing with data gathered from multiple sources on the web (ranging from booksellers to Wikipedia pages to stock analyst predictions) is that these sources disagree, and we must decide which of their (often mutually exclusive) claims we should accept. Current state-of-the-art information credibility algorithms known as "fact-finders" are transitive voting systems with rules specifying how votes iteratively flow from sources to claims and then back to sources. While this is quite tractable and often effective, fact-finders also suffer from substantial limitations; in particular, a lack of transparency obfuscates their credibility decisions and makes them difficult to adapt and analyze: knowing the mechanics of how votes are calculated does not readily tell us what those votes mean, and finding, for example, that a source has a score of 6 is not informative. We introduce a new approach to information credibility, Latent Credibility Analysis (LCA), constructing strongly principled, probabilistic models where the truth of each claim is a latent variable and the credibility of a source is captured by a set of model parameters. This gives LCA models clear semantics and modularity that make extending them to capture additional observed and latent credibility factors straightforward. Experiments over four real-world datasets demonstrate that LCA models can outperform the best fact-finders in both unsupervised and semi-supervised settings.},
	booktitle = {Proceedings of the 22Nd {International} {Conference} on {World} {Wide} {Web}},
	publisher = {ACM},
	author = {Pasternack, Jeff and Roth, Dan},
	year = {2013},
	keywords = {Graphical models, credibility, trust, veracity},
	pages = {1009--1020}
}


@article{lesot_similarity_2008,
	title = {Similarity measures for binary and numerical data: a survey},
	volume = {1},
	issn = {1755-3210},
	shorttitle = {Similarity measures for binary and numerical data},
	doi = {10.1504/IJKESDP.2009.021985},
	abstract = {Similarity measures aim at quantifying the extent to which objects resemble each other. Many techniques in data mining, data analysis or information retrieval require a similarity measure, and selecting an appropriate measure for a given problem is a difficult task. In this paper, the diverse forms similarity measures can take are examined, as well as their relationships and respective properties. Their semantic differences are highlighted and numerical tools to quantify these differences are proposed, considering several points of view and including global and local comparisons, order-based and value-based comparisons, and mathematical properties such as derivability. The paper studies similarity measures for two types of data: binary and numerical data, i.e., set data represented by the presence or absence of characteristics and data represented by real vectors.},
	number = {1},
	journal = {International Journal of Knowledge Engineering and Soft Data Paradigms},
	author = {Lesot, Marie-Jeanne and Rifqi, Maria and Benhadda, Hamid},
	year = {2008},
	pages = {63--84}
}


@article{turek_efficient_2016,
	title = {Efficient {Markov} chain {Monte} {Carlo} sampling for hierarchical hidden {Markov} models},
	volume = {23},
	issn = {1352-8505, 1573-3009},
	doi = {10.1007/s10651-016-0353-z},
	abstract = {Traditional Markov chain Monte Carlo (MCMC) sampling of hidden Markov models (HMMs) involves latent states underlying an imperfect observation process, and generates posterior samples for top-level parameters concurrently with nuisance latent variables. When potentially many HMMs are embedded within a hierarchical model, this can result in prohibitively long MCMC runtimes. We study combinations of existing methods, which are shown to vastly improve computational efficiency for these hierarchical models while maintaining the modeling flexibility provided by embedded HMMs. The methods include discrete filtering of the HMM likelihood to remove latent states, reduced data representations, and a novel procedure for dynamic block sampling of posterior dimensions. The first two methods have been used in isolation in existing application-specific software, but are not generally available for incorporation in arbitrary model structures. Using the NIMBLE package for R, we develop and test combined computational approaches using three examples from ecological capture–recapture, although our methods are generally applicable to any embedded discrete HMMs. These combinations provide several orders of magnitude improvement in MCMC sampling efficiency, defined as the rate of generating effectively independent posterior samples. In addition to being computationally significant for this class of hierarchical models, this result underscores the potential for vast improvements to MCMC sampling efficiency which can result from combinations of known algorithms.},
	language = {en},
	number = {4},
	journal = {Environmental and Ecological Statistics},
	author = {Turek, Daniel and Valpine, Perry de and Paciorek, Christopher J.},
	year = {2016},
	pages = {549--564}
}

@inproceedings{bilenko_adaptive_2003,
	address = {New York, NY, USA},
	series = {{KDD} '03},
	title = {Adaptive {Duplicate} {Detection} {Using} {Learnable} {String} {Similarity} {Measures}},
	isbn = {978-1-58113-737-8},
	doi = {10.1145/956750.956759},
	abstract = {The problem of identifying approximately duplicate records in databases is an essential step for data cleaning and data integration processes. Most existing approaches have relied on generic or manually tuned distance metrics for estimating the similarity of potential duplicates. In this paper, we present a framework for improving duplicate detection using trainable measures of textual similarity. We propose to employ learnable text distance functions for each database field, and show that such measures are capable of adapting to the specific notion of similarity that is appropriate for the field's domain. We present two learnable text similarity measures suitable for this task: an extended variant of learnable string edit distance, and a novel vector-space based measure that employs a Support Vector Machine (SVM) for training. Experimental results on a range of datasets show that our framework can improve duplicate detection accuracy over traditional techniques.},
	booktitle = {Proceedings of the {Ninth} {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {ACM},
	author = {Bilenko, Mikhail and Mooney, Raymond J.},
	year = {2003},
	keywords = {data cleaning, distance metric learning, record linkage, string edit distance, SVM applications, trained similarity measures},
	pages = {39--48}
}

@article{tancredi_2015_regression,
  title={Regression analysis with linked data: problems and possible solutions},
  author={Tancredi, Andrea and Liseo, Brunero},
  journal={Statistica},
  volume={75},
  number={1},
  pages={19--35},
  year={2015}
}

@article{konda2016magellan,
  title={Magellan: Toward building entity matching management systems},
  author={Konda, Pradap and Das, Sanjib and Suganthan GC, Paul and Doan, AnHai and Ardalan, Adel and Ballard, Jeffrey R and Li, Han and Panahi, Fatemah and Zhang, Haojun and Naughton, Jeff and others},
  journal={Proceedings of the VLDB Endowment},
  volume={9},
  number={12},
  pages={1197--1208},
  year={2016},
  publisher={VLDB Endowment}
}

@inproceedings{das2017falcon,
  title={Falcon: Scaling up hands-off crowdsourced entity matching to build cloud services},
  author={Das, Sanjib and GC, Paul Suganthan and Doan, AnHai and Naughton, Jeffrey F and Krishnan, Ganesh and Deep, Rohit and Arcaute, Esteban and Raghavendra, Vijay and Park, Youngchoon},
  booktitle={Proceedings of the 2017 ACM International Conference on Management of Data},
  pages={1431--1446},
  year={2017},
  organization={ACM}
}

@article{price2015documents,
  title={Documents of war: Understanding the Syrian conflict},
  author={Price, Megan and Gohdes, Anita and Ball, Patrick},
  journal={Significance},
  volume={12},
  number={2},
  pages={14--19},
  year={2015},
  publisher={Wiley Online Library}
}

@article{chen2018unique,
  title={Unique entity estimation with application to the {S}yrian conflict},
  author={Chen, Beidi and Shrivastava, Anshumali and Steorts, Rebecca C},
  journal={The Annals of Applied Statistics},
  volume={12},
  number={2},
  pages={1039--1067},
  year={2018},
  publisher={Institute of Mathematical Statistics} }

@article{copas1990record,
  title={Record linkage: statistical models for matching computer records},
  author={Copas, JB and Hilton, FJ},
  journal={Journal of the Royal Statistical Society. Series A (Statistics in Society)},
  pages={287--320},
  year={1990},
  publisher={JSTOR}
}

@Manual{mcmcse,
    title = {mcmcse: Monte Carlo Standard Errors for MCMC},
    author = {James M. Flegal and John Hughes and Dootika Vats and Ning Dai},
    year = {2017},
    address = {Riverside, CA, Denver, CO, Coventry, UK, and Minneapolis, MN},
    note = {R package version 1.3-2},
  }

@article{hof12,
	Author = {Hof, M. H. P. and Zwinderman, A. H.},
	Journal = {Statistics in Medicine},
	Number = {30},
	Pages = {4231--4242},
	Publisher = {Wiley Online Library},
	Title = {Methods for analyzing data from probabilistic linkage strategies based on partially identifying variables},
	Volume = {31},
	Year = {2012}}
	
@article{DalzellReiter18,
	Author = {Nicole M. Dalzell and Jerome P. Reiter},
	Date-Added = {2018-09-02 00:37:26 +0000},
	Date-Modified = {2018-09-02 00:41:45 +0000},
	Journal = {Journal of Computational and Graphical Statistics},
	Number = {0},
	Pages = {1-11},
	Title = {Regression Modeling and File Matching Using Possibly Erroneous Matching Variables},
	Volume = {0},
	Year = {2018}}	
	
@inproceedings{steorts2018generalized,
	Author = {Steorts, Rebecca C and Tancredi, Andrea and Liseo, Brunero},
	Booktitle = {International Conference on Privacy in Statistical Databases},
	Organization = {Springer},
	Pages = {297--313},
	Title = {Generalized Bayesian Record Linkage and Regression with Exact Error Propagation},
	Year = {2018}}	


@article{gutman2013bayesian,
	Author = {Gutman, Roee and Afendulis, Christopher C and Zaslavsky, Alan M},
	Journal = {Journal of the American Statistical Association},
	Number = {501},
	Pages = {34--47},
	Publisher = {Taylor \& Francis Group},
	Title = {A Bayesian procedure for file linking to analyze end-of-life medical costs},
	Volume = {108},
	Year = {2013}}

@article{HofRavelliZwinderman17,
	Author = {Michel H. Hof and Anita C. Ravelli and Aeilko H. Zwinderman To},
	Date-Added = {2018-09-02 00:56:38 +0000},
	Date-Modified = {2018-09-02 01:02:12 +0000},
	Journal = {Journal of the American Statistical Association},
	Number = {520},
	Pages = {1504-1515},
	Title = {A Probabilistic Record Linkage Model for Survival Data},
	Volume = {112},
	Year = {2017}}
	
@incollection{vatsalan_2013,
  title={Sorted nearest neighborhood clustering for efficient private blocking},
  author={Vatsalan, Dinusha and Christen, Peter},
  booktitle={Advances in Knowledge Discovery and Data Mining},
  pages={341--352},
  year={2013},
  publisher={Springer}
}

@article{goldenberg_2010,
  title={A survey of statistical network models},
  author={Goldenberg, Anna and Zheng, Alice X and Fienberg, Stephen E and Airoldi, Edoardo M},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={2},
  number={2},
  pages={129--233},
  year={2010},
  publisher={Now Publishers Inc.}
}

@inproceedings{christen_2009,
  title={Similarity-aware indexing for real-time entity resolution},
  author={Christen, Peter and Gayler, Ross and Hawking, David},
  booktitle={Proceedings of the 18th ACM Conference on Information and Knowledge Management},
  pages={1565--1568},
  year={2009}
}

@inproceedings{WYP:2010,
	Author = {W.E. Winkler and  W.E. Yancey and E.H. Porter },
	Booktitle = {Proceedings of American Statistical Association Section on Survey Research Methods },
	Date-Added = {2014-05-16 14:56:15 +0000},
	Date-Modified = {2014-05-16 14:58:20 +0000},
	Title = {Fast Record Linkage of Very Large Files in Support of Decennial and Administrative Records Projects},
	Year = {2010}}

@article{Herzog:2010,
	Author = {Herzog, T.N. and Scheuren, F.J. and Winkler, W.E.},
	Date-Added = {2014-05-16 14:52:18 +0000},
	Date-Modified = {2014-05-16 14:55:13 +0000},
	Journal = {Wiley Interdisciplinary Reviews: Computational Statistics },
	Pages = {DOI: 10.1002/wics.108},
	Title = {Record Linkage},
	Volume = {2},
	Year = {2010}}

@article{BMCRF:2003,
	Author = {M. Bilenko and  R. Mooney and  W.W. Cohen and  P. Ravikumar and S.E. Fienberg },
	Date-Added = {2014-05-16 14:49:37 +0000},
	Date-Modified = {2014-05-16 14:55:52 +0000},
	Journal = {IEEE Intelligent Systems},
	Number = {5},
	Pages = {16--23},
	Title = {Adaptive Name-Matching in Information Integration },
	Volume = {18},
	Year = {2003}}


@article{Ugarte2014,
title={On fitting spatio-temporal disease mapping models using approximate Bayesian inference},
author= {Ugarte, Maria Dolores, and Adin, Aritz, and Goicoa, Tom\'as and Militino,  Ana F.},
journal={Statistical Methods in Medical Research},
pages={DOI: 10.1177/0962280214527528},
year= {2014}
}



@article{militino_2012,
title={Estimating the percentage of food expenditure in small areas using bias-corrected P-spline based estimators},
author= {Militino,  Ana F., and Goicoa, Tom\'as  and Ugarte, Maria Dolores},
journal={Computational Statistics and Data Analysis},
volume={53},
  pages={3616- 3629},
year= {2012}
}


@inproceedings{steorts_2014_louis,
  title={Conformability for Bayesian Estimates},
  author={Steorts, R. and Louis, T.},
  year={2014},
  organization={In preparation}
}

@inproceedings{hernandez_1995,
    author = {Hern\'{a}ndez, Mauricio A. and Stolfo, Salvatore J.},
    title = {The Merge/Purge Problem for Large Databases},
    year = {1995},
    isbn = {0897917316},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    doi = {10.1145/223784.223807},
    booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
    pages = {127–138},
    numpages = {12},
    location = {San Jose, California, USA},
}
  


@inproceedings{mccallum_2000,
  title={Efficient clustering of high-dimensional data sets with application to reference matching},
  author={McCallum, Andrew and Nigam, Kamal and Ungar, Lyle H},
  booktitle={Proceedings of 6th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages={169--178},
  year={2000}
}


@book{lewis_1997,
  title={Elements of the Theory of Computation},
  author={Lewis, Harry R and Papadimitriou, Christos H},
  year={1997},
  publisher={Prentice Hall PTR}
}

@article{kullback_1951,
  title={On information and sufficiency},
  author={Kullback, Solomon and Leibler, Richard A},
  journal={The Annals of Mathematical Statistics},
  pages={79--86},
  year={1951} 
}

@article{pasula_2003,
  title={Identity uncertainty and citation matching},
  author={Pasula, Hanna and Marthi, Bhaskara and Milch, Brian and Russell, Stuart and Shpitser, Ilya},
  journal={Advances in Neural Information Processing Systems},
  pages={1425--1432},
  year={2003} 
}

@article{ban_2005,
  title={Clustering with Bregman Divergences},
  author={Banerjee, Arindam and Merugu, Srujana and Dhillon, Inderjit and Ghosh, Joydeep},
  journal={Journal of Machine Learning Research},
  volume={6},
  pages={1705--1749},
  year={2005},
  editor={John Lafferty}
}

@inproceedings{frigyik_2008,
  title={Functional Bregman Divergence and Bayesian
Estimation of Distributions},
  author={Bela, Frigyik and Srivastave, Santosh and Gupta, Maya},
  booktitle={IEEE TRANSACTIONS ON INFORMATION THEORY},
  volume = {54},
  pages={5130--5139},
  year={2008}
}

@inproceedings{vatsalan_2011,
  title={An efficient two-party protocol for approximate matching in private record linkage},
  author={Vatsalan, Dinusha and Christen, Peter and Verykios, Vassilios S},
  booktitle={Proceedings of the Ninth Australasian Data Mining Conference-Volume 121},
  pages={125--136},
  year={2011}
}

@inproceedings{kuzu_2011,
  title={A constraint satisfaction cryptanalysis of Bloom filters in private record linkage},
  author={Kuzu, Mehmet and Kantarcioglu, Murat and Durham, Elizabeth and Malin, Bradley},
  booktitle={Privacy Enhancing Technologies},
  pages={226--245},
  year={2011},
  organization={Springer}
}

@inproceedings{karakasidis_2012,
  title={Reference table based $k$-anonymous private blocking},
  author={Karakasidis, Alexandros and Verykios, Vassilios S},
  booktitle={Proceedings of the 27th Annual ACM Symposium on Applied Computing},
  pages={859--864},
  year={2012} 
}

@article{singh_1994,
  title={Time series {EBLUP}s for small areas using survey data},
  author={Singh, AC and Mantel, HJ and Thomas, BW},
  journal={Survey Methodology},
  volume={20},
  number={1},
  pages={33--43},
  year={1994}
}

@article{souza_2009,
  title={Small area population prediction via hierarchical models},
  author={Souza, Debora F and Moura, Fernando AS and Migon, Helio S},
  journal={Catalogue no. 12-001-X},
  pages={203},
  year={2009}
}

@article{pratesi_2008,
  title={Small area estimation: the EBLUP estimator based on spatially correlated random area effects},
  author={Pratesi, Monica and Salvati, Nicola},
  journal={Statistical methods and applications},
  volume={17},
  number={1},
  pages={113--141},
  year={2008},
  publisher={Springer}
}

@article{datta_1999,
  title={Hierarchical Bayes estimation of unemployment rates for the states of the US},
  author={Datta, Gauri S and Lahiri, Partha and Maiti, Tapabrata and Lu, Kim L},
  journal={Journal of the American Statistical Association},
  volume={94},
  number={448},
  pages={1074--1082},
  year={1999},
  publisher={Taylor \& Francis Group}
}

@inproceedings{datta_1996,
  title={Estimation of median income of four-person families: a Bayesian approach},
  author={Datta, GS and Ghosh, M and Nangia, Narinder and Natarajan, K},
  booktitle={Bayesian analysis in statistics and econometrics: essays in honor of Arnold Zellner},
  pages={129--140},
  year={1996}
}


@article{stodden_2013,
  title={“Setting the Default to Reproducible” in Computational Science Research},
  author={Stodden, Victoria and Borwein, Jonathan and Bailey, David H},
  journal={SIAM News, June},
  volume={3},
  year={2013}
}

@online{molina_2013,
  author = {Molina, Isabel and Marhenda, Yolada},
  title = {{Package sae}},
  year = 2013,
  url = {http://cran.r-project.org/web/packages/sae/sae.pdf},
  urldate = {2010-09-30}
}

@article{berend_2014,
  title={Minimum KL-divergence on complements of $ L\_1 $ balls},
  author={Berend, Daniel and Harremoes, Peter and Kontorovich, Aryeh},
  publisher={IEEE},
  year={2014}
}

@article{louis_2014,
	Author = {Steorts, Rebecca and Louis, T.A.},
	Date-Added = {2010-04-22 16:04:05 -0400},
	Date-Modified = {2010-04-22 16:04:05 -0400},
	Journal = {In Preparation},
	Title = {Constrained Bayesian Benchmarking},
	Year = {2014}}

@artile{pfeffermann_2014,
title={SINGLE AND TWO-STAGE CROSS-SECTIONAL AND TIME SERIES BENCHMARKING PROCEDURES FOR SMALL AREA ESTIMATION},
author= {Pfeffermann, Danny and Sikov, Anna and Tiller, Richard},
journal={TEST},
year= {2014}
}

@article{hall2011_random,
  title={Random differential privacy},
  author={Hall, Rob and Rinaldo, Alessandro and Wasserman, Larry},
  journal={arXiv preprint arXiv:1112.2680},
  year={2011}
}

@article{hall_2011,
  title={Secure multiple linear regression based on homomorphic encryption},
  author={Hall, Rob and Fienberg, Stephen E and Nardi, Yuval},
  journal={Journal of Official Statistics},
  volume={27},
  number={4},
  pages={669},
  year={2011}
}

@book{Davison-Hinkley-bootstrap,
	author = {A. C. Davison and D. V. Hinkley},
	title = {Bootstrap Methods and their Applications},
	address = {Cambridge, England},
	publisher = {Cambridge University Press},
	year = 1997}

@inproceedings{price_2013,
  title={Full updated statistical analysis of documentation of killing in the {S}yrian {A}rab {R}epulic},
  author={Price, M. and Klinger, J. and Qtiesh, A. and Ball, P.},
  year={2013},
  organization={Human Rights Data Analysis Group, commissioned by the United Nations Office of the High Commissioner for Human Rights (OHCHR) }
}

@phdthesis{durham_2012,
  title={A framework for accurate, efficient private record linkage},
  author={Durham, Elizabeth Ashley},
  year={2012},
  school={Vanderbilt University}
}

@inproceedings{hall_2011,
  title={Privacy-preserving record linkage},
  author={Hall, Rob and Fienberg, Stephen E},
  booktitle={Privacy in Statistical Databases},
  pages={269--283},
  year={2011},
  organization={Springer}
}

@article{pauleve_2010,
  title={Locality sensitive hashing: A comparison of hash function types and querying mechanisms},
  author={Paulev{\'e}, Lo{\"\i}c and J{\'e}gou, Herv{\'e} and Amsaleg, Laurent},
  journal={Pattern Recognition Letters},
  volume={31},
  number={11},
  pages={1348--1358},
  year={2010}
}


@article{steorts_2014_spatial,
title={{Spatial Smoothing and Clustering for Small Area Estimation }},
author={Steorts, R. and Shalizi, C.},
journal={In preparation},
year={2014}
}

@article{wehbe_2014,
title={{Regularized Brain Reading with
Shrinkage and Smoothing }},
author={Wehbe, L. and Ramdas, A. and Steorts, R. and Shalizi, C.},
journal={Submitted},
year={2014}
}





@article{kass_1996,
  title={The selection of prior distributions by formal rules},
  author={Kass, Robert E and Wasserman, Larry},
  journal={Journal of the American Statistical Association},
  volume={91},
  number={435},
  pages={1343--1370},
  year={1996},
  publisher={Taylor \& Francis Group}
}

@article{lai_2011,
  title={Disambiguation and co-authorship networks of the US Patent Inventor Database},
  author={Lai, Ronald and D’Amour, Alexander and Yu, Amy and Sun, Ye and Torvik, Vetle and Fleming, Lee},
  journal={Harvard Institute for Quantitative Social Science, Cambridge, MA},
  volume={2138},
  year={2011}
}

@article{carayol_2009,
  title={Who’s Who in Patents. A Bayesian approach},
  author={Carayol, Nicolas and Cassi, Lorenzo},
  journal={Cahiers du GREThA},
  volume={7},
  pages={2009--07},
  year={2009},
  publisher={Groupe de Recherche en Economie Th{\'e}orique et Appliqu{\'e}e}
}

@inproceedings{christen_2014,
title = {Noise-Tolerant Approximate Blocking for Dynamic Real-time Entity Resolution },
author = {Liang, Huizhi and Wang, Yanzhe and Christen, Peter and Gayler, Ross},
booktitle = {18th Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD14)},
pages = {forthcoming},
year={2014},
editor = {Zhou, Zhi-Hua and Chen, Arbee L. P. and Tseng, Vincent S. and Tu, Bao Ho},
publisher = {Springer}
}


@article{clauset_2004,
  title={Finding community structure in very large networks},
  author={Clauset, Aaron and Newman, Mark EJ and Moore, Cristopher},
  journal={Physical Review E},
  volume={70},
  number={6},
  pages={066111},
  year={2004} 
} 

@article{fortunato_2010,
  title={Community detection in graphs},
  author={Fortunato, Santo},
  journal={Physics Reports},
  volume={486},
  number={3},
  pages={75--174},
  year={2010} 
}

@inproceedings{winkler_2006,
  title={Overview of record linkage and current research directions},
  author={Winkler, William E},
  booktitle={Bureau of the Census},
  year={2006},
  organization={Citeseer}
}

@Manual{R_2013,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2013},
    url = {http://www.R-project.org/},
  }

 @Manual{borg_2012,
    title = {RecordLinkage: Record Linkage in R},
    author = {Andreas Borg and Murat Sariyar},
    year = {2012},
    note = {R package version 0.4-1},
    url = {http://CRAN.R-project.org/package=RecordLinkage},
  }


@article{kleiner_2011,
  title={A scalable bootstrap for massive data},
  author={Kleiner, Ariel and Talwalkar, Ameet and Sarkar, Purnamrita and Jordan, Michael I},
  journal={arXiv preprint arXiv:1112.5016},
  year={2011}
}


@inproceedings{al_2005,
  title={Blocking-aware private record linkage},
  author={Al-Lawati, Ali and Lee, Dongwon and McDaniel, Patrick},
  booktitle={Proceedings of the 2nd international workshop on Information quality in information systems},
  pages={59--68},
  year={2005} 
}
@article{sarma_2011,
  title={CBLOCK: An Automatic Blocking Mechanism for Large-Scale De-duplication Tasks},
  author={Sarma, Anish Das and Jain, Ankur and Machanavajjhala, Ashwin and Bohannon, Philip},
  journal={arXiv preprint arXiv:1111.3689},
  year={2011}
}

@article{breiman_2001,
  title={Random forests},
  author={Breiman, Leo},
  journal={Machine Learning},
  volume={45},
  number={1},
  pages={5--32},
  year={2001},
  publisher={Springer}
}

@article{christen_2012b,
  title={A survey of indexing techniques for scalable record linkage and deduplication},
  author={Christen, Peter},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={24},
  number={9},
  pages={1537--1555},
  year={2012}
}


@inproceedings{gan_2012,
  title={Locality-sensitive hashing scheme based on dynamic collision counting},
  author={Gan, Junhao and Feng, Jianlin and Fang, Qiong and Ng, Wilfred},
  booktitle={Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data},
  pages={541--552},
  year={2012} 
}


@inproceedings{das_2012,
  title={An automatic blocking mechanism for large-scale de-duplication tasks},
  author={Das Sarma, Anish and Jain, Ankur and Machanavajjhala, Ashwin and Bohannon, Philip},
  booktitle={Proceedings of the 21st ACM international conference on Information and knowledge management},
  pages={1055--1064},
  year={2012} 
}


@book{rajaraman_2012,
  title={Mining of Massive Datasets},
  author={Rajaraman, Anand and Ullman, Jeffrey David},
  year={2012},
  publisher={Cambridge University Press}
}

@article{tantrum_2004,
  title={Hierarchical model-based clustering of large datasets through fractionation and refractionation},
  author={Tantrum, Jeremy and Murua, Alejandro and Stuetzle, Werner},
  journal={Information Systems},
  volume={29},
  number={4},
  pages={315--326},
  year={2004} 
}

@inproceedings{cutting_1992,
  title={Scatter/gather: A cluster-based approach to browsing large document collections},
  author={Cutting, Douglass R and Karger, David R and Pedersen, Jan O and Tukey, John W},
  booktitle={Proceedings of the 15th annual international ACM SIGIR conference on Research and development in information retrieval},
  pages={318--329},
  year={1992},
  organization={ACM}
}

@phdthesis{ventura_2013,
AUTHOR = "Ventura, Sam",
TITLE = "Large-Scale Clustering Methods with Applications to
Record Linkage",
SCHOOL = "CMU",
YEAR = "2013",
type = "PhD Thesis Proposal",
address = "Pittsburgh, PA",
}

@article{pfeffermann_2013,
  title={New important developments in small area estimation},
  author={Pfeffermann, Danny},
  journal={Statistical Science},
  volume={28},
  number={1},
  pages={40--68},
  year={2013},
  publisher={Institute of Mathematical Statistics}
}

@article{wainwright_2008,
  title={Graphical models, exponential families, and variational inference},
  author={Wainwright, Martin J and Jordan, Michael I},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={1},
  number={1-2},
  pages={1--305},
  year={2008},
  publisher={Now Publishers Inc.}
}

@article{broderick_2012,
  title={MAD-Bayes: MAP-based asymptotic derivations from Bayes},
  author={Broderick, Tamara and Kulis, Brian and Jordan, Michael I},
  journal={arXiv preprint arXiv:1212.2126},
  year={2012}
}

@article{broderick_2013,
  title={Streaming Variational Bayes},
  author={Broderick, Tamara and Boyd, Nicholas and Wibisono, Andre and Wilson, Ashia C and Jordan, Michael I},
  journal={arXiv preprint arXiv:1307.6769},
  year={2013}
}

@article{quantin_1998,
  title={Automatic record hash coding and linkage for epidemiological follow-up data confidentiality},
  author={Quantin, C and Bouzelat, H and Allaert, FA and Benhamiche, AM and Faivre, J and Dusserre, L},
  journal={Methods of information in medicine},
  volume={37},
  number={3},
  pages={271--277},
  year={1998},
  publisher={Schattauer}
}

@inproceedings{kim_2010,
  title={HARRA: fast iterative hashed record linkage for large-scale data collections},
  author={Kim, Hung-sik and Lee, Dongwon},
  booktitle={Proceedings of the 13th International Conference on Extending Database Technology},
  pages={525--536},
  year={2010},
  organization={ACM}
}



@inproceedings{inan_2010,
  title={Private Record Matching Using Differential Privacy},
  author={Inan, Ali and Kantarcioglu, Murat and Ghinita, Gabriel and Bertino, Elisa},
  booktitle={Proceedings of the 13th International Conference on Extending Database Technology},
  pages={123--134},
  year={2010} 
}

@inproceedings{gionis_1999,
  title={Similarity search in high dimensions via hashing},
  author={Gionis, Aristides and Indyk, Piotr and Motwani, Rajeev and others},
  booktitle={VLDB},
  volume={99},
  pages={518--529},
  year={1999}
}

@article{torvik_2009,
  title={Author name disambiguation in MEDLINE},
  author={Torvik, Vetle I and Smalheiser, Neil R},
  journal={ACM Transactions on Knowledge Discovery from Data (TKDD)},
  volume={3},
  number={3},
  pages={11},
  year={2009} 
}

@inproceedings{giles_2009,
  title={Disambiguating authors in academic publications using random forests},
  author={Treeratpituk, Pucktada and Giles, C Lee},
  booktitle={Proceedings of the 9th ACM/IEEE-CS joint conference on Digital libraries},
  pages={39--48},
  year={2009} 
}

@inproceedings{han_2004,
  title={Two supervised learning approaches for name disambiguation in author citations},
  author={Han, Hui and Giles, Lee and Zha, Hongyuan and Li, Cheng and Tsioutsiouliklis, Kostas},
  booktitle={Digital Libraries, 2004. Proceedings of the 2004 Joint ACM/IEEE Conference on},
  pages={296--305},
  year={2004} 
}

@techreport{winkler_2002,
  title={Methods for record linkage and Bayesian networks},
  author={Winkler, William E},
  year={2002},
  institution={Technical report, Statistical Research Division, US Census Bureau, Washington, DC}
}


@inproceedings{christen_2006,
  title={Privacy-preserving data linkage and geocoding: Current approaches and research directions},
  author={Christen, Peter},
  booktitle={Data Mining Workshops, 2006. ICDM Workshops 2006. 6th IEEE International Conference on},
  pages={497--501},
  year={2006} 
}


@article{steorts_2013b,
  title={A {B}ayesian Approach to Graphical Record Linkage and De-duplication},
  author={Steorts, R. C. and Hall, Rob and Fienberg, S.E.},
  journal={Submitted},
  year={2013}
}

@article{ventura2015seeing,
  title={Seeing the non-stars:(some) sources of bias in past disambiguation approaches and a new public tool leveraging labeled records},
  author={Ventura, Samuel L and Nugent, Rebecca and Fuchs, Erica RH},
  journal={Research Policy},
  volume={44},
  number={9},
  pages={1672--1701},
  year={2015},
  publisher={Elsevier}
}

@inproceedings{treeratpituk2009disambiguating,
  title={Disambiguating authors in academic publications using random forests},
  author={Treeratpituk, Pucktada and Giles, C Lee},
  booktitle={Proceedings of the 9th ACM/IEEE-CS joint conference on Digital libraries},
  pages={39--48},
  year={2009}
}

@misc{west2008pseudocode,
  title={Pseudocode for calculating Eigenfactor TM Score and Article Influence TM Score using data from Thomson-Reuters Journal Citations Reports},
  author={West, Jevin and Bergstrom, Carl T},
  year={2008},
  publisher={mimeo, University of Washington}
}

@article{ventura_2012,
  title={Methods matter: Revamping inventor disambiguation algorithms with classification models and labeled inventor records},
  author={Ventura, Samuel L and Nugent, Rebecca and Fuchs, Erica},
  journal={SSRN eLibrary},
  year={2012}
}

@article{reiter_2007,
  title={The multiple adaptations of multiple imputation},
  author={Reiter, Jerome P and Raghunathan, Trivellore E},
  journal={Journal of the American Statistical Association},
  volume={102},
  number={480},
  pages={1462--1471},
  year={2007},
  publisher={Taylor \& Francis}
}

@article{ashburner2008spm8,
 title={SPM8 manual},
 author={Ashburner, J. and Chen, CC and Flandin, G. and Henson, R. and Kiebel, S. and Kilner, J. and Litvak, V. and Moran, R. and Penny, W. and Stephan, K. and others},
 journal={Functional Imaging Laboratory, Institute of Neurology},
 year={2008}
}

@book{gelman_2003,
	author = "Andrew Gelman and John B. Carlin and Hal S. Stern and Donald B. Rubin",
	title = "{Bayesian} Data Analysis",
	edition = "Second",
	address = "London",
	publisher = "CRC Press",
	year = 2003}


@book{hastie_2001,
 title={The Elements of Statistical Learning: Data Mining, Inference, and Prediction},
 author={Trevor Hastie and Robert Tibshirani and Jerome Friedman},
 year={2001},
 publisher={Springer},
 address ={New York}
}

@article{norman_2006,
  title = "Beyond mind-reading: multi-voxel pattern analysis of {fMRI} data",
  author = "Kenneth A. Norman and Sean M. Polyn and Greg J. Detre and James V. Haxby",
  journal = "Trends in Cognitive Sciences",
  volume = 10,
  year = 2006,
  pages = "424--430",
  doi = "10.1016/j.tics.2006.07.005"}

@article{pereira_2009,
  author = "Francisco Pereira and Tom Mitchell and Matthew Botvinick",
  title = "Machine learning classifiers and {fMRI}: {A} tutorial overview",
  journal = "NeuroImage",
  volume = 45,
  year = 2009,
  pages = "S199--S209",
  doi = "10.1016/j.neuroimage.2008.11.007"}

@article{yarkoni_2011,
  author = "Tal Yarkoni and Russell A. Poldrack and Thomas E. Nichols and Van Essen, David C. and Tor D. Wager",
  title = "Large-scale automated synthesis of human functional neuroimaging data",
  journal = "Nature Methods",
  volume = 8,
  year = 2011,
  pages = "665--670",
  doi = "10.1038/nmeth.1635"}

@article{poldrack_2008,
  author="Russell A. Poldrack",
  title = "The role of {fMRI} in Cognitive Neuroscience: where do we stand?",
  journal = "Current Opinion in Neurobiology",
  volume = 18,
  year = 2008,
  pages = "223--227",
  doi = "10.1016/j.conb.2008.07.006"}


@book{bilenko_2006,
  title={Learnable similarity functions and their application to record linkage and clustering},
  author={Bilenko, Mikhail Yuryevich},
  volume={67},
  number={12},
  year={2006}
}

@book{ashby_2011,
  author={F. Gregory Ashby},
  title ={Statistical Analysis of {fMRI} Data},
  address = {Cambridge, Massachusetts},
  publisher ={MIT Press},
  year={2011}}

@book{gigerenzer_1989,
  title={The Empire of Chance: How Probability Changed Science and Everyday Life},
  author = {Gerd Gigerenzer and Zeno Swijtink and Theodore Porter and Lorraine Daston and John Beatty and Lorenz Kruger},
  address = {Cambridge, England},
  publisher = {Cambridge University Press},
  year = {1989}}

@misc{lee_2011,
  title={Spatial Bayesian Variable Selection Models on Functional Magnetic Resonance Imaging Time-Series Data},
  author={Lee, Kuo-Jung and Jones, Galin L. and Caffo, Brian S. and Bassett, Susan Spear},
  howpublishedl={Preprint},
  year={2011}
}

@incollection{pillow_2013,
  title={Bayesian Structure Learning for Functional Neuroimaging},
 author={Park, Mijung and Koyejo, Oluwasanmi and Ghosh, Joydeep and Poldrack, Russell A and Pillow, Jonathan W},
  booktitle={16th International Conference on Artificial Intelligence and Statistics},
  year={2013},
  editor = {Carlos M. Carlvaho and Pradeep Ravikumar},
  pages = {489--497},
}

@article{genovese_2000,
 title={A {Bayesian} time-course model for functional magnetic resonance imaging data},
 author={Genovese, Christopher R},
 journal={JASA},
 volume={95},
 pages={691--703},
 year={2000},
 publisher={Taylor \& Francis Group}
}

@misc{lee_2013,
  title={Spatial {B}ayesian selection models on functional magnetic resonance imaging time series data},
  author={Lee, K.-J. and Jones, G. L. and  Caffo, B. S. and Bassett, S.},
  journal={Bayesian Analysis},
  year={2013},
  publisher={International Society for Bayesian Analysis}
}




@article{kyung_2010,
  title={Penalized regression, standard errors, and Bayesian lassos},
  author={Kyung, Minjung and Gill, Jeff and Ghosh, Malay and Casella, George},
  journal={Bayesian Analysis},
  volume={5},
  pages={369--411},
  year={2010},
  publisher={International Society for Bayesian Analysis}
}

@article{hoerl_1970,
  title={Ridge regression: Biased estimation for nonorthogonal problems},
  author={Hoerl, Arthur E and Kennard, Robert W},
  journal={Technometrics},
  volume={12},
  pages={55--67},
  year={1970},
  publisher={Taylor \& Francis Group}
}

@article{tibshirani_1996,
  title={Regression shrinkage and selection via the lasso},
  author={Tibshirani, Robert},
  journal={Journal of the Royal Statistical Society. Series B (Methodological)},
  volume = {58},
  pages={267--288},
  year={1996},
}

@article{zou_2005,
  title={Regularization and variable selection via the elastic net},
  author={Zou, Hui and Hastie, Trevor},
  journal={JRSSB (Statistical Methodology)},
  volume={67},
  pages={301--320},
  year={2005},
  publisher={Wiley Online Library}
}

@book{shepherd_1994,
  title={Neurobiology},
  author={Shepherd, Gordon M},
  edition ={3},
  year={1994},
  publisher={Oxford University Press}
}

@article{fleming_2007,
  title={Small worlds and regional innovation},
  author={Fleming, Lee and King, {III}, Charles and Juda, Adam},
  journal={Organization Science},
  volume={18},
  pages={938--954},
  year={2007},
  publisher={INFORMS}
}



@article{copas_1990,
	Author = {Copas, J. and Hilton, {F.J.}},
	Journal = {Journal of the Royal Statistical Society, Series A},
	Title = {Record Linkage: Statistical Models for Matching Computer Records},
	Volume = {153},
	Number = {3},
	pages =	 {287-320},
	Year = {1990}}	
	
@article{everson_2000,
	Author = {Everson, P. and Morris, C.},
	Journal = {Journal of the Royal Statistical Society, Series B},
	Title = {Inference of multivariate normal hierarchical models},
	Volume = {62},
	pages =	 {399-412},
	Year = {2000}}	

@article{cowles_1996,
	Author = {Cowles, M. and Carlin, B.},
	Journal = {Journal of the American Statistical Association},
	Title = {Markov chain {M}onte {C}arlo Convergence Diagnostics: A Comparative Review},
	Volume = {91},
	Number= {434},
	Pages = {883-904},
	Year = {1996}
	}

@article{datta_1991,
	Author = {Datta, G. and Ghosh, M.},
	Journal = {The Annal of Statistics},
	Title = {Bayesian Prediction in Linear Models: Applications to Small Area Estimation},
	Volume = {19},
	Number= {4},
	Pages = {1748-1770},
	Year = {1991}
	}

@article{gates_2011,
	Author = {Gates, W.},
	Journal = {Journal of Privacy and Confidentiality},
	Pages = {3-40},
	Title = {How Uncertainty about Privacy and Confidentiality Is Hampering Efforts to More Effectively Use Administrative Records in Producing {U.S.} National Statistics},
	Volume = {10},
	Number= {905},
	Year = {2011}
	}
	
@article{wheaton_2009,
	Author = {Wheaton, W. and Cajka, J. and Chasteen, B. and Wagener, D. and Cooley, P. and Ganapathi, L. and Roberts, D. and Allpress, J.},
	Journal = {Methods Rep RTI Press},
	Title = {Synthesized Population Databases: A U.S. Geospatial Database for Agent-Based Models},
	Volume = {3},
	Number= {2},
	Year = {2009},
	url = {doi:  10.3768/rtipress.2009.mr.0010.0905}
	}
	
@article{beckman_1996,
	Author = {Beckman, R. and Baggerly, K. and Mc{K}ay, D.},
	Journal = {Transportation Research Part A: Policy and Practice},
	Title = {Creating synthetic baseline populations},
	Volume = {30},
	Number= {6},
	Year = {1996},
	Pages = {415-429}
	}	
	
	

@book{acs_2007,
	Address = {Using the American Community Survey, National Research Council},
	Author = {Using the American Community Survey, National Research Council},
	Date-Added = {2013-05-30 03:49:33 +0000},
	Date-Modified = {2013-05-30 03:51:42 +0000},
	Publisher = {Springer},
	Title = {Using the American Community Survey Ð Benefits and Challenges, Panel on the Functionality and Usability of Data from the American Community Survey},
	Year = {2007}}	
	
@misc{midas_2012,
	Author = {MIDAS},
	Title = {Models of Infectious Disease Agent Study},
	Date-Modified = {2013-05-30 04:51:43 +0000},
	Url = {https://www.epimodels.org/midas/Rpubsyntdata1.do},
	Year = 2012,
	Bdsk-Url-1 = {https://www.epimodels.org/midas/Rpubsyntdata1.do}	}
	

@article{battese_1988,
	Author = {Battese, G. and Harter, R. and Fuller, W.},
	Journal = {Journal of the American Statistical Association},
	Pages = {28-36},
	Title = {An Error-Components Model for Prediction of County Crop Area Using Survey and Satellite Data},
	Volume = {83},
	Year = {1988}}
	
@article{ghosh_2009,
	Author = {Ghosh, M. and Kim, D. and Sinha, K. and Maiti,T. and Katzoff, M. and Parsons, V.},
	Date-Added = {2010-04-22 16:04:05 -0400},
	Date-Modified = {2010-04-22 16:04:05 -0400},
	Journal = {Survey Methodology},
	Pages = {53-66},
	Title = {Hierarchical and empirical {B}ayes small domain estimation of the proportion
of persons without health insurance
or minority subpopulations},
	Volume = {35},
	Number = {1},
	Year = {2009}}	
	
@article{ghosh_1998,
	Author = {Ghosh, M. and Natarajan, K. and Stroud, T. and Carlin, B.},
	Date-Added = {2010-04-22 16:04:05 -0400},
	Date-Modified = {2010-04-22 16:04:05 -0400},
	Journal = {Journal of the American Statistical Association},
	Pages = {53-66},
	Title = {Generalized Linear Models for Small Area-Estimation},
	Volume = {93},
	Number = {441},
	Year = {1998}}			
	
	
@article{isaki_2004,
	Author = {Isaki, C.T. and Tsay, J.H and Fuller, W.A.},
	Date-Added = {2010-04-22 16:04:05 -0400},
	Date-Modified = {2010-04-22 16:04:05 -0400},
	Journal = {Survey Methodology},
	Pages = {35-44},
	Title = {Weighting sample data subject 
to independent controls},
	Volume = {20},
	Year = {2004}}			

@article{liseo_jos_2011,
	Author = {Liseo, B. and Tancredi, A.},
	Date-Added = {2013-05-30 03:56:51 +0000},
	Date-Modified = {2013-05-30 03:58:16 +0000},
	Journal = {Journal of Official Statistics},
	Number = {3},
	Pages = {491--505},
	Title = {Bayesian estimation of population size via linkage of multivariate Normal data sets},
	Volume = {27},
	Year = {2011}}
	
@article{christen_2011,
 author =  {Peter Christen},
 title    =   {A Survey of Indexing Techniques for Scalable Record
            Linkage and Deduplication},
 journal = {IEEE Transactions on Knowledge and Data Engineering},
 volume =  24,
 number =  9,
 year =    2012
}
	

@techreport{Winkler_1999,
	Institution = {Statistical Research Division, U.S. Bureau of the Census},
	Author = {W.E. Winkler},
	Date-Added = {2013-05-30 03:43:45 +0000},
	Date-Modified = {2013-05-31 02:21:07 +0000},
	Title = {The state of record linkage and current research problems},
	Type = {Technical Report},
	Year = {1999}}

@article{sadinle_multi_1,
	Author = {Sadinle, M. and Fienberg, S.E.},
	Date-Modified = {2013-05-31 02:24:32 +0000},
	Journal = {Journal of the American Statistical Association},
	Title = {A Generalized {F}ellegi-{S}unter Framework for Multiple Record Linkage with Application to Homicide Record-Systems},
	Volume = {108},
	Number = {502},
	pages = {385-397},
	Year = {2013}}

@misc{sadinle_multi_2,
	Author = {Sadinle, M. and Hall, R. and Fienberg, S.E.},
	Date-Modified = {2013-05-31 02:24:22 +0000},
	Howpublished = {ISI World Congress, Dublin},
	Note = {Invited Paper},
	Title = {Approaches to Multiple Record Linkage},
	Year = {2011}}

@inproceedings{hall12,
	Address = {Berlin},
	Author = {Hall, R. and Fienberg, S.E.},
	Booktitle = {Privacy in Statistical Databases 2012},
	Date-Modified = {2013-05-31 02:32:24 +0000},
	Editor = {J. Domingo-Ferrer and I. Tinnirello},
	Pages = {131--142},
	Publisher = {Springer},
	Series = {Lecture Notes in Computer Science},
	Title = {Valid Statistical Inference on Automatically Matched Files},
	Volume = {7556},
	Year = {2012}}
	

@article{smmcmc,
	Author = {Jain, S. and Neal, R.},
	Journal = {Journal of Computational and Graphical Statistics},
	Pages = {158--182},
	Title = {A Split-Merge Markov Chain Monte Carlo Procedure for the Dirichlet Process Mixture Model},
	Volume = {13},
	Year = {2004}}

@article{zhang_2011,
	Author = {Zhang, S. and Midthune, D. and Guenther, P.M. and Krebs-Smith, S.M. and Kipnis, V. and Dodd, K.W. and Buckman, D.W. and Tooze, J.A. and Freedman, L. and Carroll, R.J.},
	Journal = {Annals of Applied Statistics},
	Number = {2B},
	Pages = {1456-87},
	Title = {A new multivariate measurement error model with zero-inflated dietary data, and its application to dietary assessment},
	Volume = {5},
	Year = {2011}}

@article{carroll_2013,
	Author = {Carroll, R.J.},
	Journal = {Statistical Science},
	Number = {?},
	Pages = {?},
	Title = {Estimating the Distribution of Dietary Consumption Patterns},
	Volume = {?},
	Year = {2013}}

@article{little_1991,
	Author = {Little, R.},
	Journal = {Journal of Official Statistics},
	Number = {4},
	Pages = {405-424},
	Title = {Inference with survey weights},
	Volume = {7},
	Year = {1991}}

@article{fienberg_2009,
	Author = {Fienberg, S.},
	Journal = {Journal of Privacy and Confidentiality},
	Number = {2},
	Title = {The Relevance or Irrelevance of Weights for Confidentiality and Statistical Analyses},
	Volume = {1},
	Year = {2009}}

@article{chaud_2008,
	Author = {Chaudhuri, S. and Handock, M. and Rendall, M.},
	Journal = {Journal of the Royal Statistical Society, B,},
	Number = {2},
	Pages = {311-328},
	Title = {A Conditional empirical likelihood approach for combining sampling design and population level information},
	Volume = {70},
	Year = {2008}}

@article{elliot_2000,
	Author = {Elliot, M. and Little, R.},
	Journal = {Journal of Privacy and Confidentiality},
	Number = {3},
	Pages = {191-209},
	Title = {Model-Based Alternatives to Trimming Survey Weights},
	Volume = {16},
	Year = {2000}}

@article{pfeffermann_1993,
	Author = {Pfeffermann, D.},
	Journal = {International Statistical Review},
	Number = {1},
	Pages = {317-337},
	Title = {The Role of Sampling Weights When Modeling Survey Data},
	Volume = {61},
	Year = {1993}}
	
@article{pfeffermann_2013,
	Author = {Pfeffermann, D.},
	Journal = {Statistical Science},
	Number = {1},
	Pages = {40--68},
	Title = {New Important Developments in Small Area Estimation},
	Volume = {28},
	Year = {2013}}	
	
	

@article{gelman_2007,
	Author = {Gelman, A.},
	Journal = {Statistical Science},
	Number = {2},
	Pages = {153-164},
	Title = {Struggles with survey weighting and regression modeling},
	Volume = {22},
	Year = {2007}}

@article{little_2007,
	Author = {Little, R.},
	Journal = {Statistical Science},
	Number = {2},
	Pages = {171-174},
	Title = {Comment: struggles with survey weighting and regression modeling},
	Volume = {22},
	Year = {2007}}

@article{ugarte_2009,
	Author = {Ugarte, {M. D.} and Goicoa, T. and Militino, {A. F.}},
	Journal = {TEST},
	Pages = {342-364},
	Title = {Benchmarked estimates in small areas using linear mixed models with restrictions.},
	Volume = {18},
	Year = {2009}}

@misc{liseo_2013,
	Author = {B. Liseo and A. Tancredi},
	Date-Modified = {2013-05-30 04:51:43 +0000},
	Title = {Some advances on {B}ayesian record linkage and inference for linked data},
	Url = {http://www.ine.es/e/essnetdi_ws2011/ppts/Liseo_Tancredi.pdf},
	Year = 2013,
	Bdsk-Url-1 = {http://www.ine.es/e/essnetdi_ws2011/ppts/Liseo_Tancredi.pdf}}

@misc{winkler_2000,
	Author = {W.E. Winkler},
	Date-Modified = {2013-05-31 02:23:04 +0000},
	Howpublished = {American Statistical Association, Proceedings of the Section on Survey Research Methods, 20--29},
	Title = {Machine Learning, Information Retrieval, and Record Linkage},
	Url = {http://www.niss.org/affiliates/dqworkshop/papers/winkler.pdf},
	Year = 2000,
	Bdsk-Url-1 = {http://www.niss.org/affiliates/dqworkshop/papers/winkler.pdf}}

@article{fellegi_1969,
	Author = {Fellegi, I. and Sunter, A.},
	Date-Modified = {2013-05-31 02:28:43 +0000},
	Journal = {Journal of the American Statistical Association},
	Number = {328},
	Pages = {1183--1210},
	Title = {A Theory for Record Linkage},
	Volume = {64},
	Year = {1969}}

@article{butar_2003,
	Author = {Butar, F. and Lahiri, P.},
	Journal = {J. Statist. Plann. Inference},
	Pages = {63-76},
	Title = {On measures of uncertainty of empirical Bayes small area estimators.},
	Volume = {112},
	Year = {2003}}

@article{jain_2004,
	Author = {S. Jain and R. Neal},
	Date-Modified = {2013-05-31 02:41:39 +0000},
	Journal = {Journal of Computational and Graphical Statistics},
	Pages = {158--182},
	Title = {A Split-Merge {M}arkov Chain {M}onte {C}arlo Procedure for the {D}irichlet Process Mixture Model},
	Volume = {13},
	Year = {2004}}

@article{bell_2013,
	Author = {Bell, W.R. and Datta, {G.S.} and Ghosh, M.},
	Date-Added = {2010-04-22 16:04:05 -0400},
	Date-Modified = {2010-04-22 16:04:05 -0400},
	Journal = {Biometrika},
	Title = {Benchmarked Small Area Estimators},
	Volume = {100},
	Number={1},
	Pages = {189-202},
	Year = {2013}}
	

@article{ghosh_2008,
	Author = {Ghosh, M. and Mergel, V. and Datta, G.},
	Date-Added = {2010-04-22 16:04:05 -0400},
	Date-Modified = {2010-04-22 16:04:05 -0400},
	Journal = {Journal of Multivariate Analysis},
	Pages = {1941-1961},
	Title = {Estimation, Prediction and the Stein Phenomenon under Divergence Loss},
	Volume = {99},
	Year = {2008}}

@article{bell_1999,
	Author = {Bell, W.R.},
	Date-Added = {2010-04-22 16:04:05 -0400},
	Date-Modified = {2010-04-22 16:04:05 -0400},
	Journal = {Bulletin of the International Statistical Institute, 52nd Session, Helsinki},
	Title = {Accounting for uncertainty about variances in small area estimation},
	Year = {1999}}

@article{bell_2010,
	Author = {Bell, W.R. and Datta, G.S. and Ghosh, M.},
	Date-Added = {2010-04-22 16:04:05 -0400},
	Date-Modified = {2010-04-22 16:04:05 -0400},
	Journal = {Preprint},
	Title = {Benchmarked Small Area Estimators},
	Year = {2010}}

@book{berger_1985,
	Author = {Berger, J.O.},
	Date-Added = {2010-04-22 16:04:05 -0400},
	Date-Modified = {2010-04-22 16:04:05 -0400},
	Publisher = {Springer-Verlag, New York},
	Title = {Statistical Decision Theory and Bayesian Analysis, 2nd Edition},
	Year = {1985}}

@article{datta_2000,
	Author = {Datta, G.S. and Lahiri, P.},
	Date-Added = {2010-04-22 16:04:05 -0400},
	Date-Modified = {2010-04-22 16:04:05 -0400},
	Journal = {Statistica Sinica},
	Pages = {613-627},
	Title = {A unified measure of uncertainty of estimated best linear unbiased predictors in small area estimation problems.},
	Volume = {10},
	Year = {2000}}

@article{datta_2005,
	Author = {Datta, G.S. and Rao, J.N.K. and Smith, D.D.},
	Date-Added = {2010-04-22 16:04:05 -0400},
	Date-Modified = {2010-04-22 16:04:05 -0400},
	Journal = {Biometrika},
	Pages = {183-196},
	Title = {On measuring the variability of small area estimators under a basic area level model},
	Volume = {92},
	Year = {2005}}


@article{datta_2011,
  title={Bayesian benchmarking with applications to small area estimation},
  author={Datta, GS and Ghosh, M and Steorts, R and Maples, J},
  journal={Test},
  volume={20},
  number={3},
  pages={574--588},
  year={2011},
  publisher={Springer}
}

@article{deville_1992,
	Author = {Deville, J.C. and Sarndal, C.E.},
	Date-Added = {2010-04-22 16:04:05 -0400},
	Date-Modified = {2010-04-22 16:04:05 -0400},
	Journal = {Journal of the American Statistical Assocation},
	Number = {418},
	Title = {Calibration Estimators in Survey Sampling},
	Volume = {87},
	Year = {1992}}

@article{ghosh_1992,
	Author = {Ghosh, M.},
	Date-Added = {2010-04-22 16:04:05 -0400},
	Date-Modified = {2010-04-22 16:04:05 -0400},
	Journal = {Journal of the American Stastical Association},
	Pages = {533-540},
	Title = {Constrained Bayes estimation with applications},
	Volume = {87},
	Year = {1992}}

@article{ghosh_2011,
	Author = {Ghosh, M.},
	Journal = {Statistical Science},
	Pages = {187-202},
	Title = {Objective Priors: An Introduction for Frequentists},
	Volume = {26},
	Year = {2011}}

@article{ghosh_2013,
	Author = {Ghosh, M. and Steorts, R.},
	Journal = {TEST},
	Volume = {22},
        Pages = {670-687},
	Title = {Two-Stage Bayesian Benchmarking as Applied to Small Area Estimation},
	Year = {2013}}

@article{steorts_2011,
	Author = {Steorts, R. and Ghosh, M.},
	Journal = {Statistica Sinica},
	Title = {On Estimation of Mean Squared Errors of Benchmarked Empirical Bayes Estimators},
	Volume = {2},
	Year = {2013}}

@article{steorts_2013,
	Author = {Steorts, R.},
	Journal = {Working Paper},
	Title = {Divergence losses under benchmarking for small area estimation},
	Year = {2013}}

@article{isaki_2000,
	Author = {Isaki, C.T. and Tsay, J.H and Fuller, W.A.},
	Date-Added = {2010-04-22 16:04:05 -0400},
	Date-Modified = {2010-04-22 16:04:05 -0400},
	Journal = {Suvery Methodology},
	Pages = {31-42},
	Title = {Estimation of census adjustment factors},
	Volume = {26},
	Year = {2000}}

@article{little_2004,
	Author = {Little, R.J.},
	Journal = {Journal of the American Stastical Association},
	Number = {466},
	Pages = {546-556},
	Title = {To Model or Not to Model? {C}ompeting Modes of Inference for Finite Population Sampling},
	Volume = {99},
	Year = {2004}}

@article{louis_1984,
	Author = {Louis, T.A.},
	Date-Added = {2010-04-22 16:04:05 -0400},
	Date-Modified = {2010-04-22 16:04:05 -0400},
	Journal = {Journal of the American Stastical Association},
	Number = {393-398},
	Title = {Estimating a population of parameter values using Bayes and empirical Bayes methods},
	Volume = {79},
	Year = {1984}}

@article{nandram_2007,
	Author = {Nandram, B. and Toto, Ma. C.S. and Choi, J.W.},
	Date-Added = {2010-04-22 16:04:05 -0400},
	Date-Modified = {2010-04-22 16:04:05 -0400},
	Journal = {Preprint},
	Title = {A Bayesian benchmarking for small areas},
	Year = {2007}}

@article{fuller_2007,
	Author = {Fuller, W.A.},
	Date-Added = {2010-04-22 16:04:05 -0400},
	Date-Modified = {2010-04-22 16:04:05 -0400},
	Journal = {Preprint},
	Title = {Small area prediction subject to restrection},
	Year = {2007}}

@article{fay_1979,
	Author = {Fay, R.E. and Herriot, R.A.},
	Date-Added = {2010-04-22 16:04:05 -0400},
	Date-Modified = {2010-04-22 16:04:05 -0400},
	Journal = {Journal of the American Stastical Association},
	Pages = {269-277},
	Title = {Estimates of income from small places: an application of {James-Stein} procedures to census data},
	Volume = {74},
	Year = {1979}}

@article{pfeffermann_1981,
	Author = {Pfeffermann, D. and Nathan, G.},
	Date-Added = {2010-04-22 16:04:05 -0400},
	Date-Modified = {2010-04-22 16:04:05 -0400},
	Journal = {Journal of the American Stastical Association},
	Pages = {681-689},
	Title = {Regression analysis of data from a cluster sample},
	Volume = {76},
	Year = {1981}}

@article{pfeffermann_1991,
	Author = {Pfeffermann, D. and Barnard, C.H.},
	Date-Added = {2010-04-22 16:04:05 -0400},
	Date-Modified = {2010-04-22 16:04:05 -0400},
	Journal = {Journal of Business and Economic Statistics},
	Pages = {31-42},
	Title = {Some new estimators for small area means with application to the assessment of farmland values},
	Volume = {9},
	Year = {1991}}

@article{pfeffermann_2006,
	Author = {Pfeffermann, D. and Tiller, R.},
	Date-Added = {2010-04-22 16:04:05 -0400},
	Date-Modified = {2010-04-22 16:04:05 -0400},
	Journal = {Journal of the American Stastical Association},
	Pages = {1387-1397},
	Title = {Small area estimation with state-space models subject to benchmark constraints},
	Volume = {101},
	Year = {2006}}

@article{prasad_1990,
	Author = {Prasad, N.G.N. and Rao, J.N.K.},
	Date-Added = {2010-04-22 16:04:05 -0400},
	Date-Modified = {2010-04-22 16:04:05 -0400},
	Journal = {Journal of the American Stastical Association},
	Pages = {163-171},
	Title = {The estimation of the mean squared error of small-area estimators},
	Volume = {85},
	Year = {1990}}

@book{rao_2003,
	Author = {Rao, J.N.K.},
	Date-Added = {2010-04-22 16:04:05 -0400},
	Date-Modified = {2010-04-22 16:04:05 -0400},
	Publisher = {Wiley, New York},
	Title = {Small Area Estimation},
	Year = {2003}}

@article{srivastava_1992,
	Author = {Srivastava, J. and Ouyang, Z.},
	Journal = {Journal of Planning and Statistics},
	Pages = {199-218},
	Title = {Studies on a general estimator in sampling utilizing extraneous information through a sample weight function},
	Volume = {31},
	Year = {1992}}

@article{sweeting_2006,
	Author = {Sweeting, T. and Datta, G.S. and Ghosh, M.},
	Journal = {Annals of Statistics},
	Pages = {441-468},
	Title = {Nonsubjective priors via predictive relative entropy regret},
	Volume = {34},
	Year = {2006}}

@article{you_2004,
	Author = {You, Y. and Rao, J.N.K},
	Date-Added = {2010-04-22 16:04:05 -0400},
	Date-Modified = {2010-04-22 16:04:05 -0400},
	Journal = {Statistics in Transition},
	Pages = {631-640},
	Title = {Benchmarking hierarchical{B}ayes small area estimators in the Canadian census undercoverage estimation},
	Volume = {6},
	Year = {2004}}

@article{you_2003,
	Author = {You, Y. and Rao, J.N.K},
	Date-Added = {2010-04-22 16:04:05 -0400},
	Date-Modified = {2010-04-22 16:04:05 -0400},
	Journal = {Journal of Statistical Planning and Inference},
	Pages = {197-208},
	Title = {Pseudo hierarchical Bayes small area estimation combining unit level models and survey weights},
	Volume = {111},
	Year = {2003}}

@article{you_2002,
	Author = {You, Y. and {Rao, J.N.K}},
	Date-Added = {2010-04-22 16:04:05 -0400},
	Date-Modified = {2010-04-22 16:04:05 -0400},
	Journal = {The Canadian Journal of Statistics},
	Pages = {431-439},
	Title = {A pseudo-empirical best linear unbiased prediction approach to small area estimation using survey weights.},
	Volume = {30},
	Year = {2002}}

@article{wang_2008,
	Author = {Wang, J. and Fuller, W.A. and Qu, Y.},
	Date-Added = {2010-04-22 16:04:05 -0400},
	Date-Modified = {2010-04-22 16:04:05 -0400},
	Journal = {Survey Methodology},
	Pages = {29-36},
	Title = {Small area estimation under a restriction},
	Volume = {34},
	Year = {2008}}

@article{zellner_1986,
	Author = {Zellner, A.},
	Date-Added = {2010-04-22 16:04:05 -0400},
	Date-Modified = {2010-04-22 16:04:05 -0400},
	Journal = {Advances in Econometrics},
	Pages = {171-182},
	Title = {Further results on Bayesian minimum expected loss (MELO) estimates and posterior distributions for structural coefficients.},
	Year = {1986}}

@article{zellner_1988,
	Author = {Zellner, A.},
	Date-Added = {2010-04-22 16:04:05 -0400},
	Date-Modified = {2010-04-22 16:04:05 -0400},
	Journal = {Journal of Econometrics},
	Pages = {27-50},
	Title = {Bayesian analysis in econometrics},
	Volume = {37},
	Year = {1988}}

@book{zellner_1994,
	Author = {Zellner, A.},
	Date-Added = {2010-04-22 16:04:05 -0400},
	Date-Modified = {2010-04-22 16:04:05 -0400},
	Publisher = {Springer-Verlag, New York},
	Series = {Bayesian and non-Bayesian estimation using balanced loss functions},
	Title = {Statistical Decision Theory and Related Topics},
	Year = {1994}}
	
@incollection{lehtonen_2009,
	Author = {Lehtonen, R. and Veijanen, A.},
	Date-Added = {2010-04-22 16:04:05 -0400},
	Date-Modified = {2010-04-22 16:04:05 -0400},
	Publisher = {North-Holland, Amsterdam},
	Editor = {D. Pfeffermann and C. R. Rao}, 
	Series = {Handbook of Statistics},
	Booktitle = {Sample Surveys: Inference and Analysis},
	Title = {Design-based methods of estimation for domains and small areas.},
	Volume = {29B},
	Pages = {219--249},
	Year = {2009}}	
	

@article{mitchell2008predicting,
  title={Predicting human brain activity associated with the meanings of nouns},
  author={Mitchell, Tom M. and Shinkareva, Svetlana V. and Carlson, Andrew and Chang, Kai-Min and Malave, Vicente L. and Mason, Robert A. and Just, Marcel Adam},
  journal={Science},
  volume={320},
  pages={1191--1195},
  year={2008},
  publisher={American Association for the Advancement of Science}
}

@article{sudre2012tracking,
  title={Tracking neural coding of perceptual and semantic features of concrete nouns},
  author={Sudre, Gustavo and Pomerleau, Dean and Palatucci, Mark and Wehbe, Leila and Fyshe, Alona and Salmelin, Riitta and Mitchell, Tom},
  journal={NeuroImage},
  volume={62},
  pages={451--463},
  year={2012},
  publisher={Elsevier}
}

@article{tzourio2002automated,
  title={Automated anatomical labeling of activations in {SPM} using a macroscopic anatomical parcellation of the {MNI} {MRI} single-subject brain},
  author={Tzourio-Mazoyer, N and Landeau, B and Papathanassiou, D and Crivello, F and Etard, O and Delcroix, N and Mazoyer, Bernard and Joliot, M},
  journal={Neuroimage},
  volume={15},
  pages={273--289},
  year={2002},
  publisher={Elsevier}
}

@inproceedings{palatucci2009zero,
  title={Zero-shot learning with semantic output codes},
  author={Palatucci, Mark and Pomerleau, Dean and Hinton, Geoffrey E and Mitchell, Tom M},
  booktitle={Advances in neural information processing systems},
  pages={1410--1418},
  year={2009}
}

@article{smith2004overview,
  title={Overview of {fMRI} analysis},
  author={Smith, Stephen M},
  journal={British journal of radiology},
  volume={77},
  number={suppl 2},
  pages={S167--S175},
  year={2004},
  publisher={Br Inst Radiology}
}

@article{naselaris2011encoding,
  title={Encoding and decoding in fMRI},
  author={Naselaris, Thomas and Kay, Kendrick N and Nishimoto, Shinji and Gallant, Jack L},
  journal={NeuroImage},
  volume={56},
  pages={400--410},
  year={2011},
  publisher={Elsevier}
}


@misc{friedmanglmnet,
  title={Glmnet for Matlab},
  year = 2010,
  author={Friedman, J and Hastie, T and Tibshirani, R and Jiang, H}
}

@article{sadinle2,
	author = {Sadinle, Mauricio},
	title = {{A Bayesian Framework for Duplicate Detection, Record Linkage, and Subsequent Inference with Linked Files}},
	year = {2013}
}


@article{larsen,
	author = {Larsen, M. D. and Rubin, D. B.},
	title = {{Iterative Automated Record Linkage Using Mixture Models}},
	journal = {Journal of the American Statistical Association},
	volume = {96},
	number = {453},
	year = {2001}
}


@article{evt,
	author = {Murat Sariyar and Andreas Borg and Klaus Pommerening},
	title = {{Controlling false match rates in record linkage using extreme value theory}},
	journal = {Journal of Biomedical Informatics},
	volume = {in press},
	year = {2011}
}


@article{hartigan,
	author = {Hartigan, J.A.},
	title = {{Clustering Algorithms}},
	journal = {John Wiley \& Sons, New York},
	year = {1975}
}


@article{fellegi,
	author = {Ivan P. Fellegi and Alan B. Sunter},
	title = {{A Theory for Record Linkage}},
	journal = {Journal of the American Statistical Association},
	volume = {64},
	year = {1969},
	Number = {328},
	Pages = {1183--1210}
}

@article{rand,
	author = {William B. Rand},
	title = {{Objective criteria for the evaluation of clustering methods}},
	journal = {Journal of the American Statistical Association},
	volume = {66},
	year = {1971},
	Number = {336},
	Pages = {846--850}
}


@misc{winkler-conv,
	author = {Winkler, W.E.},
	title = {{Personal communication}},
	year = {2013},
}



@misc{christen-conv,
	author = {Christen, Peter},
	title = {{Personal communication}},
	year = {2011},
}


@article{beka,
	Title = {{Parametric Bayesian Inference for High Dimensional Multiple Record Linkage}},
	Author = {Steorts, Rebecca C. and Hall, Robert and Fienberg, Stephen},
	Year = {2013}
}


@article{iris,
	Title = {{The use of multiple measurements in taxonomic problems}},
	Author = {Fisher, R. A.},
	Pages = {179–-188},
	Year = {1936},
	Volume = {7},
	Journal = {Annals of Eugenics}
}


@article{minimax,
	Title = {{Hierarchical Clustering With Prototypes via Minimax Linkage}},
	Author = {Jacob Bien and Robert Tibshirani},
	Pages = {1075--1084},
	Year = {2012},
	Journal = {Annals of Eugenics},
	URL = {http://www-stat.stanford.edu/~jbien/jasa2011minimax.pdf}
}


@article{tantrum,
	Title = {{Hierarchical model-based clustering of large datasets through fractionation and refractionation}},
	Author = {Jeremy Tantrum and Alejandro Murua and Werner Stuetzle},
	Pages = {315–-326},
	Year = {2004},
	Journal = {Information Systems},
	Volume = {29},
	URL = {http://www.dms.umontreal.ca/~murua/research/hierarchicalMBC.pdf}
}



@article{cutting,
	Title = {{Scatter/gather: a cluster-based approach to browsing large document collections}},
	Author = {D. Cutting and D. Karger and J. Pedersen and J. Tukey},
	Year = {1992},
	Journal = {Proceedings of  15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
	Pages = {318--329}
}



@article{blb,
	Title = {{A Scalable Bootstrap for Massive Data}},
	Author = {Ariel Kleiner and Ameet Talwalkar and Purnamrita Sarkar and Michael I. Jordan},
	Year = {2012},
	URL = {http://arxiv.org/abs/1112.5016},
}

@article{strings,
	Title = {{Approximate string matching}},
	Author = {Wikipedia},
	Year = {2013},
	URL = {http://en.wikipedia.org/wiki/Approximate_string_matching},
}

@article{nltcs,
	Title = {{The National Long-Term Care Survey}},
	Author = {Kenneth G. Manton},
	Institution = {Duke University},
	Year = {2013},
	URL = {http://www.nltcs.aas.duke.edu/data.htm},
}


@article{jw,
	Title = {{String Comparator Metrics and Enhanced Decision Rules in the Fellegi-Sunter Model of Record Linkage}},
	Author = {Winkler, W.E.},
	Pages = {354--359},
	Year = {1990},
	Journal = {Proceedings of the Section on Survey Research Methods (American Statistical Association)},
}


@article{winkler95,
	Title = {{Matching and Record Linkage}},
	Author = {Winkler, W.E.},
	Pages = {355--384},
	Year = {1995},
	Journal = {Business Survey Methods, New York: J. Wiley},
}

@article{han,
	Title = {{Two Supervised Learning Approaches for Name Disambiguation in Author Citations}},
	Author = {H. Han and L. Giles and H. Zha and C. Li and K. Tsioutsiouliklis},
	Year = {2004},
	Journal = {Joint Conference on Digital Libraries}
}




@article{f1,
	Title = {{Small Worlds and Regional Innovation}},
	Author = {L. Fleming and C. King III and A. Juda},
	Year = {2007},
	Journal = {Organizational Science},
	Volume = {18},
	Number = {6}
}


@article{f2,
	Title = {{The careers and co-authorship networks of U.S. patent-holders, since 1975}},
	Author = {R. Lai and A. D'Amour and L. Fleming},
	Year = {2009}
}


@article{f3,
	Title = {{Disambiguation and Co-authorship Networks of the U.S. Patent Inventor Database}},
	Author = {R. Lai and A. D'Amour and A. Yu and Y. Sun and L. Fleming},
	Year = {2011}
}


@article{treeratpituk,
	Title = {{Disambiguating Authors in Academic Publications using Random Forests}},
	Author = {P. Treeratpituk and C.L. Giles},
	Journal = {Joint Conference on Digital Libaries},
	Year = {2009}
}


@article{torvik,
	Title = {{Author Name Disambiguation in MEDLINE}},
	Author = {V. Torvik and N. Smalheiser},
	Journal = {ACM Transactions on Knowledge Discovery from Data},
	Year = {2009},
	Volume = {3}, 
	Number = {3},
	Article = {11}
}



@article{uspto,
	Title = {{USPTO Assignee Harmonization}},
	Author = {USPTO},
	Year = {2006},
	URL = {http://www.uspto.gov/web/offices/ac/ido/oeip/taf/data/misc/data_cd.doc/assignee_harmonization/_read_me_assignees_69_10Nov05.txt}
}



@article{carayol,
	Title = {{Who's Who in Patents: A Bayesian approach}},
	Author = {N. Carayol and L. Cassi},
	Year = {2009}
}






@unpublished{wunmi,
	Title = {{Economic Downturns, Technology Trajectories, and the Careers of Scientists}},
	Author = {E. Akinsanmi and R. Reagans and E. Fuchs},
	Year = {2012},
	Journal = {Carnegie Mellon University working paper},
}



@unpublished{sam-epp,
	Title = {{Methods Matter:  Rethinking Inventor Disambiguation Algorithms with Classification Models and Labeled Inventor Records}},
	Author = {S. L. Ventura and R. Nugent and E.R.H. Fuchs},
	Year = {2013},
	Journal = {Currently Under Review},
}


@book{hastie,
	Title = {{The Elements of Statistical Learning:  Data Mining, Inference, and Prediction, Second Edition}},
	Author = {Hastie, T. and Tibshirani, R. and Friedman, J.},
	Year = {2009},
	Publisher = {Springer-Verlag},
}


@article{breiman,
	Title = {{Random Forests}},
	Author = {Breiman, L.},
	Volume = {45},
	Number = {1},
	Pages = {5-32},
	Year = {2001},
	Journal = {Machine Learning},
}







@article{martins,
	Title = {{A Supervised Machine Learning Approach for Duplicate Detection for Gazetteer Records}},
	Author = {B. Martins},
	Year = {2011}, 
	Journal = {Lecture Notes in Computer Science},
	Volume = {6631},
	Pages = {34--51}
}


@article{cox1972rmald,
	Title = {{Regression Models and Life-Tables (with discussion)}},
	Author = {Cox, D.R},
	Volume = {34},
	Pages = {187-220},
	Year = {1972},
	Journal = {Journal of the Royal Statistical Society, Series B},
}

@article{hirotsu2002umpmafmdotsatd,
	Title = {{Using a Markov Process Model of an Association Football Match to Determine the Optimal Timing of Substitution and Tactical Decisions}},
	Author = {Hirotsu, Nobuyoshi and Wright, Michael},
	Volume = {53},
	Number = {1},
	Year = {2002},
	Journal = {Journal of the Operational Research Society},
}


@article{rosenbaum2004mhnphttw,
	Title = {{Measuring How NBA Players Help Their Teams Win}},
	Author = {Dan T. Rosenbaum},
	Year = {2004},
	URL = {http://www.82games.com/comm30.htm},
}


@inproceedings{lock2009b+rscnp,
	Title = {{Beyond +/-: A Rating System to Compare NHL Players}},
	Author = {Dennis Lock and MIchael Schuckers},
	Year = {2009},
	Journal = {Joint Statistical Meetings},
	Note = {Presentation at Joint Statistical Meetings},
}



@article{tibshirani1996rsasvl,
	Title = {{Regression Shrinkage and Selection via the Lasso}},
	Author = {Tibshirani, Robert},
	Volume = {58},
	Number = {1},
	Pages = {267–288},
	Year = {1996},
	Journal = {Journal of the Royal Statistical Society, Series B (Methodology)},
}

@article{tibshirani1997lmfvscm,
	Title = {{The Lasso Method for Variable Selection in the Cox Model}},
	Author = {Robert Tibshirani},
	Volume = {16},
	Pages = {385-395},
	Year = {1997},
	Journal = {Statistics in Medicine},
	Local-URL = {%UD%/T/Tibshirani (1997) The Lasso Method for Variable Selection in the Cox Model.pdf},
}


@article{zou2005regularization,
	Title = {{Regularization and variable selection via the elastic net}},
	Author = {Zou, H. and Hastie, T.},
	Volume = {67},
	Number = {2},
	Pages = {301--320},
	Year = {2005},
	Journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	Local-URL = {%UD%/Z/Zou (2005) Regularization and variable selection via the elastic net.pdf},
}




@article{wikipedia2012p,
	Title = {{Plus-Minus}},
	Author = {Wikipedia},
	Year = {2012},
	URL = {http://en.wikipedia.org/wiki/Plus-minus},
}





@article{macdonald2011rapsfnp,
	Title = {{A Regression-based Adjusted Plus-Minus Statistic for NHL Players}},
	Author = {Brian Macdonald},
	Volume = {7},
	Number = {3},
	Year = {2011},
	Journal = {Journal of Quantitative Analysis in Sports},
}


@inproceedings{macdonald2012egmfentap,
	Title = {{An Expected Goals Model for Evaluating NHL Teams and Players}},
	Author = {Brian Macdonald},
	Year = {2012},
	Local-URL = {%UD%/M/Macdonald (2012) An Expected Goals Model for Evaluating NHL Teams and Players.pdf},
booktitle = {MIT Sloan Sports Analytics Conference 2012},}



@article{macdonald2012apfnpurr,
	Title = {{Adjusted Plus-Minus for NHL Players using Ridge Regression}},
	Author = {Brian Macdonald},
	Year = {2012},
	URL = {http://arxiv.org/abs/1201.0317v1},
	Local-URL = {%UD%/M/Macdonald (2012) Adjusted Plus-Minus for NHL Players using Ridge Regression.pdf},
}





@unpublished{schuckers2011nhlsrbuaoeampaa,
	Title = {{National Hockey League Skater Ratings Based upon All On-Ice Events: An Adjusted Minus/Plus Probability (AMPP) Approach}},
	Author = {Michael E. Schuckers and Dennis F. Lock and Chris Wells and C. J. Knickerbocker and Robin H. Lock},
	Year = {2011},
	Local-URL = {%UD%/S/Schuckers (2011) National Hockey League Skater Ratings Based upon All On-Ice Events An Adjusted Minus-Plus Probability (AMPP) Approach.pdf},
}



@article{sill2010ina+uraot,
	Title = {{Improved NBA Adjusted +/- Using Regularization and Out-of-Sample Testing}},
	Author = {Joseph Sill},
	Year = {2010},
	Journal = {MIT Sloan Sports Analytics Conference},
	Local-URL = {%UD%/S/Sill (2010) Improved NBA Adjusted +-- Using Regularization and Out-of-Sample Testing.pdf},
}




@article{thomas2006ippalihs,
	Title = {{The Impact of Puck Possession and Location on Ice Hockey Strategy}},
	Author = {A.C. Thomas},
	Volume = {2},
	Number = {1},
	Year = {2006},
	Comment = {Paper number 1!},
	Journal = {Journal for Quantitative Analysis in Sports},
	Tag = {sports, },
	Local-URL = {%UD%/T/Thomas - 2006 - The Impact of Puck Possession and Location on Ice Hockey Strategy.pdf},
}

@article{thomas2007itgih,
	Title = {{Inter-Arrival Times of Goals in Ice Hockey}},
	Author = {A.C. Thomas},
	Volume = {3},
	Number = {3},
	Year = {2007},
	Journal = {Journal of Quantitative Analysis in Sports},
	Tag = {sports, },
	Local-URL = {%UD%/T/Thomas - 2007 - Inter-Arrival Times of Goals in Ice Hockey.pdf},
}



@article{hoerl1970rrbefnp,
	Title = {{Ridge regression: Biased estimation for nonorthogonal problems}},
	Author = {Hoerl, A. E. and R. W. Kennard},
	Volume = {12},
	Pages = {55–67},
	Year = {1970},
	Journal = {Technometrics},
}



@article{brown2008ipbaftebabm,
	Title = {{In-season Prediction of Batting Averages -- A Field Test of Empirical Bayes and Bayes Methodologies}},
	Author = {Lawrence D. Brown},
	Volume = {2},
	Number = {1},
	Pages = {113–152},
	Year = {2008},
	Journal = {The Annals of Applied Statistics},
	Local-URL = {%UD%/B/Brown - 2008 - In-season Prediction of Batting Averages -- A Field Test of Empirical Bayes and Bayes Methodologies.pdf},
}


@article{james1961eql,
	Title = {{Estimation with quadratic loss}},
	Author = {James, W. and Stein, C},
	Volume = {1},
	Pages = {367--379},
	Year = {1961},
	Journal = {Proc. 4th Berkeley Symp. Probab. Statist},
}



@article{beaudoin2010sfpgh,
	Title = {{Strategies for Pulling the Goalie in Hockey}},
	Author = {David Beaudoin and Tim B. Swartz},
	Volume = {64},
	Number = {3},
	Year = {2010},
	Journal = {The American Statistician},
	Local-URL = {%UD%/B/Beaudoin - 2010 - Strategies for Pulling the Goalie in Hockey.pdf},
}


@incollection{morrison1976otpgpmacsuih,
	Title = {{On the Optimal Time to Pull the Goalie: A Poisson Model Applied to a Common Strategy Used in Ice Hockey}},
	Author = {Morrison, D.G.},
	Volume = {4},
	Year = {1976},
	Booktitle = {TIMS Studies in Management Science},
}


@article{ilardi2008aprnaif2,
	Title = {{Adjusted Plus-Minus Ratings: New and Improved for 2007-2008}},
	Author = {Ilardi, S. and A. Barzilai},
	Year = {2008},
	URL = {http://www.82games.com/ilardi2.htm},
}


@article{cook2006vsb,
	Title = {{Validation of Software for Bayesian Models Using Posterior Quantiles}},
	Author = {Samantha R. Cook and Andrew Gelman and Donald B. Rubin},
	Volume = {15},
	Number = {3},
	Pages = {675--692},
	Year = {2006},
	Journal = {Journal of Computational and Graphical Statistics},
	Local-URL = {%UD%/C/Cook - 2006 - Validation of Software for Bayesian Models Using Posterior Quantiles.pdf},
}


@article{gramacy2013epchrlr,
	Title = {{Estimating Player Contribution in Hockey with Regularized Logistic Regression}},
	Author = {Robert B. Gramacy and Shane T. Jensen and Matt Taddy},
	Year = {2013},
	URL = {http://arxiv.org/abs/1209.5026},
	Local-URL = {%UD%/G/Gramacy (2013) Estimating Player Contribution in Hockey with Regularized Logistic Regression.pdf},
}

@article{berry1999bdes,
	Title = {{Bridging Different Eras in Sports}},
	Author = {Scott M. Berry and C. Shane Reese and Patrick D. Larkey},
	Volume = {94},
	Number = {447},
	Pages = {661-676},
	Year = {1999},
	Journal = {Journal of the American Statistical Association},
	Local-URL = {%UD%/B/Berry - 1999 - Bridging Different Eras in Sports.pdf},
}



@article{dawid1994spbi,
	Title = {{Selection Paradoxes of Bayesian Inference}},
	Author = {A. P. Dawid},
	Pages = {211-220},
	Year = {1994},
	Journal = {Lecture Notes-Monograph Series, Vol. 24, Multivariate Analysis and Its Applications},
	Local-URL = {%UD%/D/Dawid (1994) Selection Paradoxes of Bayesian Inference.pdf},
}

@book{klein-reif,
  Title = {{The Hockey Compendium: NHL Facts, Stats and Stories}},
  Author = {Jeff Z. Klein and Karl-Eric Reif},
  Year = {2001},
}


@article{hans2011enrmonp,
	Title = {{Elastic Net Regression Modeling With the Orthant Normal Prior}},
	Author = {Chris Hans},
	Volume = {106},
	Number = {496},
	Pages = {1383-1393},
	Year = {2011},
	Journal = {Journal of the American Statistical Association},
	Local-URL = {%UD%/H/Hans (2011) Elastic Net Regression Modeling With the Orthant Normal Prior.pdf},
}


@article{li2010ben,
	Title = {{The Bayesian Elastic Net}},
	Author = {Qing Li and Nan Lin},
	Volume = {5},
	Number = {1},
	Pages = {151–170},
	Year = {2010},
	Journal = {Bayesian Analysis},
	Local-URL = {%UD%/L/Li (2010) The Bayesian Elastic Net.pdf},
}


@article{park2008bl,
	Title = {{The Bayesian Lasso}},
	Author = {Trevor Park and George Casella},
	Volume = {103},
	Number = {482},
	Year = {2008},
	Journal = {Journal of the American Statistical Association},
	Local-URL = {%UD%/P/Park (2008) The Bayesian Lasso.pdf},
}


@ARTICLE{Minnotte93themode,
    author = {Michael C. Minnotte and David W. Scott},
    title = {The Mode Tree: A Tool for Visualization of Nonparametric Density Features},
    journal = {Journal of Computational and Graphical Statistics},
    year = {1993},
    volume = {2},
    pages = {51--68}
}


@article{hartigan1981,
     jstor_articletype = {research-article},
     title = {Consistency of Single Linkage for High-Density Clusters},
     author = {Hartigan, J. A.},
     journal = {Journal of the American Statistical Association},
     jstor_issuetitle = {},
     volume = {76},
     number = {374},
     jstor_formatteddate = {Jun., 1981},
     pages = {pp. 388-394},
     url = {http://www.jstor.org/stable/2287840},
     ISSN = {01621459},
     language = {English},
     year = {1981},
}


@article{Dunn1946,
  author = {Dunn, Halbert L},
  year = {1946},
  title = {Record Linkage},
  journal={American Journal of Public Health and the Nation's Health},
  volume = {36},
  number = {12},
  pages = {1412--1416}
}

@article{Tepping1968,
    author = {Tepping, Benjamin J.},
    journal = {Journal of the American Statistical Association},
    mendeley-groups = {Entity Resolution/Classical sources},
    number = {324},
    pages = {1321--1332},
    title = {A Model for Optimum Linkage of Records},
    volume = {63},
    year = {1968}
}

@article{Newcombe1962,
    author = {Newcombe, H. B. and Kennedy, James M},
    journal = {Communications of the ACM},
    number = {11},
    pages = {563--566},
    title = {{Record Linkage: Making Maximum Use of the Discriminating Power of Identifying Information}},
    volume = {5},
    year = {1962}
}

@article{Newcombe1969,
    author = {Newcombe, Howard B.},
    journal = {Population (French Edition)},
    number = {4},
    pages = {653},
    title = {{Couplage de donn{\'{e}}es pour les {\'{e}}tudes d{\'{e}}mographiques}},
    volume = {24},
    year = {1969}
}

@article{Newcombe1962b,
    author = {Newcombe, Howard B and Rhynas, Philip O W},
    journal = {Eugenics Quarterly},
    number = {1},
    pages = {25--35},
    publisher = {Routledge},
    title = {{Child spacing following stillbirth and infant death}},
    volume = {9},
    year = {1962}
}

@article{Newcombe1965,
    author = {Newcombe, Howard B},
    journal = {The Eugenics Review},
    number = {3},
    pages = {109--125},
    title = {{The Study of Mutation and Selection in Human Populations}},
    volume = {57},
    year = {1965}
}

@article{Newcombe1963,
    author = {Newcombe, Howard B.},
    journal = {Annals of Human Genetics},
    number = {4},
    pages = {367--382},
    title = {{Screening for effects of maternal age and birth order in a register of handicapped children}},
    volume = {27},
    year = {1963}
}

@article{Newcombe1965b,
    author = {Newcombe, Howard B. and Tavendale, Olwyn G.},
    journal = {Obstetrical and Gynecological Survey},
    number = {4},
    pages = {655--656},
    title = {{Effects of father's age on the risk of child handicap or death}},
    volume = {20},
    year = {1965}
}

@article{Jaro1989,
    author = {Jaro, Matthew A.},
    journal = {Journal of the American Statistical Association},
    number = {406},
    pages = {414--420},
    title = {{Advances in record-linkage methodology as applied to matching the 1985 census of Tampa, Florida}},
    volume = {84},
    year = {1989}
}

@inproceedings{Winkler1988,
    author = {Winkler, William E.},
    booktitle = {Proceedings of the Section on Survey Research Methods, American Statistical Association},
    pages = {667--671},
    title = {{Using the EM Algorithm for Weight Computation in the Fellegi-Sunter Model of Record Linkage}},
    year = {1988}
}

@incollection{Winkler1985,
    address = {Arlington, Virginia},
    author = {Winkler, William E.},
    booktitle = {Record Linkage Techniques},
    editor = {Kilss, Beth and Alvey, Wendy},
    pages = {438--443},
    publisher = {U.S. Internal Revenue Services, Publication 1299},
    title = {{Exact Matching Lists of Businesses: Blocking, Subfield Identification, Information Theory}},
    year = {1985}
}

@techreport{Kelley1984,
    author = {Kelley, Robert Patrick},
    booktitle = {Statistical research division report series},
    publisher = {Bureau of the Census},
    title = {{Blocking considerations for record linkage under conditions of uncertainty}},
    year = {1984}
}

@article{Thibaudeau1993,
    author = {Thibaudeau, Yves},
    journal = {Survey Methodology},
    number = {1},
    title = {{The Discrimination Power of Dependency Structures in Record Linkage}},
    volume = {19},
    year = {1993}
}

@article{Winkler1993,
    author = {Winkler, William E.},
    journal = {Proceedings of the Section on Survey Research Methods, American Statistical Association},
    number = {M},
    pages = {274--279},
    title = {{Improved Decision Rules In The Fellegi-Sunter Model Of Record Linkage}},
    year = {1993}
}

@article{DuBois1969,
    author = {{Du Bois}, N. S.D.Andrea},
    journal = {Journal of the American Statistical Association},
    number = {325},
    pages = {163--174},
    title = {{A Solution to the Problem of Linking Multivariate Documents}},
    volume = {64},
    year = {1969}
}


@article{Verykios2003,
    author = {Verykios, Vassilios S. and Moustakides, George V. and Elfeky, Mohamed G.},
    journal = {VLDB Journal},
    number = {1},
    pages = {28--40},
    title = {{A Bayesian decision model for cost optimal record matching}},
    volume = {12},
    year = {2003}
}


@inproceedings{Armstrong1992,
    author = {Armstrong, J.B. and Mayda, J.E.},
    booktitle = {Proceedings of the Section on Survey Research Methodology},
    file = {:Users/olivierbinette/Documents/Mendeley/Armstrong, Mayda - 1992 - Estimation of Record Linkage Models Using Dependent Data.pdf:pdf},
    mendeley-groups = {Entity Resolution/Applications,Entity Resolution},
    pages = {853 -- 858},
    publisher = {American Statistical Association},
    title = {{Estimation of Record Linkage Models Using Dependent Data}},
    year = {1992}
}

@incollection{belin1990proposed,
    address = {Washington, DC, USA},
    author = {Belin, Thomas R.},
    booktitle = {Statistics of Income and Related Administrative Record Research},
    pages = {167--172},
    publisher = {International Revenue Service},
    title = {A Proposed Improvement in Computer Matching Techniques},
    year = {1990}
}


@inproceedings{Winkler1992,
    author = {Winkler, William E.},
    booktitle = {Proceedings of the Section on Survey Research Methods},
    file = {:home/olivier/Downloads/1992{\_}140.pdf:pdf},
    keywords = {dependence,em algorithm,linear,log,string},
    mendeley-groups = {Entity Resolution,Entity Resolution/FS-type},
    pages = {829--834},
    title = {{Comparative analysis of record linkage decision rules}},
    year = {1992}
}

@article{Daggy2014,
    author = {Daggy, Joanne and Xu, Huiping and Hui, Siu and Grannis, Shaun},
    title = {Evaluating Latent Class Models With Conditional Dependence in Record Linkage},
    journal = {Statistics in Medicine},
    volume = {33},
    number = {24},
    pages = {4250-4265},
    keywords = {latent class, record linkage, loglinear model, random effects},
    doi = {10.1002/sim.6230},
    year = {2014}
}


@article{Xu2019,
    author = {Xu, Huiping and Li, Xiaochun and Shen, Changyu and Hui, Siu L. and Grannis, Shaun},
    journal = {Annals of Applied Statistics},
    number = {3},
    pages = {1753--1790},
    title = {{Incorporating conditional dependence in latent class models for probabilistic record linkage: Does it matter?}},
    volume = {13},
    year = {2019}
}


@article{Sadinle2013,
    author = {Sadinle, Mauricio and Fienberg, Stephen E.},
    journal = {Journal of the American Statistical Association},
    number = {502},
    pages = {385--397},
    title = {{A generalized fellegi-sunter framework for multiple record linkage with application to homicide record systems}},
    volume = {108},
    year = {2013}
}

@article{Nigam2000,
    author = {Nigam, Kamal and McCallum, Andrew Kachites and Thrun, Sebastian and Mitchell, Tom},
    journal = {Machine Learning},
    title = {{Text Classification from Labeled and Unlabeled Documents using EM}},
    year = {2000}
}

@article{Criminisi2011,
    author = {Criminisi, Antonio and Shotton, Jamie and Konukoglu, Ender},
    journal = {Foundations and Trends in Computer Graphics and Vision},
    number = {2-3},
    pages = {81--227},
    title = {{Decision forests: A unified framework for classification, regression, density estimation, manifold learning and semi-supervised learning}},
    volume = {7},
    year = {2011}
}

@article{Szummer2002,
    author = {Szummer, Martin and Jaakkola, Tommi},
    journal = {Advances in Neural Information Processing Systems},
    title = {{Partially labeled classification with markov random walks}},
    year = {2002}
}

@book{Chapelle2006,
    address = {Cambridge, Massachusetts},
    author = {Chapelle, Olivier and Bernhard, Sc{\"{o}}lkopf and Zien, Alexander},
    booktitle = {Adaptive Computation and Machine Learning Thomas},
    editor = {Dietterich, Thomas},
    file = {:Users/olivierbinette/Documents/Mendeley/Chapelle, Bernhard, Zien - 2006 - Semi-Supervised Learning.pdf:pdf},
    issn = {10514651},
    mendeley-groups = {Entity Resolution/Semi-supervised},
    pages = {508},
    publisher = {The MIT Press Cambridge,},
    title = {{Semi-Supervised Learning}},
    year = {2006}
}


@misc{Goldberg2009,
    author = {Goldberg, Xiaojin},
    booktitle = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
    isbn = {9781598295481},
    issn = {19394608},
    pages = {1--116},
    title = {{Introduction to semi-supervised learning}},
    volume = {6},
    year = {2009}
}

@article{fleming2007small,
  title={Small worlds and regional innovation},
  author={Fleming, Lee and King III, Charles and Juda, Adam I},
  journal={Organization Science},
  volume={18},
  number={6},
  pages={938--954},
  year={2007},
  publisher={Informs}
}


@article{li2014disambiguation,
  title={Disambiguation and co-authorship networks of the US patent inventor database (1975--2010)},
  author={Li, Guan-Cheng and Lai, Ronald and D’Amour, Alexander and Doolin, David M and Sun, Ye and Torvik, Vetle I and Amy, Z Yu and Fleming, Lee},
  journal={Research Policy},
  volume={43},
  number={6},
  pages={941--955},
  year={2014},
  publisher={Elsevier}
}

@article{tang2010bibliometric,
  title={Bibliometric fingerprints: name disambiguation based on approximate structure equivalence of cognitive maps},
  author={Tang, Li and Walsh, John},
  journal={Scientometrics},
  volume={84},
  number={3},
  pages={763--784},
  year={2010},
  publisher={Akad{\'e}miai Kiad{\'o}, co-published with Springer Science+ Business Media BV~…}
}


@article{Enamorado2019,
    author = {Enamorado, Ted},
    title = {{Active Learning for Probabilistic Record Linkage}},
    year = {2019},
    note={Available at SSRN: \url{https://ssrn.com/abstract=3257638} or \url{http://dx.doi.org/10.2139/ssrn.3257638}}
}

@inproceedings{Bellare2012,
    author = {Bellare, Kedar and Iyengar, Suresh and Parameswaran, Aditya G. and Rastogi, Vibhor},
    title = {Active Sampling for Entity Matching},
    year = {2012},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    booktitle = {Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
    pages = {1131–1139},
    numpages = {9},
    location = {Beijing, China}
}

@article{Smith1975,
    author = {Smith, M. E. and Newcombe, H. B.},
    journal = {Methods of Information in Medicine},
    number = {3},
    pages = {118--125},
    pmid = {1196137},
    title = {{Methods for computer linkage of hospital admission separation records into cumulative health histories}},
    volume = {14},
    year = {1975}
}


@article{Hernandez1998,
    author = {Hern{\'{a}}ndez, Mauricio A. and Stolfo, Salvatore J.},
    doi = {10.1023/A:1009761603038},
    issn = {13845810},
    journal = {Data Mining and Knowledge Discovery},
    number = {1},
    pages = {9--37},
    title = {{Real-world data is dirty: Data cleansing and the merge/purge problem}},
    volume = {2},
    year = {1998}
}

@article{Gruenheid2014,
    author = {Gruenheid, Anja and Dong, Xin Luna and Srivastava, Divesh},
    doi = {10.14778/2732939.2732943},
    isbn = {4159862349},
    issn = {21508097},
    journal = {Proceedings of the VLDB Endowment},
    number = {9},
    pages = {697--708},
    title = {{Incremental record linkage}},
    volume = {7},
    year = {2014}
}


@article{Li2014,
    author = {Li, Guan Cheng and Lai, Ronald and D'Amour, Alexander and Doolin, David M. and Sun, Ye and Torvik, Vetle I. and Yu, Amy Z. and Lee, Fleming},
    doi = {10.1016/j.respol.2014.01.012},
    issn = {00487333},
    journal = {Research Policy},
    number = {6},
    pages = {941--955},
    publisher = {Elsevier B.V.},
    title = {{Disambiguation and co-authorship networks of the U.S. patent inventor database (1975-2010)}},
    url = {http://dx.doi.org/10.1016/j.respol.2014.01.012},
    volume = {43},
    year = {2014}
}


@article{Kejriwal2015,
    author = {Kejriwal, Mayank and Miranker, Daniel P.},
    doi = {10.1007/978-3-319-18818-8_24},
    isbn = {9783319188171},
    issn = {16113349},
    journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
    pages = {388--402},
    title = {{Semi-supervised instance matching using boosted classifiers}},
    volume = {9088},
    year = {2015}
}

@inproceedings{Wang2015,
    address = {Cham},
    author = {Wang, Qing and Vatsalan, Dinusha and Christen, Peter},
    booktitle = {Advances in Knowledge Discovery and Data Mining},
    editor = {Cao, Tru and Lim, Ee-Peng and Zhou, Zhi-Hua and Ho, Tu-Bao and Cheung, David and Motoda, Hiroshi},
    isbn = {978-3-319-18032-8},
    pages = {562--573},
    publisher = {Springer International Publishing},
    title = {{Efficient Interactive Training Selection for Large-Scale Entity Resolution}},
    year = {2015}
}

@INPROCEEDINGS{Christen2016,
  author={P. {Christen} and D. {Vatsalan} and Q. {Wang}},
  booktitle={2015 IEEE International Conference on Data Mining}, 
  title={Efficient Entity Resolution with Adaptive and Interactive Training Data Selection}, 
  year={2015},
  volume={},
  number={},
  pages={727-732},}



@article{Vesdapunt2014,
    author = {Vesdapunt, Norases and Bellare, Kedar and Dalvi, Nilesh},
    title = {Crowdsourcing Algorithms for Entity Resolution},
    year = {2014},
    issue_date = {August 2014},
    publisher = {VLDB Endowment},
    volume = {7},
    number = {12},
    issn = {2150-8097},
    url = {https://doi.org/10.14778/2732977.2732982},
    doi = {10.14778/2732977.2732982},
    journal = {Proc. VLDB Endow.},
    pages = {1071–1082},
    numpages = {12}
}

@article{Wang2012,
    author = {Wang, Jiannan and Kraska, Tim and Franklin, Michael J. and Feng, Jianhua},
    doi = {10.14778/2350229.2350263},
    eprint = {1208.1927},
    issn = {21508097},
    journal = {Proceedings of the VLDB Endowment},
    number = {11},
    pages = {1483--1494},
    title = {{CrowdER: Crowdsourcing entity resolution}},
    volume = {5},
    year = {2012}
}

@article{Sarawagi2002,
    author = {Sarawagi, Sunita and Bhamidipaty, Anuradha},
    doi = {10.1145/775085.775087},
    file = {:home/olivier/Downloads/SarawagiKDD2002.pdf:pdf},
    isbn = {158113567X},
    journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
    mendeley-groups = {Entity Resolution/Active Learning,Entity Resolution/Crowdsourcing},
    pages = {269--278},
    title = {{Interactive deduplication using active learning}},
    year = {2002}
}

  
@report{Trajtenberg2008,
  title={Identification and mobility of Israeli patenting inventors},
  author={Trajtenberg, Manuel and Shiff, Gil},
  year={2008},
  publisher={Pinhas Sapir Center for Development}
}

@article{Kopcke2010,
    author = {K{\"{o}}pcke, Hanna and Rahm, Erhard},
    doi = {10.1016/j.datak.2009.10.003},
    issn = {0169023X},
    journal = {Data and Knowledge Engineering},
    keywords = {Entity matching,Entity resolution,Match optimization,Matcher combination,Training selection},
    mendeley-groups = {Entity Resolution/Software},
    number = {2},
    pages = {197--210},
    publisher = {Elsevier B.V.},
    title = {{Frameworks for entity matching: A comparison}},
    url = {http://dx.doi.org/10.1016/j.datak.2009.10.003},
    volume = {69},
    year = {2010}
}

@InProceedings{Rahm2016,
    author="Rahm, Erhard",
    editor="Pokorn{\'y}, Jaroslav
    and Ivanovi{\'{c}}, Mirjana
    and Thalheim, Bernhard
    and {\v{S}}aloun, Petr",
    title="The Case for Holistic Data Integration",
    booktitle="Advances in Databases and Information Systems",
    year="2016",
    publisher="Springer International Publishing",
    address="Cham",
    pages="11--27"
}


@InProceedings{Kooli2018,
    author="Kooli, Nihel
    and Allesiardo, Robin
    and Pigneul, Erwan",
    editor="Nguyen, Ngoc Thanh
    and Hoang, Duong Hung
    and Hong, Tzung-Pei
    and Pham, Hoang
    and Trawi{\'{n}}ski, Bogdan",
    title="Deep Learning Based Approach for Entity Resolution in Databases",
    booktitle="Intelligent Information and Database Systems",
    year="2018",
    publisher="Springer International Publishing",
    address="Cham",
    pages="3--12",
    isbn="978-3-319-75420-8"
}

@article{Kasai2020,
    author = {Kasai, Jungo and Qian, Kun and Gurajada, Sairam and Li, Yunyao and Popa, Lucian},
    doi = {10.18653/v1/p19-1586},
    eprint = {1906.08042},
    isbn = {9781950737482},
    journal = {ACL 2019 - 57th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference},
    pages = {5851--5861},
    title = {{Low-resource deep entity resolution with transfer and active learning}},
    year = {2020}
}

@article{Ebraheem2017,
    author = {Ebraheem, Muhammad and Thirumuruganathan, Saravanan and Joty, Shafiq and Ouzzani, Mourad and Tang, Nan},
    journal={arXiv preprint},
    note = {{arXiv:1710.00597}},
    year = {2017}
}

@article{Mudgal2018,
    author = {Mudgal, Sidharth and Li, Han and Rekatsinas, Theodoros and Doan, AnHai and Park, Youngchoon and Krishnan, Ganesh and Deep, Rohit and Arcaute, Esteban and Raghavendra, Vijay},
    doi = {10.1145/3183713.3196926},
    isbn = {9781450347037},
    keywords = {acm reference format,anhai doan,deep learning,entity matching,entity resolution,han li,sidharth mudgal,theodoros rekatsinas,young-},
    pages = {19--34},
    title = {{Deep Learning for Entity Matching}},
    year = {2018}
}

@article{Gottapu2016,
    author = {Gottapu, Ram Deepak and Dagli, Cihan and Ali, Bharami},
    doi = {10.1016/j.procs.2016.09.306},
    journal = {Procedia Computer Science},
    keywords = {convolutional neural network,crowdsourcing,hybrid machine-human model,word embedding,word stemming},
    pages = {153--158},
    publisher = {Elsevier Masson SAS},
    title = {{Entity Resolution Using Convolutional Neural Network}},
    volume = {95},
    year = {2016}
}

@article{Cochinwala2001,
    author = {Cochinwala, Munir and Kurien, Verghese and Lalk, Gail and Shasha, Dennis},
    doi = {10.1016/S0020-0255(00)00070-0},
    issn = {00200255},
    journal = {Information Sciences},
    mendeley-groups = {Entity Resolution/ML methods,Entity Resolution/Supervised},
    number = {1-4},
    pages = {1--15},
    title = {{Efficient data reconciliation}},
    volume = {137},
    year = {2001}
}

@article{Verykios2000,
    author = {Verykios, Vassilios S. and Elmagarmid, Ahmed K. and Houstis, Elias N.},
    doi = {10.1016/S0020-0255(00)00013-X},
    issn = {00200255},
    journal = {Information sciences},
    number = {1},
    pages = {83--98},
    title = {{Automating the approximate record-matching process}},
    volume = {126},
    year = {2000}
}

@article{Elfeky2002,
    author = {Elfeky, Mohamed G. and Verykios, Vassilios S. and Elmagarmid, Ahmed K.},
    doi = {10.1109/icde.2002.994694},
    file = {:Users/olivierbinette/Documents/Mendeley/Elfeky, Verykios, Elmagarmid - 2002 - TAILOR A record linkage toolbox.pdf:pdf},
    journal = {Proceedings - International Conference on Data Engineering},
    mendeley-groups = {Entity Resolution/Supervised},
    pages = {17--28},
    title = {{TAILOR: A record linkage toolbox}},
    year = {2002}
}

@inproceedings{Christen2007,
    author = {Christen, Peter},
    title = {A Two-Step Classification Approach to Unsupervised Record Linkage},
    year = {2007},
    publisher = {Australian Computer Society, Inc.},
    booktitle = {Proceedings of the Sixth Australasian Conference on Data Mining and Analytics - Volume 70},
    pages = {111–119},
    numpages = {9},
    location = {Gold Coast, Australia}
}


@article{Torvik2005,
    author = {Torvik, Vetle I. and Weeber, Marc and Swanson, Don R. and Smalheiser, Neil R.},
    doi = {10.1002/asi.20105},
    issn = {15322882},
    journal = {Journal of the American Society for Information Science and Technology},
    mendeley-groups = {Entity Resolution/Supervised,Entity Resolution/ML methods},
    number = {2},
    pages = {140--158},
    title = {{A probabilistic similarity metric for medline records: A model for author name disambiguation}},
    volume = {56},
    year = {2005}
}

@inproceedings{Christen2008,
    author = {Christen, Peter},
    title = {Automatic Record Linkage Using Seeded Nearest Neighbour and Support Vector Machine Classification},
    year = {2008},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    booktitle = {Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
    pages = {151–159},
    numpages = {9},
    location = {Las Vegas, Nevada, USA},
}
  

@article{Azoulay2007,
    author = {Azoulay, Pierre and Michigan, Ryan and Sampat, Bhaven N.},
    title = {The Anatomy of Medical School Patenting},
    journal = {New England Journal of Medicine},
    volume = {357},
    number = {20},
    pages = {2049-2056},
    year = {2007},
}


@techreport{Azoulay2011,
  title={The Diffusion of Scientific Knowledge Across Time and Space: Evidence from Professional Transitions for the Superstars of Medicine},
  author={Azoulay, Pierre and Zivin, Joshua S Graff and Sampat, Bhaven N},
  year={2011},
  institution={NBER Working Paper Series},
  number = {16683}
}

@inproceedings{Ventura2014,
    address = {Cham},
    author = {Ventura, Samuel L and Nugent, Rebecca and Erica R.H. Fuchs},
    booktitle = {Privacy in Statistical Databases},
    editor = {Domingo-Ferrer, J.},
    isbn = {978-3-319-11257-2},
    pages = {283--298},
    publisher = {Springer International Publishing},
    title = {{Hierarchical Linkage Clustering with Distributions of Distances for Large-Scale Record Linkage}},
    year = {2014}
}

@ARTICLE{Christophides2019,
       author = {{Christophides}, Vassilis and {Efthymiou}, Vasilis and
         {Palpanas}, Themis and {Papadakis}, George and {Stefanidis}, Kostas},
        title = "{End-to-End Entity Resolution for Big Data: A Survey}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Databases},
         year = 2019,
          eid = {arXiv:1905.06397},
          note = {arXiv:1905.06397},
archivePrefix = {arXiv},
       eprint = {1905.06397},
}

@article{Bhattacharya2006,
    author = {Bhattacharya, Indrajit and Getoor, Lise},
    doi = {10.1137/1.9781611972764.5},
    isbn = {089871611X},
    journal = {Proceedings of the Sixth SIAM International Conference on Data Mining},
    pages = {47--58},
    title = {{A Latent Dirichlet Model for Unsupervised Entity Resolution}},
    year = {2006}
}

@article{Potosky1993,
    abstract = {The National Cancer Institute and the Health Care Financing Administration share a strong research interest in cancer costs, access to cancer prevention and treatment services, and cancer patient outcomes. To develop a database for such research, the two agencies have undertaken a collaborative effort to link Medicare Program data with the Surveillance, Epidemiology, and End Results (SEER) Program database. The SEER Program is a system of 9 population-based tumor registries that collect standardized clinical information on cases diagnosed in separate, geographically defined areas covering approximately 10{\%} of the US population. Using a deterministic matching algorithm, the records of 94{\%} of SEER registry cases diagnosed at age 65 or older between 1973 to 1989, or more than 610,000 persons, were successfully linked with Medicare claims files. The resulting database, combining clinical characteristics with information on utilization and costs, will permit the investigation of the contribution of various patient and health care setting factors to treatment patterns, costs, and medical outcomes. {\textcopyright} 1993, J. B. Lippincott Company.},
    author = {Potosky, Arnold L. and Riley, Gerald F. and Lubitz, James D. and Mentnech, Renee M. and Kessler, Larry G.},
    doi = {10.1097/00005650-199308000-00006},
    file = {:home/olivier/Downloads/3765984.pdf:pdf},
    issn = {15371948},
    journal = {Medical Care},
    keywords = {Cancer,Costs of cancer,Medicare,Prevention of cancer,Tumor registry},
    mendeley-groups = {Entity Resolution/Deterministic},
    number = {8},
    pages = {732--748},
    pmid = {8336512},
    title = {{Potential for cancer related health services research using a linked medicare-tumor registry database}},
    volume = {31},
    year = {1993}
}

@book{Dusetzina2014,
    address = {Rockville, MD},
    author = {Dusetzina, Stacie B and Tyree, Seth and Meyer, Anne-Marie and Meyer, Adrian and Green, Laura and Carpenter, William R},
    booktitle = {(Prepared by the University of North Carolina at Chapel Hill under Contract No. 290-2010-000141.)},
    publisher = {Agency for Healthcare Research and Quality},
    title = {{Linking Data for Health Services Research: A Framework and Instructional Guide}},
    year = {2014}
}


@article{Gomatam2002,
    abstract = {We consider the problem of record linkage in the situation where we have only non-unique identifiers, like names, sex, race etc., as common identifiers in databases to be linked. For such situations much work on probabilistic methods of record linkage can be found in the statistical literature. However, although many groups undoubtedly still use deterministic procedures, not much literature is available on deterministic strategies. Furthermore, there appears to exist almost no documentation on the comparison of results for the two strategies. In this work we compare a stepwise deterministic linkage strategy with a probabilistic strategy, as implemented in AUTOMATCH, for a situation in which the truth is known. The comparison was carried out on a linkage between medical records from the Regional Perinatal Intensive Care Centers database and educational records from the Florida Department of Education. Social security numbers, available in both databases, were used to decide the true status of each record pair after matching. Match rates and error rates for the two strategies are compared and a discussion of their similarities and differences, strengths and weaknesses is presented. Copyright {\textcopyright} 2002 John Wiley {\&} Sons, Ltd.},
    author = {Gomatam, Shanti and Carter, Randy and Ariet, Mario and Mitchell, Glenn},
    doi = {10.1002/sim.1147},
    file = {:home/olivier/Downloads/10.1.1.455.5537.pdf:pdf},
    issn = {02776715},
    journal = {Statistics in Medicine},
    keywords = {AUTOMATCH,Document linkage,Exact matching,Hierarchical linkage strategies,Probabilistic matching,Stepwise deterministic linkage},
    mendeley-groups = {Entity Resolution/Deterministic},
    number = {10},
    pages = {1485--1496},
    title = {An Empirical Comparison of Record Linkage Procedures},
    volume = {21},
    year = {2002}
}

@article{Tromp2011,
    abstract = {Objective: To gain insight into the performance of deterministic record linkage (DRL) vs. probabilistic record linkage (PRL) strategies under different conditions by varying the frequency of registration errors and the amount of discriminating power. Study Design and Setting: A simulation study in which data characteristics were varied to create a range of realistic linkage scenarios. For each scenario, we compared the number of misclassifications (number of false nonlinks and false links) made by the different linking strategies: deterministic full, deterministic N-1, and probabilistic. Results: The full deterministic strategy produced the lowest number of false positive links but at the expense of missing considerable numbers of matches dependent on the error rate of the linking variables. The probabilistic strategy outperformed the deterministic strategy (full or N-1) across all scenarios. A deterministic strategy can match the performance of a probabilistic approach providing that the decision about which disagreements should be tolerated is made correctly. This requires a priori knowledge about the quality of all linking variables, whereas this information is inherently generated by a probabilistic strategy. Conclusion: PRL is more flexible and provides data about the quality of the linkage process that in turn can minimize the degree of linking errors, given the data provided. {\textcopyright} 2011 Elsevier Inc. All rights reserved.},
    author = {Tromp, Miranda and Ravelli, Anita C. and Bonsel, Gouke J. and Hasman, Arie and Reitsma, Johannes B.},
    doi = {10.1016/j.jclinepi.2010.05.008},
    file = {:home/olivier/Downloads/1-s2.0-S0895435610002258-main.pdf:pdf},
    issn = {08954356},
    journal = {Journal of Clinical Epidemiology},
    keywords = {Data quality,Deterministic linkage,Medical record linkage,Probabilistic linkage,Registries,Simulation study},
    mendeley-groups = {Entity Resolution/Deterministic},
    number = {5},
    pages = {565--572},
    publisher = {Elsevier Inc},
    title = {{Results from simulated data sets: Probabilistic record linkage outperforms deterministic record linkage}},
    url = {http://dx.doi.org/10.1016/j.jclinepi.2010.05.008},
    volume = {64},
    year = {2011}
}

@article{Campbell2008,
    author = {Campbell, Kevin M. and Deck, Dennis and Krupski, Antoinette},
    journal = {Health Informatics Journal},
    number = {1},
    pages = {5--15},
    title = {Record Linkage Software in the Public Domain: A Comparison of Link Plus, The Link King, and a 'Basic' Deterministic Algorithm},
    volume = {14},
    year = {2008}
}

@article{Hassanzadeh2009,
    author = {Hassanzadeh, Oktie and Chiang, Fei and Lee, Hyun Chul and Miller, Ren\'{e}e J.},
    title = {Framework for Evaluating Clustering Algorithms in Duplicate Detection},
    year = {2009},
    issue_date = {August 2009},
    publisher = {VLDB Endowment},
    volume = {2},
    number = {1},
    doi = {10.14778/1687627.1687771},
    journal = {Proceedings of the VLDB Endowment},
    pages = {1282–1293},
    numpages = {12}
}
  


@article{Monge1997,
    author = {Monge, Alvaro E. and Elkan, Charles P.},
    doi = {10.1.1.28.8405},
    journal = {Proceedings of the SIGMOD 1997 workshop on research issues on data mining and knowledge discovery},
    pages = {23--29},
    title = {{An Efficient Domain-Independent Algorithm for Detecting Approximately Duplicate Database Records}},
    year = {1997}
}

@article{Korn2007,
    abstract = {Kron (2007) argues for the promotion of intentionality in the internal culture of the museum so that life-long learning becomes embedded in the staff mindset. Intentions are defined as operationalizations of the mission statement because they can be measured and convey the meaning of the mission. The mission is also tied to identity insofar it should make explicit what the museum is, who it serves, and the specific ways in which it aims to serve. (259) (strictly speaking, we should talk about an ecosystem of intentions considering the museum is not one, but many people with goals which are not always aligned)},
    author = {Korn, Randi},
    doi = {10.1111/j.2151-6952.2007.tb00269.x},
    issn = {00113069},
    journal = {Curator: The Museum Journal},
    number = {2},
    pages = {255--264},
    title = {{The Case for Holistic Intentionality}},
    volume = {50},
    year = {2007}
}

@article{Bansal2004,
    author = {Bansal, Nikhil and Blum, Avrim and Chawla, Shuchi},
    journal = {Machine Learning},
    number = {1-3},
    pages = {89--113},
    title = {{Correlation Clustering}},
    volume = {56},
    year = {2004}
}

@inproceedings{Cohen2002,
    author = {Cohen, William W. and Richman, Jacob},
    title = {Learning to Match and Cluster Large High-Dimensional Data Sets for Data Integration},
    year = {2002},
    isbn = {158113567X},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    booktitle = {Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
    pages = {475–480},
    numpages = {6},
    location = {Edmonton, Alberta, Canada},
}
  


@article{Ailon2008,
    author = {Ailon, Nir and Charikar, Moses and Newman, Alantha},
    journal = {Journal of the ACM},
    number = {5},
    pages = {1--27},
    title = {Aggregating Inconsistent Information: Ranking and Clustering},
    volume = {55},
    year = {2008}
}

@article{Gionis2007,
    author = {Gionis, Aristides and Mannila, Heikki and Tsaparas, Panayiotis},
    title = {Clustering Aggregation},
    year = {2007},
    issue_date = {March 2007},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {1},
    number = {1},
    journal = {ACM Trans. Knowl. Discov. Data},
    pages = {4–es},
    numpages = {30},
}
  

@inproceedings{Filkov2003,
    author = {Filkov, Vladimir and Skiena, Steven},
    booktitle = {Proceedings of the International Conference on Tools with Artificial Intelligence},
    doi = {10.1109/tai.2003.1250220},
    issn = {10636730},
    pages = {418--426},
    title = {{Integrating Microarray Data by Consensus Clustering}},
    year = {2003}
}

@article{Charikar2003,
    title = "Clustering With Qualitative Information",
    journal = "Journal of Computer and System Sciences",
    volume = "71",
    number = "3",
    pages = "360 - 383",
    year = "2005",
    author = "Charikar, Moses and Guruswami, Venkatesan and Wirth, Anthony",
}

@book{Hartigan1975,
    author = {Hartigan, John A},
    file = {:Users/olivierbinette/Documents/Mendeley/Hartigan - 1975 - Clustering Algorithms.pdf:pdf},
    mendeley-groups = {Entity Resolution/Clustering},
    title = {{Clustering Algorithms}},
    year = {1975}
}

@article{Johnson1967,
    author = {Johnson, Stephen C.},
    doi = {10.1007/BF02289588},
    issn = {00333123},
    journal = {Psychometrika},
    number = {3},
    pages = {241--254},
    title = {{Hierarchical clustering schemes}},
    volume = {32},
    year = {1967}
}

@book{Naumann2010,
    author = {Naumann, Felix and Herschel, Melanie},
    booktitle = {Synthesis Lectures on Data Management},
    doi = {10.2200/s00262ed1v01y201003dtm003},
    file = {:home/olivier/Downloads/9feb60d96bec7ea1edfa42ad7d177f2e1d99.pdf:pdf},
    isbn = {9781608452200},
    issn = {2153-5418},
    mendeley-groups = {Entity Resolution/Clustering},
    number = {1},
    pages = {1--87},
    title = {{An Introduction to Duplicate Detection}},
    volume = {2},
    year = {2010}
}

@book{Han2011,
  author    = {Han, Jiawei and Kamber, Micheline and Pei, Jian},
  title     = {Data Mining: Concepts and Techniques},
  publisher = {Morgan Kaufmann Publishers},
  year      = {2011},
  series    = {},
  address   = {Waltham, MA, USA},
  edition   = {3}
}


@article{Bien2011,
    author = {Bien, Jacob and Tibshirani, Robert},
    doi = {10.1198/jasa.2011.tm10183},
    issn = {0162-1459},
    journal = {Journal of the American Statistical Association},
    keywords = {Agglomerative,Dendrogram,Unsupervised learning},
    language = {eng},
    number = {495},
    pages = {1075--1084},
    title = {{Hierarchical Clustering With Prototypes via Minimax Linkage}},
    url = {https://pubmed.ncbi.nlm.nih.gov/26257451 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4527350/},
    volume = {106},
    year = {2011}
}

@MISC{APgeorgia,
   author =       {Ben Nadler},
   title =        {Voting rights become a flashpoint in Georgia governor’s race},
   editor =       {Associated Press},
   month =        {October},
   year =         {2018},
   url = {https://apnews.com/fb011f39af3b40518b572c8cce6e906c},
   note =         {Online; posted October 9, 2018; retrieved July 17, 2020},
 }

@misc{ReutersGeorgia,
    author =       {Ax, Joseph},
    title =        {Georgia Lawsuit is Latest Blow in U.S. Fight Over Voting Rights},
    editor =       {Reuters},
    month =        {October},
    year =         {2018},
    url = {https://www.reuters.com/article/usa-election-registrations/georgia-lawsuit-is-latest-blow-in-u-s-fight-over-voting-rights-idUSKCN1ML333},
    note = {Online; posted October 12, 2018; retrieved July 17, 2020}
}

 @MISC{NCSL,
   author =       {National Conference on State Legislatures},
   title =        {Voter List Accuracy},
   editor =       {},
   month =        {March},
   year =         {2020},
   url = {https://www.ncsl.org/research/elections-and-campaigns/voter-list-accuracy.aspx},
   note =         {Online; posted March 20, 2020; retrieved July 17, 2020},
 }

@misc{PeoplevKemp,
title = {{Complaint, Georgia Coalition for the People's Agenda, Inc. v. Kemp}},
year = {2018}
}

@misc{TedGeorgia,
    title = {Georgia’s ‘Exact Match’ Law Could Potentially Harm Many Eligible Voters},
    author = {Ted Enamorado},
    url = {https://www.washingtonpost.com/news/monkey-cage/wp/2018/10/20/georgias-exact-match-law-could-disenfranchise-3031802-eligible-voters-my-research-finds/},
    year={2018},
    month={October},
    note={Online; posted October 20, 2020; retrieved July 17, 2020}
}


@article{Matsakis2010,
    author = {Matsakis, Nicholas Elias},
    pages = {137},
    title = {Active Duplicate Detection with Bayesian Nonparametric Models},
    year = {2010},
    note = {PhD Thesis}
}

@misc{Gregg2015,
    author={Gregg, Forest and Derek Eder},
    year={2015},
    title={Dedupe},
    url={https://github.com/dedupeio/dedupe}
}

@misc{deBruin2016,
    author={de Bruin, Jonathan},
    title = {recordlinkage},
    year = {2016},
    url={https://pypi.org/project/recordlinkage/}
}

@INPROCEEDINGS{Christen08febrl,
    author = {Peter Christen},
    title = {Febrl – An open source data cleaning, deduplication and record linkage system with a graphical user interface (Demonstration Session},
    booktitle = {In ACM International Conference on Knowledge Discovery and Data Mining},
    year = {2008},
    pages = {1065--1068}
}

@inproceedings{Yash2019,
    author = {Govind, Yash and Konda, Pradap and Suganthan G.C., Paul and Martinkus, Philip and Nagarajan, Palaniappan and Li, Han and Soundararajan, Aravind and Mudgal, Sidharth and Ballard, Jeff R. and Zhang, Haojun and Ardalan, Adel and Das, Sanjib and Paulsen, Derek and Singh Saini, Amanpreet and Paulson, Erik and Park, Youngchoon and Carter, Marshall and Sun, Mingju and Fung, Glenn M. and Doan, AnHai},
    title = {Entity Matching Meets Data Science: A Progress Report from the Magellan Project},
    year = {2019},
    isbn = {9781450356435},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3299869.3314042},
    doi = {10.1145/3299869.3314042},
    booktitle = {Proceedings of the 2019 International Conference on Management of Data},
    pages = {389–403},
    numpages = {15},
    location = {Amsterdam, Netherlands},
}

@article{sariyar2010recordlinkage,
  title={The RecordLinkage package: Detecting errors in data},
  author={Sariyar, Murat and Borg, Andreas},
  journal={The R Journal},
  volume={2},
  number={2},
  pages={61--67},
  year={2010}
}

@data{Lai2011_data,
    author = {Ronald Lai and Alexander D'Amour and Amy Yu and Ye Sun and Lee Fleming},
    publisher = {Harvard Dataverse},
    title = {{Disambiguation and Co-authorship Networks of the U.S. Patent Inventor Database (1975 - 2010)}},
    UNF = {UNF:5:RqsI3LsQEYLHkkg5jG/jRg==},
    year = {2011},
    version = {V5},
    doi = {10.7910/DVN/5F1RRI},
    url = {https://doi.org/10.7910/DVN/5F1RRI}
}

@article{Sadosky2015,
    archivePrefix = {arXiv},
    arxivId = {1510.07714},
    author = {Sadosky, Peter and Shrivastava, Anshumali and Price, Megan and Steorts, Rebecca C.},
    eprint = {1510.07714},
    file = {:home/olivier/Downloads/1510.07714.pdf:pdf},
    mendeley-groups = {Entity Resolution/Beka},
    pages = {1--25},
    title = {{Blocking Methods Applied to Casualty Records from the Syrian Conflict}},
    url = {http://arxiv.org/abs/1510.07714},
    year = {2015}
}

@article{Bilenko2006,
    abstract = {Many data mining tasks require computing similarity between pairs of objects. Pairwise similarity computations are particularly important in record linkage systems, as well as in clustering and schema mapping algorithms. Because the number of object pairs grows quadratically with the size of the dataset, computing similarity between all pairs is impractical and becomes prohibitive for large dataseis and complex similarity functions. Blocking methods alleviate this problem by efficiently selecting approximately similar object pairs for subsequent distance computations, leaving out the remaining pairs as dissimilar. Previously proposed blocking methods require manually constructing an indexbased similarity function or selecting a set of predicates, followed by hand-tuning of parameters. In this paper, we introduce an adaptive framework for automatically learning blocking functions that are efficient and accurate. We describe two predicate-based formulations of leamable blocking functions and provide learning algorithms for training them. The effectiveness of the proposed techniques is demonstrated on real and simulated dataseis, on which they prove to be more accurate than non-adaptive blocking methods. {\textcopyright} 2006 IEEE.},
    author = {Bilenko, Mikhail and Kamath, Beena and Mooney, Raymond J.},
    doi = {10.1109/ICDM.2006.13},
    file = {:home/olivier/Downloads/06-icdm.pdf:pdf},
    isbn = {0769527019},
    issn = {15504786},
    journal = {Proceedings - IEEE International Conference on Data Mining, ICDM},
    mendeley-groups = {Entity Resolution/Blocking},
    number = {December},
    pages = {87--96},
    title = {{Adaptive blocking: Learning to scale up record linkage}},
    year = {2006}
}

@article{Evangelista2010,
    author = {Evangelista, Luiz Osvaldo and Cortez, Eli and da Silva, Altigran S and {Meira Jr.}, Wagner},
    file = {:home/olivier/Downloads/blocking-jidm.pdf:pdf},
    issn = {21787107},
    journal = {Journal of Information and Data Management},
    keywords = {blocking,genetic algorithms,record linkage},
    mendeley-groups = {Entity Resolution/Blocking},
    number = {2},
    pages = {167},
    title = {{Adaptive and Flexible Blocking for Record Linkage Tasks}},
    url = {http://seer.lcc.ufmg.br/index.php/jidm/article/view/50},
    volume = {1},
    year = {2010}
}

@mist{Ball2016,
    author = {Patrick Ball},
    title = {A geeky deep-dive: database deduplication to identify victims of human rights violations},
    url = {https://hrdag.org/2016/01/08/a-geeky-deep-dive-database-deduplication-to-identify-victims-of-human-rights-violations/}
}

@article{Gu2003,
    author = {Gu, Lifang and Baxter, Rohan},
    file = {:home/olivier/Downloads/10.1.1.14.8119.pdf:pdf},
    journal = {Cmis},
    keywords = {data cleaning,entity,entity identification,list washing,merge,object isomerism,purge,reconciliation,record linkage},
    mendeley-groups = {Entity Resolution,Entity Resolution/Surveys},
    pages = {03/83},
    title = {{Record linkage: Current practice and future directions}},
    url = {http://festivalofdoubt.uq.edu.au/papers/record{\_}linkage.pdf},
    year = {2003}
}

@inbook{Jurek2019,
    address = {Cham},
    author = {{Jurek-Loughrey}, Anna and Deepak, P},
    booktitle = {Linking and Mining Heterogeneous and Multi-view Data},
    doi = {10.1007/978-3-030-01872-6_3},
    editor = {P, Deepak and Jurek-Loughrey, Anna},
    isbn = {978-3-030-01872-6},
    pages = {55--78},
    publisher = {Springer International Publishing},
    title = {{Semi-supervised and Unsupervised Approaches to Record Pairs Classification in Multi-Source Data Linkage}},
    url = {https://doi.org/10.1007/978-3-030-01872-6{\_}3},
    year = {2019}
}

@inbook{OHare2019,
    address = {Cham},
    author = {O'Hare, Kevin and Jurek-Loughrey, Anna and de Campos, Cassio},
    booktitle = {Linking and Mining Heterogeneous and Multi-view Data},
    doi = {10.1007/978-3-030-01872-6_4},
    editor = {P, Deepak and Jurek-Loughrey, Anna},
    isbn = {978-3-030-01872-6},
    pages = {79--105},
    publisher = {Springer International Publishing},
    title = {{A Review of Unsupervised and Semi-supervised Blocking Methods for Record Linkage}},
    url = {https://doi.org/10.1007/978-3-030-01872-6{\_}4},
    year = {2019}
}



@article{Winkler2014,
    abstract = {This overview gives background on a number of statistical methods that have been proven effective for record linkage. To prepare data for the main computational algorithms, we need parsing/standardization that allows us to structure the free-form names, addresses, and other fields into corresponding components. The main parameter-estimation methods are unsupervised methods that yield 'optimal' record linkage parameters. Extended methods provide estimates of false match rates in both unsupervised and, with greater accuracy, in semi-supervised situations. Finally, the paper describes ongoing research for adjusting standard statistical analyses for linkage error. {\textcopyright} 2014 Wiley Periodicals, Inc. 6 5 September/October 2014 10.1002/wics.1317 Overview Overview Published 2014. This article is a U.S. Government work and is in the public domain in the USA.},
    author = {Winkler, William E.},
    doi = {10.1002/wics.1317},
    file = {:home/olivier/Downloads/wics.1317.pdf:pdf},
    issn = {19390068},
    journal = {Wiley Interdisciplinary Reviews: Computational Statistics},
    keywords = {Classification rules,Entity resolution,False match and nonmatch rates,String comparison,Unsupervised learning},
    mendeley-groups = {Entity Resolution,Entity Resolution/Surveys},
    number = {5},
    pages = {313--325},
    title = {{Matching and record linkage}},
    volume = {6},
    year = {2014}
}

@article{Brizan2006,
    abstract = {A great deal of research is focused on formation of a data warehouse. This is an important area of research as it could save many computation cycles and thus allow accurate information provided to the right people at the right time. Two considerations when forming a data warehouse are data cleansing (including entity resolution) and with schema integration (including record linkage). Uncleansed and fragmented data requires time to decipher and may lead to increased costs for an organization, so data cleansing and schema integration can save a great many (human) computation cycles and can lead to higher organizational efficiency. In this study we survey the literature for the methodologies proposed or developed for entity resolution and record linkage. This survey provides a foundation for solving many problems in data warehousing. For instance, little or no research has been directed at the problem of maintenance of cleansed and linked relations.},
    author = {Brizan, David Guy and Tansel, Abdullah Uz},
    file = {:home/olivier/Downloads/A. Survey of Entity Resolution and Record Linkage Methodologies.pdf:pdf},
    isbn = {1941-6687},
    issn = {1941-6687},
    journal = {Communications of the IIMA},
    mendeley-groups = {Entity Resolution/Surveys},
    number = {3},
    pages = {41--50},
    title = {{A Survey of Entity Resolution and Record Linkage Methodologies}},
    volume = {6},
    year = {2006}
}

@INPROCEEDINGS{Vidhya2019,
  author={K. A. {Vidhya} and T. V. {Geetha}},
  booktitle={2019 IEEE 9th International Conference on Advanced Computing (IACC)}, 
  title={Entity Resolution and Blocking: A Review}, 
  year={2019},
  volume={},
  number={},
  pages={133-140},
 }

@article{Sayers2016,
    abstract = {Studies involving the use of probabilistic record linkage are becoming increasingly common. However, the methods underpinning probabilistic record linkage are not widely taught or understood, and therefore these studies can appear to be a ‘black box' research tool. In this article, we aim to describe the process of probabilistic record linkage through a simple exemplar. We first introduce the concept of deterministic linkage and contrast this with probabilistic linkage. We illustrate each step of the process using a simple exemplar and describe the data structure required to perform a probabilistic linkage. We describe the process of calculating and interpreting matched weights and how to convert matched weights into posterior probabilities of a match using Bayes theorem. We conclude this article with a brief discussion of some of the computational demands of record linkage, how you might assess the quality of your linkage algorithm, and how epidemiologists can maximize the value of their record-linked research using robust record linkage methods.},
    author = {Sayers, Adrian and Ben-Shlomo, Yoav and Blom, Ashley W. and Steele, Fiona},
    doi = {10.1093/ije/dyv322},
    file = {:home/olivier/Downloads/dyv322(1).pdf:pdf},
    issn = {14643685},
    journal = {International Journal of Epidemiology},
    keywords = {Bias,Data linkage,Epidemiological methods,Medical record linkage,Record linkage},
    mendeley-groups = {Entity Resolution/Surveys},
    number = {3},
    pages = {954--964},
    pmid = {26686842},
    title = {{Probabilistic record linkage}},
    volume = {45},
    year = {2016}
}

